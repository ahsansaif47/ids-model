{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "interpreter": {
      "hash": "35146d2ce121e8e653791712ad16cca408ea08bd1c84e92938f653a743d84e6c"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit ('Graphs': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "ids_batch_training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxB5BkGDjP46"
      },
      "source": [
        "# Mounting Drive\n",
        "Mounting drive to fetch dataset and other resources from the drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rMn03dljTHw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4244403e-1a9f-498b-c401-aa0134349450"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4y96wy4y3Vp"
      },
      "source": [
        "# Installing Spektral Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nv-38fZoy3Vw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c7e8828-f12a-42cc-9536-f939bdf27baa"
      },
      "source": [
        "!pip install git+https://github.com/danielegrattarola/spektral"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/danielegrattarola/spektral\n",
            "  Cloning https://github.com/danielegrattarola/spektral to /tmp/pip-req-build-uzsn6zp2\n",
            "  Running command git clone -q https://github.com/danielegrattarola/spektral /tmp/pip-req-build-uzsn6zp2\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from spektral==1.0.8) (1.1.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from spektral==1.0.8) (4.2.6)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from spektral==1.0.8) (2.6.3)\n",
            "Requirement already satisfied: numpy<1.20 in /usr/local/lib/python3.7/dist-packages (from spektral==1.0.8) (1.19.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from spektral==1.0.8) (1.1.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from spektral==1.0.8) (2.23.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from spektral==1.0.8) (1.0.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from spektral==1.0.8) (1.4.1)\n",
            "Requirement already satisfied: tensorflow>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from spektral==1.0.8) (2.7.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from spektral==1.0.8) (4.62.3)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral==1.0.8) (12.0.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral==1.0.8) (3.17.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral==1.0.8) (0.2.0)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral==1.0.8) (0.4.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral==1.0.8) (1.42.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral==1.0.8) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral==1.0.8) (2.7.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral==1.0.8) (3.3.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral==1.0.8) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral==1.0.8) (0.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral==1.0.8) (3.10.0.2)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral==1.0.8) (2.7.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral==1.0.8) (1.13.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral==1.0.8) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral==1.0.8) (0.22.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral==1.0.8) (0.37.0)\n",
            "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral==1.0.8) (2.7.0)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral==1.0.8) (2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral==1.0.8) (3.1.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral==1.0.8) (1.6.3)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow>=2.1.0->spektral==1.0.8) (1.5.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.1.0->spektral==1.0.8) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.1.0->spektral==1.0.8) (1.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.1.0->spektral==1.0.8) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.1.0->spektral==1.0.8) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.1.0->spektral==1.0.8) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.1.0->spektral==1.0.8) (1.35.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.1.0->spektral==1.0.8) (57.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.1.0->spektral==1.0.8) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.1.0->spektral==1.0.8) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.1.0->spektral==1.0.8) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=2.1.0->spektral==1.0.8) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow>=2.1.0->spektral==1.0.8) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow>=2.1.0->spektral==1.0.8) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow>=2.1.0->spektral==1.0.8) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->spektral==1.0.8) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->spektral==1.0.8) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->spektral==1.0.8) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->spektral==1.0.8) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=2.1.0->spektral==1.0.8) (3.1.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->spektral==1.0.8) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->spektral==1.0.8) (2.8.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->spektral==1.0.8) (3.0.0)\n",
            "Building wheels for collected packages: spektral\n",
            "  Building wheel for spektral (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for spektral: filename=spektral-1.0.8-py3-none-any.whl size=123386 sha256=ba3a682391200d4d62c501e5e018aa1ae8be50c9be405493cb81546ae2a75486\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-nudnqanb/wheels/af/7c/1f/e06aba9c0f493bb708968b8b396fe7523fdfb1c1c0818730be\n",
            "Successfully built spektral\n",
            "Installing collected packages: spektral\n",
            "Successfully installed spektral-1.0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWC39Ub4y3Vy"
      },
      "source": [
        "# Loading Dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGW25B2Ay3Vz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52a63868-012a-445b-b295-c39cf7360ff7"
      },
      "source": [
        "# Dataset loading\n",
        "import os\n",
        "\n",
        "dataset = \"/content/drive/MyDrive/Project GCN Dataset/Dataset (Labelled Images)/\"\n",
        "# dataset = \"./Dataset (Labelled Images)/\"\n",
        "lab_files_path = dataset+\"label/\"\n",
        "# lab_files_path = dataset+\"label/\"\n",
        "\n",
        "# total invoice files\n",
        "files = os.listdir(lab_files_path)\n",
        "\n",
        "\n",
        "# training set\n",
        "train = files[:120]\n",
        "\n",
        "# validation set\n",
        "valid_set = files[120:125]\n",
        "\n",
        "# test set\n",
        "test = files[125: ]\n",
        "\n",
        "print(\"Total Label files are: \", len(files))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Label files are:  129\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Uv-dNc1y3V0"
      },
      "source": [
        "# Geometric Algo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xBnpBBWy3V0"
      },
      "source": [
        "from PIL.Image import Image\n",
        "from networkx.algorithms.shortest_paths.weighted import single_source_dijkstra\n",
        "import matplotlib.pyplot as plt\n",
        "from pandas.core.frame import DataFrame\n",
        "from PIL import Image\n",
        "import networkx as nx\n",
        "\n",
        "\n",
        "# csv = './A-10.csv'\n",
        "# df = pd.read_csv(csv)\n",
        "df = 0\n",
        "xMIN, xMAX = [], []\n",
        "yMIN, yMAX = [], []\n",
        "Text = []\n",
        "\n",
        "\n",
        "def findRight(df, df_ind, xMIN, xMAX, yMIN, yMAX):\n",
        "    S_list = []\n",
        "    xmax = xMAX[df_ind]\n",
        "    ymin = yMIN[df_ind]\n",
        "    ymax = yMAX[df_ind]\n",
        "\n",
        "    for i in range(len(df)):\n",
        "        if(xMIN[i] > xmax):\n",
        "            if not (yMIN[i] > ymax or yMAX[i] < ymin):\n",
        "                if(yMIN[i] <= ymin and yMAX[i] <= ymax):\n",
        "                    S_list.append(i)\n",
        "                elif (yMIN[i] <= ymin and yMAX[i] >= ymax):\n",
        "                    S_list.append(i)\n",
        "                elif (yMIN[i] >= ymin and yMAX[i] <= ymax):\n",
        "                    S_list.append(i)\n",
        "                elif (yMIN[i] >= ymin and yMAX[i] >= ymax):\n",
        "                    S_list.append(i)\n",
        "                elif (yMIN[i] == ymin and yMAX[i] == ymax):\n",
        "                    S_list.append(i)\n",
        "\n",
        "    # print(S_list)\n",
        "    if S_list:\n",
        "        consec = S_list[0]\n",
        "        for j in S_list:\n",
        "            if(xMIN[consec] > xMIN[j]):\n",
        "                consec = j\n",
        "        return consec\n",
        "\n",
        "    return -1\n",
        "\n",
        "\n",
        "def findLeft(df, df_ind, xMIN, xMAX, yMIN, yMAX):\n",
        "    S_list = []\n",
        "    xmin = xMIN[df_ind]\n",
        "    ymin = yMIN[df_ind]\n",
        "    ymax = yMAX[df_ind]\n",
        "\n",
        "    for i in range(len(df)):\n",
        "        if(xMAX[i] < xmin):\n",
        "            if not (yMIN[i] > ymax or yMAX[i] < ymin):\n",
        "                if(yMIN[i] <= ymin and yMAX[i] <= ymax):\n",
        "                    S_list.append(i)\n",
        "                elif (yMIN[i] <= ymin and yMAX[i] >= ymax):\n",
        "                    S_list.append(i)\n",
        "                elif (yMIN[i] >= ymin and yMAX[i] <= ymax):\n",
        "                    S_list.append(i)\n",
        "                elif (yMIN[i] >= ymin and yMAX[i] >= ymax):\n",
        "                    S_list.append(i)\n",
        "                elif (yMIN[i] == ymin and yMAX[i] == ymax):\n",
        "                    S_list.append(i)\n",
        "    # print(S_list)\n",
        "    if S_list:\n",
        "        consec = S_list[0]\n",
        "        for j in S_list:\n",
        "            if(xMAX[j] > xMAX[consec]):\n",
        "                consec = j\n",
        "        return consec\n",
        "    return -1\n",
        "\n",
        "\n",
        "def findUp(df, df_ind, xMIN, xMAX, yMIN, yMAX):\n",
        "    S_list = []\n",
        "    xmin = xMIN[df_ind]\n",
        "    xmax = xMAX[df_ind]\n",
        "    ymin = yMIN[df_ind]\n",
        "\n",
        "    for i in range(len(df)):\n",
        "        if(yMAX[i] < ymin):\n",
        "            if not (xMAX[i] < xmin or xMIN[i] > xmax):\n",
        "                if(xMIN[i] <= xmin and xMAX[i] <= xmax):\n",
        "                    S_list.append(i)\n",
        "                elif (xMIN[i] <= xmin and xMAX[i] >= xmax):\n",
        "                    S_list.append(i)\n",
        "                elif (xMIN[i] >= xmin and xMAX[i] <= xmax):\n",
        "                    S_list.append(i)\n",
        "                elif (xMIN[i] >= xmin and xMAX[i] >= xmax):\n",
        "                    S_list.append(i)\n",
        "                elif (xMIN[i] == xmin and xMAX[i] == xmax):\n",
        "                    S_list.append(i)\n",
        "    # print(S_list)\n",
        "\n",
        "    if S_list:\n",
        "        consec = S_list[0]\n",
        "        for j in S_list:\n",
        "            if(yMAX[j] > yMAX[consec]):\n",
        "                consec = j\n",
        "        return consec\n",
        "\n",
        "    return -1\n",
        "\n",
        "\n",
        "def findDown(df, df_ind, xMIN, xMAX, yMIN, yMAX):\n",
        "    S_list = []\n",
        "    xmin = xMIN[df_ind]\n",
        "    xmax = xMAX[df_ind]\n",
        "    ymax = yMAX[df_ind]\n",
        "\n",
        "    for i in range(len(df)):\n",
        "        if(yMIN[i] > ymax):\n",
        "            if not (xMAX[i] < xmin or xMIN[i] > xmax):\n",
        "                if(xMIN[i] <= xmin and xMAX[i] <= xmax):\n",
        "                    S_list.append(i)\n",
        "                elif (xMIN[i] <= xmin and xMAX[i] >= xmax):\n",
        "                    S_list.append(i)\n",
        "                elif (xMIN[i] >= xmin and xMAX[i] <= xmax):\n",
        "                    S_list.append(i)\n",
        "                elif (xMIN[i] >= xmin and xMAX[i] >= xmax):\n",
        "                    S_list.append(i)\n",
        "                elif (xMIN[i] == xmin and xMAX[i] == xmax):\n",
        "                    S_list.append(i)\n",
        "    # print(S_list)\n",
        "    if S_list:\n",
        "        consec = S_list[0]\n",
        "        for j in S_list:\n",
        "            if(yMIN[j] < yMIN[consec]):\n",
        "                consec = j\n",
        "        return consec\n",
        "\n",
        "    return -1\n",
        "\n",
        "\n",
        "def makeGraph(df):\n",
        "    G = nx.Graph()\n",
        "    xMIN = df['xmin']\n",
        "    xMAX = df['xmax']\n",
        "    yMIN = df['ymin']\n",
        "    yMAX = df['ymax']\n",
        "    Text = df['Object']\n",
        "\n",
        "    for i in range(len(df)):\n",
        "        if findUp(df, i, xMIN, xMAX, yMIN, yMAX):\n",
        "            l = findUp(df, i, xMIN, xMAX, yMIN, yMAX)\n",
        "            if(l != -1):\n",
        "                text = Text[l]\n",
        "                G.add_edge(Text[i], text)\n",
        "        if findRight(df, i, xMIN, xMAX, yMIN, yMAX):\n",
        "            l = findRight(df, i, xMIN, xMAX, yMIN, yMAX)\n",
        "            if (l != -1):\n",
        "                text = Text[l]\n",
        "                G.add_edge(Text[i], text)\n",
        "        if findDown(df, i, xMIN, xMAX, yMIN, yMAX):\n",
        "            l = findDown(df, i, xMIN, xMAX, yMIN, yMAX)\n",
        "            if (l != -1):\n",
        "                text = Text[l]\n",
        "                G.add_edge(Text[i], text)\n",
        "        if findLeft(df, i, xMIN, xMAX, yMIN, yMAX):\n",
        "            l = findLeft(df, i, xMIN, xMAX, yMIN, yMAX)\n",
        "            if (l != -1):\n",
        "                text = Text[l]\n",
        "                G.add_edge(Text[i], text)\n",
        "    return G\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzJUAQMSy3V3"
      },
      "source": [
        "# Matrices Over Diagonal\n",
        "Place an incident Matrix over diagonal with existing matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDoDx5eZy3V4"
      },
      "source": [
        "from typing import SupportsAbs\n",
        "from numpy.core.fromnumeric import shape\n",
        "\n",
        "# Test Matrices\n",
        "# mat1 = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "# mat2 = np.array([[3, 2, 1, 7], [6, 5, 4, 9], [9, 8, 7, 4], [1, 5, 7, 2]])\n",
        "\n",
        "\n",
        "def alignDiagonally(M1, M2, prev_Len):\n",
        "    for i in range(prev_Len, np.shape(M1)[0]):\n",
        "        for j in range(prev_Len, np.shape(M1)[0]):\n",
        "            x = i - prev_Len\n",
        "            y = j - prev_Len\n",
        "            M1[i][j] = M2[x][y]\n",
        "\n",
        "    return M1\n",
        "\n",
        "\n",
        "def resizeMatrix(M, I):\n",
        "    oldMat_Len = np.shape(M)[0]\n",
        "    z = np.zeros((oldMat_Len, np.shape(I)[0]), dtype=np.int64)\n",
        "    newArray = np.append(M, z, axis=1)\n",
        "    M = newArray\n",
        "\n",
        "    # Appending 1D arrays of zeros in the original Matrix\n",
        "    # (i.e. the matrix in which we want to align othe rmatrices diagonally)\n",
        "    # for i in range(appZero):\n",
        "    #     M = np.vstack((M, L))\n",
        "    appZero = np.shape(I)[0]\n",
        "    x = oldMat_Len + appZero\n",
        "    L = np.zeros((np.shape(I)[0], x), dtype=np.int64)\n",
        "    newArray = np.append(M, L, axis=0)\n",
        "    M = newArray\n",
        "\n",
        "    M = alignDiagonally(M, I, oldMat_Len)\n",
        "    return M\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jvG2tbgy3V5"
      },
      "source": [
        "# Dataset Batch 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8EK8k-4y3V6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b767c3b-6331-48b8-c6c1-02d7e8c10e1f"
      },
      "source": [
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "from numpy.core.fromnumeric import shape\n",
        "\n",
        "Z_file = lab_files_path + train[0]\n",
        "df = pd.read_csv(Z_file)\n",
        "G = makeGraph(df)\n",
        "M1 = nx.to_numpy_array(G, dtype=np.int32)\n",
        "\n",
        "for i in range(1, 26):\n",
        "    print(\"\\n= = = = = = = = = = = = = = = =\")\n",
        "    print(\"Iteration No.: \", i)\n",
        "    # Getting file\n",
        "    f = lab_files_path + files[i]\n",
        "    # Making dataframe of file\n",
        "    df = pd.read_csv(f)\n",
        "    # Making graph of the dataframe. \n",
        "    G = makeGraph(df)\n",
        "    # Storing the graph as an incident matrix(an adjacency matrix)\n",
        "    I = nx.to_numpy_array(G, dtype=np.int32)\n",
        "    # Now resizing the original sparce matrix with the new incident matrix\n",
        "    M1 = resizeMatrix(M1, I)\n",
        "\n",
        "print(\"Dimentions of Batch 1 matrix is: \", np.shape(M1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  1\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  2\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  3\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  4\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  5\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  6\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  7\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  8\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  9\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  10\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  11\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  12\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  13\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  14\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  15\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  16\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  17\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  18\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  19\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  20\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  21\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  22\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  23\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  24\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  25\n",
            "Dimentions of Batch 1 matrix is:  (5762, 5762)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iei1uPfanVJd"
      },
      "source": [
        "# Dataset Batch 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnzFSytunXVi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "877f2907-9462-4770-8513-c85b861e18d5"
      },
      "source": [
        "Z1_file = lab_files_path + files[26]\n",
        "df = pd.read_csv(Z1_file)\n",
        "G = makeGraph(df)\n",
        "M2 = nx.to_numpy_array(G, dtype=np.int32)\n",
        "\n",
        "for i in range(27, 65):\n",
        "    print(\"\\n= = = = = = = = = = = = = = = =\")\n",
        "    print(\"Iteration No.: \", i)\n",
        "    # Getting file\n",
        "    f = lab_files_path + files[i]\n",
        "    # Making dataframe of file\n",
        "    df = pd.read_csv(f)\n",
        "    # Making graph of the dataframe. \n",
        "    G = makeGraph(df)\n",
        "    # Storing the graph as an incident matrix(an adjacency matrix)\n",
        "    I = nx.to_numpy_array(G, dtype=np.int32)\n",
        "    # Now resizing the original sparce matrix with the new incident matrix\n",
        "    M2 = resizeMatrix(M2, I)\n",
        "\n",
        "print(\"Dimentions of Batch 2 matrix is: \", np.shape(M2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  27\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  28\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  29\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  30\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  31\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  32\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  33\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  34\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  35\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  36\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  37\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  38\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  39\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  40\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  41\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  42\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  43\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  44\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  45\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  46\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  47\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  48\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  49\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  50\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  51\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  52\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  53\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  54\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  55\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  56\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  57\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  58\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  59\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  60\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  61\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  62\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  63\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  64\n",
            "Dimentions of Batch 2 matrix is:  (5410, 5410)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pyc_oLcnhA4"
      },
      "source": [
        "# Dataset Batch 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dgPoQFInjAX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab1ef908-f6c1-4b52-c6e8-3e9739a332fd"
      },
      "source": [
        "Z2_file = lab_files_path + files[65]\n",
        "df = pd.read_csv(Z2_file)\n",
        "G = makeGraph(df)\n",
        "M3 = nx.to_numpy_array(G, dtype=np.int32)\n",
        "    \n",
        "for i in range(66, 120):\n",
        "    print(\"\\n= = = = = = = = = = = = = = = =\")\n",
        "    print(\"Iteration No.: \", i)\n",
        "    # Getting file\n",
        "    f = lab_files_path + files[i]\n",
        "    # Making dataframe of file\n",
        "    df = pd.read_csv(f)\n",
        "    # Making graph of the dataframe. \n",
        "    G = makeGraph(df)\n",
        "    # Storing the graph as an incident matrix(an adjacency matrix)\n",
        "    I = nx.to_numpy_array(G, dtype=np.int32)\n",
        "    # Now resizing the original sparce matrix with the new incident matrix\n",
        "    M3 = resizeMatrix(M3, I)\n",
        "\n",
        "print(\"Dimentions of Batch 3 matrix is: \", np.shape(M3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  66\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  67\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  68\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  69\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  70\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  71\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  72\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  73\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  74\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  75\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  76\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  77\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  78\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  79\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  80\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  81\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  82\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  83\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  84\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  85\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  86\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  87\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  88\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  89\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  90\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  91\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  92\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  93\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  94\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  95\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  96\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  97\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  98\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  99\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  100\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  101\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  102\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  103\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  104\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  105\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  106\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  107\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  108\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  109\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  110\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  111\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  112\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  113\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  114\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  115\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  116\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  117\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  118\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  119\n",
            "Dimentions of Batch 3 matrix is:  (9207, 9207)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XfF5cdO8fqB"
      },
      "source": [
        "# Validation Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fy_J7XMN8hbm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f98ad666-41aa-4fe4-9bb8-d56d0a8cc973"
      },
      "source": [
        "Z3_file = lab_files_path + files[120]\n",
        "df = pd.read_csv(Z3_file)\n",
        "G = makeGraph(df)\n",
        "M4 = nx.to_numpy_array(G, dtype=np.int32)\n",
        "    \n",
        "for i in range(121, 125):\n",
        "    print(\"\\n= = = = = = = = = = = = = = = =\")\n",
        "    print(\"Iteration No.: \", i)\n",
        "    # Getting file\n",
        "    f = lab_files_path + files[i]\n",
        "    # Making dataframe of file\n",
        "    df = pd.read_csv(f)\n",
        "    # Making graph of the dataframe. \n",
        "    G = makeGraph(df)\n",
        "    # Storing the graph as an incident matrix(an adjacency matrix)\n",
        "    I = nx.to_numpy_array(G, dtype=np.int32)\n",
        "    # Now resizing the original sparce matrix with the new incident matrix\n",
        "    M4 = resizeMatrix(M4, I)\n",
        "\n",
        "print(\"Dimentions of Validation matrix is: \", np.shape(M4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  121\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  122\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  123\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  124\n",
            "Dimentions of Validation matrix is:  (420, 420)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-G1pDPki_d-"
      },
      "source": [
        "# Test Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A19wgJhIjCMM",
        "outputId": "6fd52863-8f2c-459c-e8c5-5cb79ff583dd"
      },
      "source": [
        "Z4_file = lab_files_path + files[125]\n",
        "df = pd.read_csv(Z4_file)\n",
        "G = makeGraph(df)\n",
        "M5 = nx.to_numpy_array(G, dtype=np.int32)\n",
        "    \n",
        "for i in range(126, len(files)):\n",
        "    print(\"\\n= = = = = = = = = = = = = = = =\")\n",
        "    print(\"Iteration No.: \", i)\n",
        "    # Getting file\n",
        "    f = lab_files_path + files[i]\n",
        "    # Making dataframe of file\n",
        "    df = pd.read_csv(f)\n",
        "    # Making graph of the dataframe. \n",
        "    G = makeGraph(df)\n",
        "    # Storing the graph as an incident matrix(an adjacency matrix)\n",
        "    I = nx.to_numpy_array(G, dtype=np.int32)\n",
        "    # Now resizing the original sparce matrix with the new incident matrix\n",
        "    M5 = resizeMatrix(M5, I)\n",
        "\n",
        "print(\"Dimentions of Test matrix is: \", np.shape(M5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  126\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  127\n",
            "\n",
            "= = = = = = = = = = = = = = = =\n",
            "Iteration No.:  128\n",
            "Dimentions of Test matrix is:  (298, 298)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ccv9nDguy3V6"
      },
      "source": [
        "# Saving Batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xl9pdApcy3V7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93c71515-b8b6-4915-90d2-4e84aabb605e"
      },
      "source": [
        "print(\"Saving Matrix\")\n",
        "# np.save(\"./Matrices/Matrix_b1.npy\", M1)\n",
        "np.save(\"/content/drive/MyDrive/Project GCN Dataset/Mat1.npy\", M1)\n",
        "np.save(\"/content/drive/MyDrive/Project GCN Dataset/Mat2.npy\", M2)\n",
        "np.save(\"/content/drive/MyDrive/Project GCN Dataset/Mat3.npy\", M3)\n",
        "np.save(\"/content/drive/MyDrive/Project GCN Dataset/ValMat.npy\", M4)\n",
        "np.save(\"/content/drive/MyDrive/Project GCN Dataset/TestMat.npy\", M5)\n",
        "\n",
        "print(\"Saved\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Matrix\n",
            "Saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlsdUcFky3V8"
      },
      "source": [
        "# Loading Matrix\n",
        "Loading the sparce matrix in a variable A."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-x2DoB5Dy3V8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d139325f-28a9-49bc-90c5-09a588f5dd2d"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# loading matrix\n",
        "# A = np.load(\"./Matrices/Mat1.npy\")\n",
        "A1 = np.load(\"/content/drive/MyDrive/Project GCN Dataset/Mat1.npy\")\n",
        "A2 = np.load(\"/content/drive/MyDrive/Project GCN Dataset/Mat2.npy\")\n",
        "A3 = np.load(\"/content/drive/MyDrive/Project GCN Dataset/Mat3.npy\")\n",
        "A4 = np.load(\"/content/drive/MyDrive/Project GCN Dataset/ValMat.npy\")\n",
        "A5 = np.load(\"/content/drive/MyDrive/Project GCN Dataset/TestMat.npy\")\n",
        "print(\"Printing Sparse Batch 1..\")\n",
        "print(A1)\n",
        "print(\"Printing Sparse Batch 2..\")\n",
        "print(A2)\n",
        "print(\"Printing Sparse Batch 3..\")\n",
        "print(A3)\n",
        "print(\"Printing Sparse Validation Batch..\")\n",
        "print(A4)\n",
        "print(np.shape(A1))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Printing Sparse Batch 1..\n",
            "[[0 1 1 ... 0 0 0]\n",
            " [1 0 0 ... 0 0 0]\n",
            " [1 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 1]\n",
            " [0 0 0 ... 0 1 0]]\n",
            "Printing Sparse Batch 2..\n",
            "[[1 1 0 ... 0 0 0]\n",
            " [1 1 1 ... 0 0 0]\n",
            " [0 1 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 1 0]\n",
            " [0 0 0 ... 1 0 1]\n",
            " [0 0 0 ... 0 1 0]]\n",
            "Printing Sparse Batch 3..\n",
            "[[0 1 1 ... 0 0 0]\n",
            " [1 0 0 ... 0 0 0]\n",
            " [1 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 1 0]\n",
            " [0 0 0 ... 1 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "Printing Sparse Validation Batch..\n",
            "[[0 1 0 ... 0 0 0]\n",
            " [1 0 1 ... 0 0 0]\n",
            " [0 1 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 1]\n",
            " [0 0 0 ... 0 1 0]]\n",
            "(5762, 5762)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWLSs9pHtEXs"
      },
      "source": [
        "# Varifying Dataaset Batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-tPj7ADtKzc",
        "outputId": "7f9d6779-adc8-4409-9200-6c2c3d6fe5b8"
      },
      "source": [
        "t_batch_sizes = np.shape(A1)[0] + np.shape(A2)[0] + np.shape(A3)[0] + np.shape(A4)[0] + np.shape(A5)[0]\n",
        "print(\"Total batch size is: \", t_batch_sizes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total batch size is:  21097\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoEyVDoHy3V9"
      },
      "source": [
        "# Checking Sparse Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBLWJfi7y3V9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "5000e894-70b4-4735-cc69-74dc909cc552"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.matshow(A1)\n",
        "print(np.shape(A1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5762, 5762)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2FA3nxPy3V9"
      },
      "source": [
        "# Generating Labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "if0NJVfVy3V-"
      },
      "source": [
        "from numpy import nan\n",
        "\n",
        "\n",
        "def returnLabels(direc, files):\n",
        "    labels = []\n",
        "    for f in range(len(files)):\n",
        "        seenList = []\n",
        "        df = pd.read_csv(direc + files[f])\n",
        "        text = df['Object'].to_list()\n",
        "        T_labels = df['labels'].to_list()\n",
        "\n",
        "        for i in range(len(text)):\n",
        "            if(text[i] not in seenList):\n",
        "                labels.append(T_labels[i])\n",
        "            seenList.append(text[i])\n",
        "\n",
        "    return labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvHVJy9Ay3V-"
      },
      "source": [
        "# Encoding Labels\n",
        "1. Get a list of labels from CSV files.\n",
        "2. Catagorically encode the labels "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IO_taAN0y3V-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "600d9959-22d4-44ae-b1ab-c818db66e800"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "labels = returnLabels(lab_files_path, files)\n",
        "print(\"Total labels are: \", len(labels))\n",
        "\n",
        "print(\"Encoding list..\")\n",
        "lab_encoder = LabelEncoder()\n",
        "encodings = lab_encoder.fit_transform(labels)\n",
        "print(\"Total Encodings are: \", len(encodings))\n",
        "\n",
        "\n",
        "uni_enc = set(encodings)\n",
        "print(\"Unique Encodings are: \", uni_enc)\n",
        "encodings = np.transpose(encodings)\n",
        "\n",
        "# Encodings for Mat_b1 are\n",
        "e1_encodings_size = np.shape(A1)[0]\n",
        "print(\"Size of batch1 matrix is: \", e1_encodings_size)\n",
        "e1 = encodings[0:e1_encodings_size]\n",
        "print(\"Total encodings for batch1 matrix are: \", len(e1))\n",
        "\n",
        "# # Encodings for Mat_b2 are\n",
        "e2_encodings_size = np.shape(A2)[0]\n",
        "print(\"Size of batch2 matrix is: \", e1_encodings_size)\n",
        "e2 = encodings[e1_encodings_size:e2_encodings_size+e1_encodings_size]\n",
        "print(\"Total encodings for batch2 matrix are: \", len(e2))\n",
        "\n",
        "# # Encodings for Mat_b3 are\n",
        "e3_encodings_size = np.shape(A3)[0]\n",
        "start = e2_encodings_size+e1_encodings_size\n",
        "print(\"Size of batch3 matrix is: \", e1_encodings_size)\n",
        "e3 = encodings[start:start+e3_encodings_size]\n",
        "print(\"Total encodings for batch3 matrix are: \", len(e3))\n",
        "\n",
        "# # Encodings for validation set are\n",
        "e4_encodings_size = np.shape(A4)[0]\n",
        "start += e3_encodings_size\n",
        "val_enc  = encodings[start:start+e4_encodings_size]\n",
        "\n",
        "# # Encodings for test set are\n",
        "e5_encodings_size = np.shape(A5)[0]\n",
        "start += e4_encodings_size\n",
        "test_enc  = encodings[start:]\n",
        "print(\"Length of test encodings are: \", start)\n",
        "print(\"Length of test encodings are: \", len(encodings))\n",
        "\n",
        "# print(\"Total encodings for batch4 matrix are: \", len(e4))\n",
        "\n",
        "# print(\"Total length\", (len(e1) + len(e2) + len(e3) +len(e4)))\n",
        "\n",
        "# print(\"Encodings for Batch 1 Matrix is: \", set(e1))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total labels are:  21097\n",
            "Encoding list..\n",
            "Total Encodings are:  21097\n",
            "Unique Encodings are:  {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11}\n",
            "Size of batch1 matrix is:  5762\n",
            "Total encodings for batch1 matrix are:  5762\n",
            "Size of batch2 matrix is:  5762\n",
            "Total encodings for batch2 matrix are:  5410\n",
            "Size of batch3 matrix is:  5762\n",
            "Total encodings for batch3 matrix are:  9207\n",
            "Length of test encodings are:  20799\n",
            "Length of test encodings are:  21097\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5A7G3AAy3V_"
      },
      "source": [
        "# Importing Deep Learning Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2mbZufKy3WA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5584df23-c94f-4f11-911c-b88176627d46"
      },
      "source": [
        "# from tensorflow.keras import Sequential\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dropout, Dense, Flatten, Activation\n",
        "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "\n",
        "from spektral.utils.sparse import sp_matrix_to_sp_tensor\n",
        "from spektral.layers import GCNConv\n",
        "from sklearn.metrics import classification_report\n",
        "from spektral.utils import normalized_laplacian\n",
        "from spektral.utils.sparse import sp_matrix_to_sp_tensor\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "print(\"Libraries Imported..\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries Imported..\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hfuv7OC_4ReP"
      },
      "source": [
        "# Model Hyper-Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uD5lfpfxy3WA"
      },
      "source": [
        "# Hyper-parameters\n",
        "channels = 32\n",
        "dropout = 0.5\n",
        "# learning_rate = 5e-4\n",
        "l2_reg = 0.001\n",
        "learning_rate = 0.0001\n",
        "# batch_size = np.shape(A1)[0]\n",
        "batch_size = 16\n",
        "es_patience = 10\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30zBeqfsvB04"
      },
      "source": [
        "# Node Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6G_QCPhdvEW3"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def Node_Features(A):\n",
        "  # 2-Integer node-feature matrix\n",
        "  X = np.matrix([[i, -i] for i in range(A.shape[0])])\n",
        "\n",
        "  # Applying Propogation\n",
        "  # print(\"Adj. matrix after propogation..\")\n",
        "  # print(A*X)\n",
        "\n",
        "  # Self-looping to include node's own features\n",
        "  # Creating an identity of the same shape\n",
        "  I = np.matrix(np.eye(A.shape[0]))\n",
        "  # print(\"Printing Diagonal Matrix\")\n",
        "  # print(I)\n",
        "\n",
        "  A_hat = A + I\n",
        "  \n",
        "  # Applying propogation with self-looped matrix\n",
        "  # print(\"Self-looped Adj. matrix after propogation..\")\n",
        "  # print(A_hat * X)\n",
        "\n",
        "  # Calculating degree of self-looped matrix\n",
        "  D = np.array(np.sum(A_hat, axis=0))[0]\n",
        "  \n",
        "  D = np.matrix(np.diag(D))\n",
        "  \n",
        "  # Inverse of degree(D**-1) * A_hat\n",
        "  # print(\"Inverse of degree(D**-1) * A_hat..\")\n",
        "  # print((D**-1) * A_hat)\n",
        "\n",
        "  # Compiling result\n",
        "  fin = (D**-1) * A * X\n",
        "  \n",
        "  # Applying weights\n",
        "  W = np.random.randn(X.shape[1], 8) * 0.01\n",
        "  fin = (D**-1) * A * X * W\n",
        "  \n",
        "  # Result after applying relu\n",
        "  out = relu(fin)\n",
        "  return out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bG_meI_4ZSP"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkL4ykXey3WB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1c976a4-587c-4f44-8d9f-80c7f311a7e3"
      },
      "source": [
        "A = A1\n",
        "N = A.shape[0]\n",
        "b1_features = Node_Features(A)\n",
        "F = b1_features.shape[1]\n",
        "\n",
        "classes = 12\n",
        "\n",
        "fltr = normalized_laplacian(A)\n",
        "# X_in = Input(batch_size=N, shape=(F))\n",
        "X_in = Input(batch_size=None, shape=(F,))\n",
        "print(\"Shape of X-in is: \", np.shape(X_in))\n",
        "\n",
        "# A_in = Input(tensor=sp_matrix_to_sp_tensor(fltr), sparse=True)\n",
        "# A_in = Input(shape=(None,), sparse=True)\n",
        "A_in = Input(shape=(None,))\n",
        "print(\"Shape of A-in is: \", np.shape(A_in))\n",
        "\n",
        "# D1 = Dropout(dropout)(X_in)\n",
        "G1 = GCNConv(channels, activation='LeakyReLU',kernel_regularizer=l2(l2_reg), use_bias=True)([X_in, A_in])\n",
        "# G1 = GCNConv(channels, activation='LeakyReLU', use_bias=True)([X_in, A_in])\n",
        "# D2 = Dropout(dropout)(G1)\n",
        "G2 = GCNConv(channels, activation='LeakyReLU',kernel_regularizer=l2(l2_reg), use_bias=True)([G1, A_in])\n",
        "# G2 = GCNConv(channels, activation='LeakyReLU', use_bias=True)([G1, A_in])\n",
        "\n",
        "# # BN = BatchNormalization()(G2)\n",
        "flatten = Flatten()(G2)\n",
        "\n",
        "D1 = Dense(128, activation='relu')(flatten)\n",
        "Dr1 = Dropout(dropout)(D1)\n",
        "D2 = Dense(32, activation='relu')(Dr1)\n",
        "Dr2 = Dropout(dropout)(D2)\n",
        "# D3 = Dense(16, activation='LeakyReLU')(D2)\n",
        "\n",
        "D2_out = Dense(classes, activation='softmax')(Dr2)\n",
        "\n",
        "model = Model(inputs=[X_in, A_in], outputs=D2_out)\n",
        "# other loss = sparse_categorical_crossentropy\n",
        "# model.compile(optimizer='Adagrad',loss='sparse_categorical_crossentropy', weighted_metrics=['acc'])\n",
        "model.compile(optimizer='Adam',loss='sparse_categorical_crossentropy', weighted_metrics=['acc'])\n",
        "\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X-in is:  (None, 8)\n",
            "Shape of A-in is:  (None, None)\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 8)]          0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " gcn_conv (GCNConv)             (None, 32)           288         ['input_1[0][0]',                \n",
            "                                                                  'input_2[0][0]']                \n",
            "                                                                                                  \n",
            " gcn_conv_1 (GCNConv)           (None, 32)           1056        ['gcn_conv[0][0]',               \n",
            "                                                                  'input_2[0][0]']                \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 32)           0           ['gcn_conv_1[0][0]']             \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 128)          4224        ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 128)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 32)           4128        ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 32)           0           ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 12)           396         ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 10,092\n",
            "Trainable params: 10,092\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZwSwf5U4bOn"
      },
      "source": [
        "# Model-Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcLsSpDfy3WB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fba9a284-35e0-4dae-cd4e-3228602adb9b"
      },
      "source": [
        "W = {\n",
        "    0: np.random.rand(),\n",
        "    1: np.random.rand(),\n",
        "    2: np.random.rand(),\n",
        "    3: np.random.rand(),\n",
        "    4: np.random.rand(),\n",
        "    5: np.random.rand(),\n",
        "    6: np.random.rand(),\n",
        "    7: np.random.rand(),\n",
        "    8: np.random.rand(),\n",
        "    9: np.random.rand(),\n",
        "    10: np.random.rand(),\n",
        "    11: np.random.rand(),\n",
        "}\n",
        "val_fea = Node_Features(A4)\n",
        "\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10)\n",
        "\n",
        "history = model.fit([b1_features, A1],\n",
        "          e1,\n",
        "          epochs=500,\n",
        "          batch_size=N,\n",
        "          class_weight=W,\n",
        "          validation_data=([val_fea, A4], val_enc),\n",
        "          callbacks=[callback]\n",
        "        )\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "1/1 [==============================] - 1s 1s/step - loss: 8.7769 - acc: 0.2366 - val_loss: 6.9030 - val_acc: 0.0048\n",
            "Epoch 2/500\n",
            "1/1 [==============================] - 1s 514ms/step - loss: 8.1173 - acc: 0.2298 - val_loss: 6.7751 - val_acc: 0.0048\n",
            "Epoch 3/500\n",
            "1/1 [==============================] - 0s 496ms/step - loss: 7.2266 - acc: 0.2286 - val_loss: 6.6539 - val_acc: 0.0048\n",
            "Epoch 4/500\n",
            "1/1 [==============================] - 1s 544ms/step - loss: 7.1930 - acc: 0.2255 - val_loss: 6.5518 - val_acc: 0.0048\n",
            "Epoch 5/500\n",
            "1/1 [==============================] - 0s 497ms/step - loss: 7.0154 - acc: 0.2184 - val_loss: 6.4653 - val_acc: 0.0048\n",
            "Epoch 6/500\n",
            "1/1 [==============================] - 1s 504ms/step - loss: 6.4169 - acc: 0.2058 - val_loss: 6.3923 - val_acc: 0.0024\n",
            "Epoch 7/500\n",
            "1/1 [==============================] - 0s 497ms/step - loss: 6.1215 - acc: 0.2141 - val_loss: 6.3298 - val_acc: 0.0024\n",
            "Epoch 8/500\n",
            "1/1 [==============================] - 1s 507ms/step - loss: 5.6089 - acc: 0.5705 - val_loss: 6.2758 - val_acc: 0.0000e+00\n",
            "Epoch 9/500\n",
            "1/1 [==============================] - 0s 488ms/step - loss: 5.6055 - acc: 0.5900 - val_loss: 6.2300 - val_acc: 0.0000e+00\n",
            "Epoch 10/500\n",
            "1/1 [==============================] - 1s 509ms/step - loss: 5.8137 - acc: 0.5990 - val_loss: 6.1903 - val_acc: 0.0000e+00\n",
            "Epoch 11/500\n",
            "1/1 [==============================] - 1s 501ms/step - loss: 4.9904 - acc: 0.6096 - val_loss: 6.1547 - val_acc: 0.0000e+00\n",
            "Epoch 12/500\n",
            "1/1 [==============================] - 0s 495ms/step - loss: 4.6725 - acc: 0.6328 - val_loss: 6.1234 - val_acc: 0.0000e+00\n",
            "Epoch 13/500\n",
            "1/1 [==============================] - 0s 497ms/step - loss: 4.5649 - acc: 0.6436 - val_loss: 6.0864 - val_acc: 0.0000e+00\n",
            "Epoch 14/500\n",
            "1/1 [==============================] - 1s 502ms/step - loss: 4.4271 - acc: 0.6537 - val_loss: 6.0532 - val_acc: 0.0000e+00\n",
            "Epoch 15/500\n",
            "1/1 [==============================] - 1s 504ms/step - loss: 4.4181 - acc: 0.6628 - val_loss: 6.0221 - val_acc: 0.0000e+00\n",
            "Epoch 16/500\n",
            "1/1 [==============================] - 0s 498ms/step - loss: 4.3117 - acc: 0.6670 - val_loss: 5.9931 - val_acc: 0.0024\n",
            "Epoch 17/500\n",
            "1/1 [==============================] - 0s 487ms/step - loss: 4.0393 - acc: 0.6828 - val_loss: 5.9659 - val_acc: 0.0024\n",
            "Epoch 18/500\n",
            "1/1 [==============================] - 1s 520ms/step - loss: 3.8852 - acc: 0.7030 - val_loss: 5.9401 - val_acc: 0.0024\n",
            "Epoch 19/500\n",
            "1/1 [==============================] - 0s 483ms/step - loss: 4.0476 - acc: 0.7039 - val_loss: 5.9149 - val_acc: 0.0024\n",
            "Epoch 20/500\n",
            "1/1 [==============================] - 0s 499ms/step - loss: 3.8456 - acc: 0.7256 - val_loss: 5.8909 - val_acc: 0.0024\n",
            "Epoch 21/500\n",
            "1/1 [==============================] - 0s 494ms/step - loss: 3.6407 - acc: 0.7301 - val_loss: 5.8690 - val_acc: 0.0405\n",
            "Epoch 22/500\n",
            "1/1 [==============================] - 0s 495ms/step - loss: 3.5601 - acc: 0.7345 - val_loss: 5.8482 - val_acc: 0.0524\n",
            "Epoch 23/500\n",
            "1/1 [==============================] - 0s 488ms/step - loss: 3.6435 - acc: 0.7371 - val_loss: 5.8290 - val_acc: 0.0524\n",
            "Epoch 24/500\n",
            "1/1 [==============================] - 1s 508ms/step - loss: 3.3859 - acc: 0.7358 - val_loss: 5.8105 - val_acc: 0.0524\n",
            "Epoch 25/500\n",
            "1/1 [==============================] - 1s 518ms/step - loss: 3.6427 - acc: 0.7377 - val_loss: 5.7933 - val_acc: 0.0524\n",
            "Epoch 26/500\n",
            "1/1 [==============================] - 1s 512ms/step - loss: 3.3998 - acc: 0.7414 - val_loss: 5.7779 - val_acc: 0.0524\n",
            "Epoch 27/500\n",
            "1/1 [==============================] - 0s 491ms/step - loss: 3.3868 - acc: 0.7459 - val_loss: 5.7636 - val_acc: 0.0524\n",
            "Epoch 28/500\n",
            "1/1 [==============================] - 0s 493ms/step - loss: 3.3114 - acc: 0.7495 - val_loss: 5.7491 - val_acc: 0.0524\n",
            "Epoch 29/500\n",
            "1/1 [==============================] - 0s 499ms/step - loss: 3.2844 - acc: 0.7515 - val_loss: 5.7362 - val_acc: 0.0524\n",
            "Epoch 30/500\n",
            "1/1 [==============================] - 1s 527ms/step - loss: 3.2053 - acc: 0.7526 - val_loss: 5.7237 - val_acc: 0.0524\n",
            "Epoch 31/500\n",
            "1/1 [==============================] - 1s 509ms/step - loss: 2.8677 - acc: 0.7627 - val_loss: 5.7117 - val_acc: 0.0524\n",
            "Epoch 32/500\n",
            "1/1 [==============================] - 1s 515ms/step - loss: 3.0412 - acc: 0.7578 - val_loss: 5.7003 - val_acc: 0.0548\n",
            "Epoch 33/500\n",
            "1/1 [==============================] - 1s 501ms/step - loss: 3.0000 - acc: 0.7678 - val_loss: 5.6891 - val_acc: 0.0548\n",
            "Epoch 34/500\n",
            "1/1 [==============================] - 1s 517ms/step - loss: 3.3326 - acc: 0.7565 - val_loss: 5.6786 - val_acc: 0.0548\n",
            "Epoch 35/500\n",
            "1/1 [==============================] - 1s 514ms/step - loss: 2.8675 - acc: 0.7638 - val_loss: 5.6690 - val_acc: 0.0548\n",
            "Epoch 36/500\n",
            "1/1 [==============================] - 1s 518ms/step - loss: 2.8862 - acc: 0.7675 - val_loss: 5.6588 - val_acc: 0.0548\n",
            "Epoch 37/500\n",
            "1/1 [==============================] - 1s 509ms/step - loss: 2.8861 - acc: 0.7635 - val_loss: 5.6487 - val_acc: 0.0548\n",
            "Epoch 38/500\n",
            "1/1 [==============================] - 1s 518ms/step - loss: 2.7638 - acc: 0.7750 - val_loss: 5.6392 - val_acc: 0.0548\n",
            "Epoch 39/500\n",
            "1/1 [==============================] - 1s 509ms/step - loss: 2.8961 - acc: 0.7778 - val_loss: 5.6297 - val_acc: 0.0548\n",
            "Epoch 40/500\n",
            "1/1 [==============================] - 1s 506ms/step - loss: 2.8785 - acc: 0.7781 - val_loss: 5.6202 - val_acc: 0.0548\n",
            "Epoch 41/500\n",
            "1/1 [==============================] - 1s 501ms/step - loss: 2.6692 - acc: 0.7764 - val_loss: 5.6107 - val_acc: 0.0548\n",
            "Epoch 42/500\n",
            "1/1 [==============================] - 1s 505ms/step - loss: 2.6677 - acc: 0.7788 - val_loss: 5.6011 - val_acc: 0.0548\n",
            "Epoch 43/500\n",
            "1/1 [==============================] - 1s 502ms/step - loss: 2.9296 - acc: 0.7810 - val_loss: 5.5930 - val_acc: 0.0548\n",
            "Epoch 44/500\n",
            "1/1 [==============================] - 1s 501ms/step - loss: 2.6970 - acc: 0.7821 - val_loss: 5.5857 - val_acc: 0.0548\n",
            "Epoch 45/500\n",
            "1/1 [==============================] - 0s 499ms/step - loss: 2.7027 - acc: 0.7818 - val_loss: 5.5786 - val_acc: 0.0548\n",
            "Epoch 46/500\n",
            "1/1 [==============================] - 1s 550ms/step - loss: 2.6543 - acc: 0.7870 - val_loss: 5.5723 - val_acc: 0.0548\n",
            "Epoch 47/500\n",
            "1/1 [==============================] - 1s 516ms/step - loss: 2.7761 - acc: 0.7892 - val_loss: 5.5660 - val_acc: 0.0548\n",
            "Epoch 48/500\n",
            "1/1 [==============================] - 1s 515ms/step - loss: 2.6717 - acc: 0.7866 - val_loss: 5.5601 - val_acc: 0.0548\n",
            "Epoch 49/500\n",
            "1/1 [==============================] - 1s 518ms/step - loss: 2.5643 - acc: 0.7892 - val_loss: 5.5543 - val_acc: 0.0548\n",
            "Epoch 50/500\n",
            "1/1 [==============================] - 1s 513ms/step - loss: 2.4937 - acc: 0.7861 - val_loss: 5.5488 - val_acc: 0.0548\n",
            "Epoch 51/500\n",
            "1/1 [==============================] - 1s 513ms/step - loss: 2.6101 - acc: 0.7886 - val_loss: 5.5440 - val_acc: 0.0548\n",
            "Epoch 52/500\n",
            "1/1 [==============================] - 1s 528ms/step - loss: 2.6435 - acc: 0.7907 - val_loss: 5.5397 - val_acc: 0.0548\n",
            "Epoch 53/500\n",
            "1/1 [==============================] - 1s 523ms/step - loss: 2.5824 - acc: 0.7919 - val_loss: 5.5360 - val_acc: 0.0548\n",
            "Epoch 54/500\n",
            "1/1 [==============================] - 1s 531ms/step - loss: 2.5304 - acc: 0.7907 - val_loss: 5.5323 - val_acc: 0.0548\n",
            "Epoch 55/500\n",
            "1/1 [==============================] - 1s 522ms/step - loss: 2.5221 - acc: 0.7954 - val_loss: 5.5296 - val_acc: 0.0548\n",
            "Epoch 56/500\n",
            "1/1 [==============================] - 1s 519ms/step - loss: 2.5473 - acc: 0.7933 - val_loss: 5.5271 - val_acc: 0.0548\n",
            "Epoch 57/500\n",
            "1/1 [==============================] - 1s 501ms/step - loss: 2.4110 - acc: 0.7935 - val_loss: 5.5248 - val_acc: 0.0548\n",
            "Epoch 58/500\n",
            "1/1 [==============================] - 1s 513ms/step - loss: 2.7128 - acc: 0.7887 - val_loss: 5.5228 - val_acc: 0.0548\n",
            "Epoch 59/500\n",
            "1/1 [==============================] - 1s 510ms/step - loss: 2.4431 - acc: 0.7952 - val_loss: 5.5207 - val_acc: 0.0548\n",
            "Epoch 60/500\n",
            "1/1 [==============================] - 1s 508ms/step - loss: 2.4308 - acc: 0.7978 - val_loss: 5.5180 - val_acc: 0.0548\n",
            "Epoch 61/500\n",
            "1/1 [==============================] - 0s 489ms/step - loss: 2.4693 - acc: 0.7952 - val_loss: 5.5144 - val_acc: 0.0548\n",
            "Epoch 62/500\n",
            "1/1 [==============================] - 1s 514ms/step - loss: 2.5506 - acc: 0.7881 - val_loss: 5.5096 - val_acc: 0.0548\n",
            "Epoch 63/500\n",
            "1/1 [==============================] - 0s 495ms/step - loss: 2.3379 - acc: 0.7979 - val_loss: 5.5041 - val_acc: 0.0548\n",
            "Epoch 64/500\n",
            "1/1 [==============================] - 1s 513ms/step - loss: 2.3397 - acc: 0.7945 - val_loss: 5.4980 - val_acc: 0.0548\n",
            "Epoch 65/500\n",
            "1/1 [==============================] - 1s 508ms/step - loss: 2.5245 - acc: 0.7980 - val_loss: 5.4918 - val_acc: 0.0548\n",
            "Epoch 66/500\n",
            "1/1 [==============================] - 1s 566ms/step - loss: 2.3277 - acc: 0.7984 - val_loss: 5.4854 - val_acc: 0.0548\n",
            "Epoch 67/500\n",
            "1/1 [==============================] - 1s 509ms/step - loss: 2.3878 - acc: 0.7988 - val_loss: 5.4798 - val_acc: 0.0548\n",
            "Epoch 68/500\n",
            "1/1 [==============================] - 1s 519ms/step - loss: 2.4005 - acc: 0.7995 - val_loss: 5.4742 - val_acc: 0.0548\n",
            "Epoch 69/500\n",
            "1/1 [==============================] - 1s 501ms/step - loss: 2.3890 - acc: 0.7975 - val_loss: 5.4679 - val_acc: 0.0548\n",
            "Epoch 70/500\n",
            "1/1 [==============================] - 1s 528ms/step - loss: 2.4186 - acc: 0.7972 - val_loss: 5.4611 - val_acc: 0.0548\n",
            "Epoch 71/500\n",
            "1/1 [==============================] - 1s 508ms/step - loss: 2.3403 - acc: 0.8037 - val_loss: 5.4554 - val_acc: 0.0548\n",
            "Epoch 72/500\n",
            "1/1 [==============================] - 1s 531ms/step - loss: 2.3652 - acc: 0.8006 - val_loss: 5.4487 - val_acc: 0.0571\n",
            "Epoch 73/500\n",
            "1/1 [==============================] - 1s 505ms/step - loss: 2.3642 - acc: 0.7992 - val_loss: 5.4421 - val_acc: 0.0571\n",
            "Epoch 74/500\n",
            "1/1 [==============================] - 1s 532ms/step - loss: 2.2902 - acc: 0.8006 - val_loss: 5.4346 - val_acc: 0.0571\n",
            "Epoch 75/500\n",
            "1/1 [==============================] - 1s 523ms/step - loss: 2.3545 - acc: 0.8019 - val_loss: 5.4273 - val_acc: 0.0571\n",
            "Epoch 76/500\n",
            "1/1 [==============================] - 1s 534ms/step - loss: 2.2229 - acc: 0.8045 - val_loss: 5.4202 - val_acc: 0.0595\n",
            "Epoch 77/500\n",
            "1/1 [==============================] - 0s 498ms/step - loss: 2.1636 - acc: 0.8067 - val_loss: 5.4130 - val_acc: 0.0595\n",
            "Epoch 78/500\n",
            "1/1 [==============================] - 1s 538ms/step - loss: 2.2748 - acc: 0.8041 - val_loss: 5.4059 - val_acc: 0.0595\n",
            "Epoch 79/500\n",
            "1/1 [==============================] - 1s 523ms/step - loss: 2.3018 - acc: 0.8058 - val_loss: 5.3972 - val_acc: 0.0595\n",
            "Epoch 80/500\n",
            "1/1 [==============================] - 1s 508ms/step - loss: 2.2304 - acc: 0.8043 - val_loss: 5.3891 - val_acc: 0.0619\n",
            "Epoch 81/500\n",
            "1/1 [==============================] - 1s 512ms/step - loss: 2.2410 - acc: 0.8006 - val_loss: 5.3797 - val_acc: 0.0619\n",
            "Epoch 82/500\n",
            "1/1 [==============================] - 0s 492ms/step - loss: 2.2411 - acc: 0.8061 - val_loss: 5.3687 - val_acc: 0.0619\n",
            "Epoch 83/500\n",
            "1/1 [==============================] - 1s 509ms/step - loss: 2.2461 - acc: 0.8086 - val_loss: 5.3579 - val_acc: 0.0619\n",
            "Epoch 84/500\n",
            "1/1 [==============================] - 1s 507ms/step - loss: 2.2935 - acc: 0.8074 - val_loss: 5.3472 - val_acc: 0.0619\n",
            "Epoch 85/500\n",
            "1/1 [==============================] - 1s 512ms/step - loss: 2.2563 - acc: 0.8070 - val_loss: 5.3367 - val_acc: 0.0619\n",
            "Epoch 86/500\n",
            "1/1 [==============================] - 0s 497ms/step - loss: 2.2135 - acc: 0.8077 - val_loss: 5.3263 - val_acc: 0.0619\n",
            "Epoch 87/500\n",
            "1/1 [==============================] - 1s 546ms/step - loss: 2.2220 - acc: 0.8087 - val_loss: 5.3159 - val_acc: 0.0643\n",
            "Epoch 88/500\n",
            "1/1 [==============================] - 0s 494ms/step - loss: 2.2375 - acc: 0.8039 - val_loss: 5.3061 - val_acc: 0.0643\n",
            "Epoch 89/500\n",
            "1/1 [==============================] - 1s 514ms/step - loss: 2.1282 - acc: 0.8061 - val_loss: 5.2964 - val_acc: 0.0643\n",
            "Epoch 90/500\n",
            "1/1 [==============================] - 1s 503ms/step - loss: 2.1798 - acc: 0.8097 - val_loss: 5.2869 - val_acc: 0.0643\n",
            "Epoch 91/500\n",
            "1/1 [==============================] - 1s 505ms/step - loss: 2.1812 - acc: 0.8090 - val_loss: 5.2764 - val_acc: 0.0643\n",
            "Epoch 92/500\n",
            "1/1 [==============================] - 0s 494ms/step - loss: 2.2567 - acc: 0.8038 - val_loss: 5.2664 - val_acc: 0.0643\n",
            "Epoch 93/500\n",
            "1/1 [==============================] - 1s 502ms/step - loss: 2.1822 - acc: 0.8079 - val_loss: 5.2566 - val_acc: 0.0643\n",
            "Epoch 94/500\n",
            "1/1 [==============================] - 0s 490ms/step - loss: 2.1834 - acc: 0.8082 - val_loss: 5.2460 - val_acc: 0.0643\n",
            "Epoch 95/500\n",
            "1/1 [==============================] - 1s 522ms/step - loss: 2.1848 - acc: 0.8133 - val_loss: 5.2349 - val_acc: 0.0643\n",
            "Epoch 96/500\n",
            "1/1 [==============================] - 1s 509ms/step - loss: 2.1030 - acc: 0.8108 - val_loss: 5.2221 - val_acc: 0.0643\n",
            "Epoch 97/500\n",
            "1/1 [==============================] - 1s 540ms/step - loss: 2.1268 - acc: 0.8097 - val_loss: 5.2085 - val_acc: 0.0643\n",
            "Epoch 98/500\n",
            "1/1 [==============================] - 1s 501ms/step - loss: 2.0814 - acc: 0.8090 - val_loss: 5.1956 - val_acc: 0.0643\n",
            "Epoch 99/500\n",
            "1/1 [==============================] - 1s 507ms/step - loss: 2.1059 - acc: 0.8099 - val_loss: 5.1825 - val_acc: 0.0643\n",
            "Epoch 100/500\n",
            "1/1 [==============================] - 1s 511ms/step - loss: 2.1036 - acc: 0.8127 - val_loss: 5.1698 - val_acc: 0.0643\n",
            "Epoch 101/500\n",
            "1/1 [==============================] - 1s 501ms/step - loss: 2.1879 - acc: 0.8119 - val_loss: 5.1573 - val_acc: 0.0643\n",
            "Epoch 102/500\n",
            "1/1 [==============================] - 0s 490ms/step - loss: 2.1269 - acc: 0.8130 - val_loss: 5.1455 - val_acc: 0.0643\n",
            "Epoch 103/500\n",
            "1/1 [==============================] - 1s 517ms/step - loss: 2.0937 - acc: 0.8152 - val_loss: 5.1338 - val_acc: 0.0643\n",
            "Epoch 104/500\n",
            "1/1 [==============================] - 0s 495ms/step - loss: 2.0747 - acc: 0.8167 - val_loss: 5.1221 - val_acc: 0.0643\n",
            "Epoch 105/500\n",
            "1/1 [==============================] - 1s 524ms/step - loss: 2.1807 - acc: 0.8162 - val_loss: 5.1112 - val_acc: 0.0667\n",
            "Epoch 106/500\n",
            "1/1 [==============================] - 1s 501ms/step - loss: 2.0973 - acc: 0.8133 - val_loss: 5.1014 - val_acc: 0.0667\n",
            "Epoch 107/500\n",
            "1/1 [==============================] - 1s 516ms/step - loss: 2.1456 - acc: 0.8182 - val_loss: 5.0920 - val_acc: 0.0667\n",
            "Epoch 108/500\n",
            "1/1 [==============================] - 1s 521ms/step - loss: 2.1147 - acc: 0.8126 - val_loss: 5.0822 - val_acc: 0.0667\n",
            "Epoch 109/500\n",
            "1/1 [==============================] - 1s 532ms/step - loss: 2.0837 - acc: 0.8180 - val_loss: 5.0729 - val_acc: 0.0667\n",
            "Epoch 110/500\n",
            "1/1 [==============================] - 1s 548ms/step - loss: 2.0767 - acc: 0.8147 - val_loss: 5.0639 - val_acc: 0.0667\n",
            "Epoch 111/500\n",
            "1/1 [==============================] - 1s 524ms/step - loss: 2.0909 - acc: 0.8141 - val_loss: 5.0546 - val_acc: 0.0667\n",
            "Epoch 112/500\n",
            "1/1 [==============================] - 1s 536ms/step - loss: 2.0876 - acc: 0.8144 - val_loss: 5.0445 - val_acc: 0.0667\n",
            "Epoch 113/500\n",
            "1/1 [==============================] - 1s 523ms/step - loss: 2.1282 - acc: 0.8175 - val_loss: 5.0337 - val_acc: 0.0667\n",
            "Epoch 114/500\n",
            "1/1 [==============================] - 1s 516ms/step - loss: 2.0273 - acc: 0.8193 - val_loss: 5.0237 - val_acc: 0.0667\n",
            "Epoch 115/500\n",
            "1/1 [==============================] - 1s 513ms/step - loss: 2.0829 - acc: 0.8175 - val_loss: 5.0136 - val_acc: 0.0667\n",
            "Epoch 116/500\n",
            "1/1 [==============================] - 1s 519ms/step - loss: 2.1311 - acc: 0.8174 - val_loss: 5.0039 - val_acc: 0.0667\n",
            "Epoch 117/500\n",
            "1/1 [==============================] - 1s 522ms/step - loss: 2.0361 - acc: 0.8199 - val_loss: 4.9944 - val_acc: 0.0667\n",
            "Epoch 118/500\n",
            "1/1 [==============================] - 1s 507ms/step - loss: 2.0777 - acc: 0.8212 - val_loss: 4.9849 - val_acc: 0.0667\n",
            "Epoch 119/500\n",
            "1/1 [==============================] - 1s 512ms/step - loss: 2.0152 - acc: 0.8187 - val_loss: 4.9763 - val_acc: 0.0667\n",
            "Epoch 120/500\n",
            "1/1 [==============================] - 1s 503ms/step - loss: 2.0125 - acc: 0.8186 - val_loss: 4.9675 - val_acc: 0.0667\n",
            "Epoch 121/500\n",
            "1/1 [==============================] - 1s 506ms/step - loss: 2.0446 - acc: 0.8214 - val_loss: 4.9595 - val_acc: 0.0667\n",
            "Epoch 122/500\n",
            "1/1 [==============================] - 1s 505ms/step - loss: 1.9736 - acc: 0.8208 - val_loss: 4.9520 - val_acc: 0.0667\n",
            "Epoch 123/500\n",
            "1/1 [==============================] - 1s 531ms/step - loss: 1.9842 - acc: 0.8225 - val_loss: 4.9443 - val_acc: 0.0667\n",
            "Epoch 124/500\n",
            "1/1 [==============================] - 1s 523ms/step - loss: 1.9785 - acc: 0.8208 - val_loss: 4.9367 - val_acc: 0.0667\n",
            "Epoch 125/500\n",
            "1/1 [==============================] - 1s 511ms/step - loss: 2.0290 - acc: 0.8217 - val_loss: 4.9297 - val_acc: 0.0667\n",
            "Epoch 126/500\n",
            "1/1 [==============================] - 0s 491ms/step - loss: 2.0319 - acc: 0.8215 - val_loss: 4.9230 - val_acc: 0.0667\n",
            "Epoch 127/500\n",
            "1/1 [==============================] - 1s 505ms/step - loss: 1.9594 - acc: 0.8218 - val_loss: 4.9162 - val_acc: 0.0667\n",
            "Epoch 128/500\n",
            "1/1 [==============================] - 1s 501ms/step - loss: 1.9538 - acc: 0.8228 - val_loss: 4.9098 - val_acc: 0.0667\n",
            "Epoch 129/500\n",
            "1/1 [==============================] - 1s 510ms/step - loss: 2.0053 - acc: 0.8193 - val_loss: 4.9038 - val_acc: 0.0667\n",
            "Epoch 130/500\n",
            "1/1 [==============================] - 1s 523ms/step - loss: 1.9715 - acc: 0.8223 - val_loss: 4.8976 - val_acc: 0.0667\n",
            "Epoch 131/500\n",
            "1/1 [==============================] - 1s 510ms/step - loss: 1.9652 - acc: 0.8206 - val_loss: 4.8907 - val_acc: 0.0667\n",
            "Epoch 132/500\n",
            "1/1 [==============================] - 1s 508ms/step - loss: 2.0591 - acc: 0.8198 - val_loss: 4.8839 - val_acc: 0.0667\n",
            "Epoch 133/500\n",
            "1/1 [==============================] - 1s 518ms/step - loss: 1.9502 - acc: 0.8207 - val_loss: 4.8771 - val_acc: 0.0667\n",
            "Epoch 134/500\n",
            "1/1 [==============================] - 1s 516ms/step - loss: 1.9775 - acc: 0.8225 - val_loss: 4.8706 - val_acc: 0.0667\n",
            "Epoch 135/500\n",
            "1/1 [==============================] - 1s 538ms/step - loss: 1.9761 - acc: 0.8248 - val_loss: 4.8643 - val_acc: 0.0667\n",
            "Epoch 136/500\n",
            "1/1 [==============================] - 1s 511ms/step - loss: 1.9251 - acc: 0.8193 - val_loss: 4.8585 - val_acc: 0.0667\n",
            "Epoch 137/500\n",
            "1/1 [==============================] - 1s 522ms/step - loss: 1.9737 - acc: 0.8230 - val_loss: 4.8528 - val_acc: 0.0667\n",
            "Epoch 138/500\n",
            "1/1 [==============================] - 1s 519ms/step - loss: 1.8823 - acc: 0.8230 - val_loss: 4.8468 - val_acc: 0.0667\n",
            "Epoch 139/500\n",
            "1/1 [==============================] - 1s 520ms/step - loss: 2.0466 - acc: 0.8219 - val_loss: 4.8399 - val_acc: 0.0667\n",
            "Epoch 140/500\n",
            "1/1 [==============================] - 1s 515ms/step - loss: 1.9316 - acc: 0.8234 - val_loss: 4.8331 - val_acc: 0.0667\n",
            "Epoch 141/500\n",
            "1/1 [==============================] - 1s 513ms/step - loss: 1.8959 - acc: 0.8238 - val_loss: 4.8255 - val_acc: 0.0667\n",
            "Epoch 142/500\n",
            "1/1 [==============================] - 1s 523ms/step - loss: 1.9162 - acc: 0.8256 - val_loss: 4.8183 - val_acc: 0.0667\n",
            "Epoch 143/500\n",
            "1/1 [==============================] - 1s 543ms/step - loss: 1.9280 - acc: 0.8260 - val_loss: 4.8113 - val_acc: 0.0667\n",
            "Epoch 144/500\n",
            "1/1 [==============================] - 1s 522ms/step - loss: 1.9057 - acc: 0.8240 - val_loss: 4.8043 - val_acc: 0.0667\n",
            "Epoch 145/500\n",
            "1/1 [==============================] - 1s 525ms/step - loss: 1.9183 - acc: 0.8246 - val_loss: 4.7978 - val_acc: 0.0667\n",
            "Epoch 146/500\n",
            "1/1 [==============================] - 1s 516ms/step - loss: 1.9024 - acc: 0.8259 - val_loss: 4.7911 - val_acc: 0.0667\n",
            "Epoch 147/500\n",
            "1/1 [==============================] - 1s 526ms/step - loss: 1.9541 - acc: 0.8223 - val_loss: 4.7842 - val_acc: 0.0667\n",
            "Epoch 148/500\n",
            "1/1 [==============================] - 1s 508ms/step - loss: 1.9308 - acc: 0.8267 - val_loss: 4.7779 - val_acc: 0.0667\n",
            "Epoch 149/500\n",
            "1/1 [==============================] - 1s 522ms/step - loss: 1.9236 - acc: 0.8255 - val_loss: 4.7718 - val_acc: 0.0667\n",
            "Epoch 150/500\n",
            "1/1 [==============================] - 1s 551ms/step - loss: 1.9640 - acc: 0.8245 - val_loss: 4.7663 - val_acc: 0.0667\n",
            "Epoch 151/500\n",
            "1/1 [==============================] - 1s 508ms/step - loss: 1.8782 - acc: 0.8260 - val_loss: 4.7606 - val_acc: 0.0690\n",
            "Epoch 152/500\n",
            "1/1 [==============================] - 1s 521ms/step - loss: 1.8982 - acc: 0.8260 - val_loss: 4.7550 - val_acc: 0.0690\n",
            "Epoch 153/500\n",
            "1/1 [==============================] - 1s 502ms/step - loss: 1.9368 - acc: 0.8238 - val_loss: 4.7504 - val_acc: 0.0690\n",
            "Epoch 154/500\n",
            "1/1 [==============================] - 1s 517ms/step - loss: 1.8930 - acc: 0.8258 - val_loss: 4.7459 - val_acc: 0.0690\n",
            "Epoch 155/500\n",
            "1/1 [==============================] - 1s 513ms/step - loss: 1.9023 - acc: 0.8255 - val_loss: 4.7419 - val_acc: 0.0690\n",
            "Epoch 156/500\n",
            "1/1 [==============================] - 1s 507ms/step - loss: 1.8951 - acc: 0.8259 - val_loss: 4.7381 - val_acc: 0.0690\n",
            "Epoch 157/500\n",
            "1/1 [==============================] - 1s 513ms/step - loss: 1.8505 - acc: 0.8292 - val_loss: 4.7343 - val_acc: 0.0690\n",
            "Epoch 158/500\n",
            "1/1 [==============================] - 0s 498ms/step - loss: 1.8722 - acc: 0.8254 - val_loss: 4.7303 - val_acc: 0.0690\n",
            "Epoch 159/500\n",
            "1/1 [==============================] - 0s 489ms/step - loss: 1.9109 - acc: 0.8229 - val_loss: 4.7262 - val_acc: 0.0690\n",
            "Epoch 160/500\n",
            "1/1 [==============================] - 0s 495ms/step - loss: 1.8579 - acc: 0.8253 - val_loss: 4.7221 - val_acc: 0.0690\n",
            "Epoch 161/500\n",
            "1/1 [==============================] - 1s 500ms/step - loss: 1.9322 - acc: 0.8261 - val_loss: 4.7179 - val_acc: 0.0714\n",
            "Epoch 162/500\n",
            "1/1 [==============================] - 1s 518ms/step - loss: 1.9298 - acc: 0.8290 - val_loss: 4.7144 - val_acc: 0.0714\n",
            "Epoch 163/500\n",
            "1/1 [==============================] - 1s 506ms/step - loss: 1.8931 - acc: 0.8273 - val_loss: 4.7104 - val_acc: 0.0714\n",
            "Epoch 164/500\n",
            "1/1 [==============================] - 1s 520ms/step - loss: 1.8970 - acc: 0.8280 - val_loss: 4.7062 - val_acc: 0.0714\n",
            "Epoch 165/500\n",
            "1/1 [==============================] - 1s 510ms/step - loss: 1.9009 - acc: 0.8258 - val_loss: 4.7022 - val_acc: 0.0714\n",
            "Epoch 166/500\n",
            "1/1 [==============================] - 1s 517ms/step - loss: 1.8746 - acc: 0.8263 - val_loss: 4.6980 - val_acc: 0.0714\n",
            "Epoch 167/500\n",
            "1/1 [==============================] - 1s 508ms/step - loss: 1.8781 - acc: 0.8258 - val_loss: 4.6931 - val_acc: 0.0714\n",
            "Epoch 168/500\n",
            "1/1 [==============================] - 1s 540ms/step - loss: 1.8548 - acc: 0.8265 - val_loss: 4.6885 - val_acc: 0.0714\n",
            "Epoch 169/500\n",
            "1/1 [==============================] - 1s 527ms/step - loss: 1.8987 - acc: 0.8277 - val_loss: 4.6839 - val_acc: 0.0714\n",
            "Epoch 170/500\n",
            "1/1 [==============================] - 1s 556ms/step - loss: 1.8478 - acc: 0.8269 - val_loss: 4.6792 - val_acc: 0.0714\n",
            "Epoch 171/500\n",
            "1/1 [==============================] - 1s 541ms/step - loss: 1.8542 - acc: 0.8264 - val_loss: 4.6742 - val_acc: 0.0714\n",
            "Epoch 172/500\n",
            "1/1 [==============================] - 1s 516ms/step - loss: 1.8256 - acc: 0.8277 - val_loss: 4.6685 - val_acc: 0.0714\n",
            "Epoch 173/500\n",
            "1/1 [==============================] - 1s 503ms/step - loss: 1.8741 - acc: 0.8268 - val_loss: 4.6628 - val_acc: 0.0714\n",
            "Epoch 174/500\n",
            "1/1 [==============================] - 1s 524ms/step - loss: 1.8428 - acc: 0.8259 - val_loss: 4.6570 - val_acc: 0.0714\n",
            "Epoch 175/500\n",
            "1/1 [==============================] - 1s 529ms/step - loss: 1.8569 - acc: 0.8259 - val_loss: 4.6513 - val_acc: 0.0714\n",
            "Epoch 176/500\n",
            "1/1 [==============================] - 1s 539ms/step - loss: 1.8252 - acc: 0.8265 - val_loss: 4.6458 - val_acc: 0.0714\n",
            "Epoch 177/500\n",
            "1/1 [==============================] - 1s 535ms/step - loss: 1.8857 - acc: 0.8264 - val_loss: 4.6401 - val_acc: 0.0714\n",
            "Epoch 178/500\n",
            "1/1 [==============================] - 1s 544ms/step - loss: 1.8581 - acc: 0.8299 - val_loss: 4.6342 - val_acc: 0.0714\n",
            "Epoch 179/500\n",
            "1/1 [==============================] - 1s 514ms/step - loss: 1.8113 - acc: 0.8283 - val_loss: 4.6287 - val_acc: 0.0714\n",
            "Epoch 180/500\n",
            "1/1 [==============================] - 1s 520ms/step - loss: 1.8383 - acc: 0.8285 - val_loss: 4.6225 - val_acc: 0.0714\n",
            "Epoch 181/500\n",
            "1/1 [==============================] - 1s 518ms/step - loss: 1.8854 - acc: 0.8287 - val_loss: 4.6164 - val_acc: 0.0714\n",
            "Epoch 182/500\n",
            "1/1 [==============================] - 1s 526ms/step - loss: 1.8345 - acc: 0.8285 - val_loss: 4.6106 - val_acc: 0.0714\n",
            "Epoch 183/500\n",
            "1/1 [==============================] - 1s 531ms/step - loss: 1.8139 - acc: 0.8307 - val_loss: 4.6052 - val_acc: 0.0714\n",
            "Epoch 184/500\n",
            "1/1 [==============================] - 1s 515ms/step - loss: 1.8222 - acc: 0.8279 - val_loss: 4.5993 - val_acc: 0.0714\n",
            "Epoch 185/500\n",
            "1/1 [==============================] - 1s 533ms/step - loss: 1.7969 - acc: 0.8304 - val_loss: 4.5935 - val_acc: 0.0714\n",
            "Epoch 186/500\n",
            "1/1 [==============================] - 0s 496ms/step - loss: 1.8584 - acc: 0.8292 - val_loss: 4.5879 - val_acc: 0.0714\n",
            "Epoch 187/500\n",
            "1/1 [==============================] - 1s 514ms/step - loss: 1.8169 - acc: 0.8321 - val_loss: 4.5823 - val_acc: 0.0714\n",
            "Epoch 188/500\n",
            "1/1 [==============================] - 1s 510ms/step - loss: 1.8528 - acc: 0.8268 - val_loss: 4.5765 - val_acc: 0.0762\n",
            "Epoch 189/500\n",
            "1/1 [==============================] - 1s 527ms/step - loss: 1.8006 - acc: 0.8275 - val_loss: 4.5707 - val_acc: 0.0762\n",
            "Epoch 190/500\n",
            "1/1 [==============================] - 0s 500ms/step - loss: 1.8548 - acc: 0.8274 - val_loss: 4.5646 - val_acc: 0.0762\n",
            "Epoch 191/500\n",
            "1/1 [==============================] - 1s 546ms/step - loss: 1.8528 - acc: 0.8282 - val_loss: 4.5584 - val_acc: 0.0762\n",
            "Epoch 192/500\n",
            "1/1 [==============================] - 1s 537ms/step - loss: 1.8553 - acc: 0.8295 - val_loss: 4.5523 - val_acc: 0.0762\n",
            "Epoch 193/500\n",
            "1/1 [==============================] - 1s 529ms/step - loss: 1.8384 - acc: 0.8272 - val_loss: 4.5461 - val_acc: 0.0762\n",
            "Epoch 194/500\n",
            "1/1 [==============================] - 0s 496ms/step - loss: 1.8149 - acc: 0.8276 - val_loss: 4.5400 - val_acc: 0.0762\n",
            "Epoch 195/500\n",
            "1/1 [==============================] - 1s 511ms/step - loss: 1.8429 - acc: 0.8297 - val_loss: 4.5342 - val_acc: 0.0762\n",
            "Epoch 196/500\n",
            "1/1 [==============================] - 1s 521ms/step - loss: 1.7720 - acc: 0.8291 - val_loss: 4.5290 - val_acc: 0.0762\n",
            "Epoch 197/500\n",
            "1/1 [==============================] - 1s 524ms/step - loss: 1.8037 - acc: 0.8270 - val_loss: 4.5233 - val_acc: 0.0762\n",
            "Epoch 198/500\n",
            "1/1 [==============================] - 1s 510ms/step - loss: 1.8516 - acc: 0.8265 - val_loss: 4.5177 - val_acc: 0.0762\n",
            "Epoch 199/500\n",
            "1/1 [==============================] - 1s 516ms/step - loss: 1.8226 - acc: 0.8287 - val_loss: 4.5119 - val_acc: 0.0762\n",
            "Epoch 200/500\n",
            "1/1 [==============================] - 1s 510ms/step - loss: 1.8056 - acc: 0.8282 - val_loss: 4.5060 - val_acc: 0.0762\n",
            "Epoch 201/500\n",
            "1/1 [==============================] - 1s 511ms/step - loss: 1.7946 - acc: 0.8309 - val_loss: 4.5005 - val_acc: 0.0786\n",
            "Epoch 202/500\n",
            "1/1 [==============================] - 1s 533ms/step - loss: 1.7893 - acc: 0.8293 - val_loss: 4.4953 - val_acc: 0.0786\n",
            "Epoch 203/500\n",
            "1/1 [==============================] - 1s 514ms/step - loss: 1.7937 - acc: 0.8296 - val_loss: 4.4903 - val_acc: 0.0810\n",
            "Epoch 204/500\n",
            "1/1 [==============================] - 1s 521ms/step - loss: 1.8220 - acc: 0.8275 - val_loss: 4.4851 - val_acc: 0.0810\n",
            "Epoch 205/500\n",
            "1/1 [==============================] - 1s 532ms/step - loss: 1.8029 - acc: 0.8294 - val_loss: 4.4800 - val_acc: 0.0810\n",
            "Epoch 206/500\n",
            "1/1 [==============================] - 1s 537ms/step - loss: 1.8092 - acc: 0.8277 - val_loss: 4.4754 - val_acc: 0.0810\n",
            "Epoch 207/500\n",
            "1/1 [==============================] - 1s 527ms/step - loss: 1.7693 - acc: 0.8307 - val_loss: 4.4705 - val_acc: 0.0810\n",
            "Epoch 208/500\n",
            "1/1 [==============================] - 1s 514ms/step - loss: 1.7824 - acc: 0.8302 - val_loss: 4.4659 - val_acc: 0.0810\n",
            "Epoch 209/500\n",
            "1/1 [==============================] - 1s 521ms/step - loss: 1.7939 - acc: 0.8289 - val_loss: 4.4612 - val_acc: 0.0810\n",
            "Epoch 210/500\n",
            "1/1 [==============================] - 1s 513ms/step - loss: 1.7737 - acc: 0.8293 - val_loss: 4.4572 - val_acc: 0.0810\n",
            "Epoch 211/500\n",
            "1/1 [==============================] - 1s 564ms/step - loss: 1.7882 - acc: 0.8287 - val_loss: 4.4532 - val_acc: 0.0810\n",
            "Epoch 212/500\n",
            "1/1 [==============================] - 1s 514ms/step - loss: 1.8368 - acc: 0.8298 - val_loss: 4.4493 - val_acc: 0.0810\n",
            "Epoch 213/500\n",
            "1/1 [==============================] - 1s 528ms/step - loss: 1.8177 - acc: 0.8303 - val_loss: 4.4451 - val_acc: 0.0810\n",
            "Epoch 214/500\n",
            "1/1 [==============================] - 1s 527ms/step - loss: 1.7666 - acc: 0.8300 - val_loss: 4.4408 - val_acc: 0.0810\n",
            "Epoch 215/500\n",
            "1/1 [==============================] - 1s 511ms/step - loss: 1.7830 - acc: 0.8301 - val_loss: 4.4362 - val_acc: 0.0810\n",
            "Epoch 216/500\n",
            "1/1 [==============================] - 1s 526ms/step - loss: 1.7817 - acc: 0.8304 - val_loss: 4.4317 - val_acc: 0.0810\n",
            "Epoch 217/500\n",
            "1/1 [==============================] - 1s 532ms/step - loss: 1.7523 - acc: 0.8302 - val_loss: 4.4268 - val_acc: 0.0810\n",
            "Epoch 218/500\n",
            "1/1 [==============================] - 1s 533ms/step - loss: 1.8117 - acc: 0.8283 - val_loss: 4.4222 - val_acc: 0.0810\n",
            "Epoch 219/500\n",
            "1/1 [==============================] - 1s 527ms/step - loss: 1.7878 - acc: 0.8288 - val_loss: 4.4182 - val_acc: 0.0810\n",
            "Epoch 220/500\n",
            "1/1 [==============================] - 1s 519ms/step - loss: 1.7834 - acc: 0.8289 - val_loss: 4.4147 - val_acc: 0.0810\n",
            "Epoch 221/500\n",
            "1/1 [==============================] - 1s 515ms/step - loss: 1.8270 - acc: 0.8274 - val_loss: 4.4115 - val_acc: 0.0833\n",
            "Epoch 222/500\n",
            "1/1 [==============================] - 1s 517ms/step - loss: 1.7398 - acc: 0.8312 - val_loss: 4.4080 - val_acc: 0.0833\n",
            "Epoch 223/500\n",
            "1/1 [==============================] - 1s 510ms/step - loss: 1.7831 - acc: 0.8287 - val_loss: 4.4044 - val_acc: 0.0833\n",
            "Epoch 224/500\n",
            "1/1 [==============================] - 1s 525ms/step - loss: 1.7887 - acc: 0.8268 - val_loss: 4.4009 - val_acc: 0.0833\n",
            "Epoch 225/500\n",
            "1/1 [==============================] - 1s 513ms/step - loss: 1.7439 - acc: 0.8313 - val_loss: 4.3978 - val_acc: 0.0833\n",
            "Epoch 226/500\n",
            "1/1 [==============================] - 1s 519ms/step - loss: 1.7864 - acc: 0.8305 - val_loss: 4.3949 - val_acc: 0.0833\n",
            "Epoch 227/500\n",
            "1/1 [==============================] - 1s 511ms/step - loss: 1.7293 - acc: 0.8304 - val_loss: 4.3919 - val_acc: 0.0881\n",
            "Epoch 228/500\n",
            "1/1 [==============================] - 1s 521ms/step - loss: 1.7377 - acc: 0.8297 - val_loss: 4.3886 - val_acc: 0.0881\n",
            "Epoch 229/500\n",
            "1/1 [==============================] - 1s 508ms/step - loss: 1.7794 - acc: 0.8283 - val_loss: 4.3852 - val_acc: 0.0881\n",
            "Epoch 230/500\n",
            "1/1 [==============================] - 1s 514ms/step - loss: 1.7370 - acc: 0.8303 - val_loss: 4.3822 - val_acc: 0.0881\n",
            "Epoch 231/500\n",
            "1/1 [==============================] - 1s 505ms/step - loss: 1.7943 - acc: 0.8276 - val_loss: 4.3778 - val_acc: 0.0881\n",
            "Epoch 232/500\n",
            "1/1 [==============================] - 1s 532ms/step - loss: 1.7608 - acc: 0.8303 - val_loss: 4.3738 - val_acc: 0.0881\n",
            "Epoch 233/500\n",
            "1/1 [==============================] - 1s 511ms/step - loss: 1.7551 - acc: 0.8311 - val_loss: 4.3695 - val_acc: 0.0881\n",
            "Epoch 234/500\n",
            "1/1 [==============================] - 1s 545ms/step - loss: 1.7398 - acc: 0.8305 - val_loss: 4.3654 - val_acc: 0.0905\n",
            "Epoch 235/500\n",
            "1/1 [==============================] - 1s 512ms/step - loss: 1.7473 - acc: 0.8311 - val_loss: 4.3615 - val_acc: 0.0905\n",
            "Epoch 236/500\n",
            "1/1 [==============================] - 1s 532ms/step - loss: 1.7817 - acc: 0.8303 - val_loss: 4.3578 - val_acc: 0.0905\n",
            "Epoch 237/500\n",
            "1/1 [==============================] - 1s 526ms/step - loss: 1.7460 - acc: 0.8305 - val_loss: 4.3541 - val_acc: 0.0905\n",
            "Epoch 238/500\n",
            "1/1 [==============================] - 1s 526ms/step - loss: 1.7759 - acc: 0.8291 - val_loss: 4.3505 - val_acc: 0.0905\n",
            "Epoch 239/500\n",
            "1/1 [==============================] - 1s 509ms/step - loss: 1.7790 - acc: 0.8273 - val_loss: 4.3464 - val_acc: 0.0929\n",
            "Epoch 240/500\n",
            "1/1 [==============================] - 1s 528ms/step - loss: 1.7986 - acc: 0.8309 - val_loss: 4.3422 - val_acc: 0.0929\n",
            "Epoch 241/500\n",
            "1/1 [==============================] - 1s 510ms/step - loss: 1.7642 - acc: 0.8312 - val_loss: 4.3379 - val_acc: 0.0929\n",
            "Epoch 242/500\n",
            "1/1 [==============================] - 1s 521ms/step - loss: 1.7456 - acc: 0.8302 - val_loss: 4.3336 - val_acc: 0.0929\n",
            "Epoch 243/500\n",
            "1/1 [==============================] - 1s 520ms/step - loss: 1.7327 - acc: 0.8326 - val_loss: 4.3297 - val_acc: 0.0929\n",
            "Epoch 244/500\n",
            "1/1 [==============================] - 1s 546ms/step - loss: 1.7387 - acc: 0.8312 - val_loss: 4.3261 - val_acc: 0.0952\n",
            "Epoch 245/500\n",
            "1/1 [==============================] - 1s 519ms/step - loss: 1.7163 - acc: 0.8322 - val_loss: 4.3229 - val_acc: 0.0952\n",
            "Epoch 246/500\n",
            "1/1 [==============================] - 1s 517ms/step - loss: 1.7559 - acc: 0.8276 - val_loss: 4.3199 - val_acc: 0.0952\n",
            "Epoch 247/500\n",
            "1/1 [==============================] - 1s 543ms/step - loss: 1.7851 - acc: 0.8301 - val_loss: 4.3172 - val_acc: 0.0952\n",
            "Epoch 248/500\n",
            "1/1 [==============================] - 1s 524ms/step - loss: 1.8784 - acc: 0.8295 - val_loss: 4.3143 - val_acc: 0.0952\n",
            "Epoch 249/500\n",
            "1/1 [==============================] - 1s 524ms/step - loss: 1.7712 - acc: 0.8289 - val_loss: 4.3110 - val_acc: 0.0952\n",
            "Epoch 250/500\n",
            "1/1 [==============================] - 1s 519ms/step - loss: 1.7156 - acc: 0.8310 - val_loss: 4.3077 - val_acc: 0.0952\n",
            "Epoch 251/500\n",
            "1/1 [==============================] - 1s 542ms/step - loss: 1.7812 - acc: 0.8283 - val_loss: 4.3039 - val_acc: 0.0952\n",
            "Epoch 252/500\n",
            "1/1 [==============================] - 1s 542ms/step - loss: 1.7347 - acc: 0.8304 - val_loss: 4.2999 - val_acc: 0.0952\n",
            "Epoch 253/500\n",
            "1/1 [==============================] - 1s 537ms/step - loss: 1.7243 - acc: 0.8326 - val_loss: 4.2965 - val_acc: 0.1000\n",
            "Epoch 254/500\n",
            "1/1 [==============================] - 1s 503ms/step - loss: 1.7322 - acc: 0.8287 - val_loss: 4.2930 - val_acc: 0.1024\n",
            "Epoch 255/500\n",
            "1/1 [==============================] - 1s 539ms/step - loss: 1.7327 - acc: 0.8312 - val_loss: 4.2894 - val_acc: 0.1024\n",
            "Epoch 256/500\n",
            "1/1 [==============================] - 1s 520ms/step - loss: 1.7305 - acc: 0.8307 - val_loss: 4.2856 - val_acc: 0.1024\n",
            "Epoch 257/500\n",
            "1/1 [==============================] - 1s 530ms/step - loss: 1.7185 - acc: 0.8326 - val_loss: 4.2819 - val_acc: 0.1024\n",
            "Epoch 258/500\n",
            "1/1 [==============================] - 1s 527ms/step - loss: 1.7285 - acc: 0.8299 - val_loss: 4.2780 - val_acc: 0.1024\n",
            "Epoch 259/500\n",
            "1/1 [==============================] - 1s 517ms/step - loss: 1.7284 - acc: 0.8281 - val_loss: 4.2739 - val_acc: 0.1024\n",
            "Epoch 260/500\n",
            "1/1 [==============================] - 1s 528ms/step - loss: 1.7357 - acc: 0.8302 - val_loss: 4.2698 - val_acc: 0.1024\n",
            "Epoch 261/500\n",
            "1/1 [==============================] - 1s 527ms/step - loss: 1.6922 - acc: 0.8324 - val_loss: 4.2660 - val_acc: 0.1024\n",
            "Epoch 262/500\n",
            "1/1 [==============================] - 1s 514ms/step - loss: 1.7173 - acc: 0.8316 - val_loss: 4.2626 - val_acc: 0.1024\n",
            "Epoch 263/500\n",
            "1/1 [==============================] - 1s 536ms/step - loss: 1.7251 - acc: 0.8300 - val_loss: 4.2593 - val_acc: 0.1024\n",
            "Epoch 264/500\n",
            "1/1 [==============================] - 1s 521ms/step - loss: 1.7263 - acc: 0.8295 - val_loss: 4.2558 - val_acc: 0.1024\n",
            "Epoch 265/500\n",
            "1/1 [==============================] - 1s 533ms/step - loss: 1.7276 - acc: 0.8310 - val_loss: 4.2527 - val_acc: 0.1048\n",
            "Epoch 266/500\n",
            "1/1 [==============================] - 1s 519ms/step - loss: 1.6846 - acc: 0.8324 - val_loss: 4.2499 - val_acc: 0.1048\n",
            "Epoch 267/500\n",
            "1/1 [==============================] - 1s 514ms/step - loss: 1.7251 - acc: 0.8319 - val_loss: 4.2466 - val_acc: 0.1048\n",
            "Epoch 268/500\n",
            "1/1 [==============================] - 1s 520ms/step - loss: 1.6719 - acc: 0.8328 - val_loss: 4.2435 - val_acc: 0.1048\n",
            "Epoch 269/500\n",
            "1/1 [==============================] - 1s 523ms/step - loss: 1.7225 - acc: 0.8316 - val_loss: 4.2401 - val_acc: 0.1048\n",
            "Epoch 270/500\n",
            "1/1 [==============================] - 1s 543ms/step - loss: 1.7047 - acc: 0.8336 - val_loss: 4.2370 - val_acc: 0.1048\n",
            "Epoch 271/500\n",
            "1/1 [==============================] - 1s 515ms/step - loss: 1.7766 - acc: 0.8309 - val_loss: 4.2340 - val_acc: 0.1048\n",
            "Epoch 272/500\n",
            "1/1 [==============================] - 1s 548ms/step - loss: 1.6948 - acc: 0.8318 - val_loss: 4.2311 - val_acc: 0.1048\n",
            "Epoch 273/500\n",
            "1/1 [==============================] - 1s 524ms/step - loss: 1.7254 - acc: 0.8304 - val_loss: 4.2278 - val_acc: 0.1048\n",
            "Epoch 274/500\n",
            "1/1 [==============================] - 1s 550ms/step - loss: 1.7102 - acc: 0.8307 - val_loss: 4.2248 - val_acc: 0.1048\n",
            "Epoch 275/500\n",
            "1/1 [==============================] - 1s 518ms/step - loss: 1.7019 - acc: 0.8310 - val_loss: 4.2216 - val_acc: 0.1048\n",
            "Epoch 276/500\n",
            "1/1 [==============================] - 1s 535ms/step - loss: 1.7329 - acc: 0.8339 - val_loss: 4.2183 - val_acc: 0.1048\n",
            "Epoch 277/500\n",
            "1/1 [==============================] - 1s 517ms/step - loss: 1.6783 - acc: 0.8331 - val_loss: 4.2150 - val_acc: 0.1071\n",
            "Epoch 278/500\n",
            "1/1 [==============================] - 1s 535ms/step - loss: 1.6924 - acc: 0.8309 - val_loss: 4.2118 - val_acc: 0.1071\n",
            "Epoch 279/500\n",
            "1/1 [==============================] - 1s 513ms/step - loss: 1.6929 - acc: 0.8294 - val_loss: 4.2090 - val_acc: 0.1071\n",
            "Epoch 280/500\n",
            "1/1 [==============================] - 1s 563ms/step - loss: 1.7224 - acc: 0.8321 - val_loss: 4.2061 - val_acc: 0.1071\n",
            "Epoch 281/500\n",
            "1/1 [==============================] - 1s 511ms/step - loss: 1.6675 - acc: 0.8307 - val_loss: 4.2033 - val_acc: 0.1071\n",
            "Epoch 282/500\n",
            "1/1 [==============================] - 1s 548ms/step - loss: 1.7076 - acc: 0.8301 - val_loss: 4.2009 - val_acc: 0.1095\n",
            "Epoch 283/500\n",
            "1/1 [==============================] - 1s 535ms/step - loss: 1.6716 - acc: 0.8312 - val_loss: 4.1981 - val_acc: 0.1095\n",
            "Epoch 284/500\n",
            "1/1 [==============================] - 1s 554ms/step - loss: 1.7060 - acc: 0.8298 - val_loss: 4.1955 - val_acc: 0.1095\n",
            "Epoch 285/500\n",
            "1/1 [==============================] - 1s 530ms/step - loss: 1.6673 - acc: 0.8326 - val_loss: 4.1929 - val_acc: 0.1095\n",
            "Epoch 286/500\n",
            "1/1 [==============================] - 1s 522ms/step - loss: 1.6738 - acc: 0.8303 - val_loss: 4.1902 - val_acc: 0.1095\n",
            "Epoch 287/500\n",
            "1/1 [==============================] - 1s 520ms/step - loss: 1.6790 - acc: 0.8310 - val_loss: 4.1882 - val_acc: 0.1095\n",
            "Epoch 288/500\n",
            "1/1 [==============================] - 1s 531ms/step - loss: 1.6896 - acc: 0.8301 - val_loss: 4.1868 - val_acc: 0.1095\n",
            "Epoch 289/500\n",
            "1/1 [==============================] - 0s 497ms/step - loss: 1.6827 - acc: 0.8322 - val_loss: 4.1852 - val_acc: 0.1095\n",
            "Epoch 290/500\n",
            "1/1 [==============================] - 1s 520ms/step - loss: 1.6997 - acc: 0.8293 - val_loss: 4.1836 - val_acc: 0.1119\n",
            "Epoch 291/500\n",
            "1/1 [==============================] - 1s 540ms/step - loss: 1.6932 - acc: 0.8304 - val_loss: 4.1821 - val_acc: 0.1143\n",
            "Epoch 292/500\n",
            "1/1 [==============================] - 1s 546ms/step - loss: 1.6758 - acc: 0.8312 - val_loss: 4.1804 - val_acc: 0.1143\n",
            "Epoch 293/500\n",
            "1/1 [==============================] - 1s 517ms/step - loss: 1.6839 - acc: 0.8302 - val_loss: 4.1788 - val_acc: 0.1143\n",
            "Epoch 294/500\n",
            "1/1 [==============================] - 1s 521ms/step - loss: 1.6934 - acc: 0.8293 - val_loss: 4.1776 - val_acc: 0.1143\n",
            "Epoch 295/500\n",
            "1/1 [==============================] - 1s 531ms/step - loss: 1.6672 - acc: 0.8315 - val_loss: 4.1764 - val_acc: 0.1167\n",
            "Epoch 296/500\n",
            "1/1 [==============================] - 1s 521ms/step - loss: 1.6973 - acc: 0.8327 - val_loss: 4.1755 - val_acc: 0.1167\n",
            "Epoch 297/500\n",
            "1/1 [==============================] - 1s 535ms/step - loss: 1.6625 - acc: 0.8298 - val_loss: 4.1742 - val_acc: 0.1167\n",
            "Epoch 298/500\n",
            "1/1 [==============================] - 1s 502ms/step - loss: 1.6588 - acc: 0.8311 - val_loss: 4.1725 - val_acc: 0.1167\n",
            "Epoch 299/500\n",
            "1/1 [==============================] - 1s 520ms/step - loss: 1.6851 - acc: 0.8316 - val_loss: 4.1708 - val_acc: 0.1167\n",
            "Epoch 300/500\n",
            "1/1 [==============================] - 1s 520ms/step - loss: 1.6607 - acc: 0.8328 - val_loss: 4.1690 - val_acc: 0.1167\n",
            "Epoch 301/500\n",
            "1/1 [==============================] - 1s 528ms/step - loss: 1.6679 - acc: 0.8328 - val_loss: 4.1677 - val_acc: 0.1167\n",
            "Epoch 302/500\n",
            "1/1 [==============================] - 1s 529ms/step - loss: 1.6522 - acc: 0.8322 - val_loss: 4.1664 - val_acc: 0.1167\n",
            "Epoch 303/500\n",
            "1/1 [==============================] - 1s 548ms/step - loss: 1.6533 - acc: 0.8326 - val_loss: 4.1653 - val_acc: 0.1167\n",
            "Epoch 304/500\n",
            "1/1 [==============================] - 1s 533ms/step - loss: 1.6920 - acc: 0.8315 - val_loss: 4.1641 - val_acc: 0.1190\n",
            "Epoch 305/500\n",
            "1/1 [==============================] - 1s 526ms/step - loss: 1.6755 - acc: 0.8301 - val_loss: 4.1634 - val_acc: 0.1190\n",
            "Epoch 306/500\n",
            "1/1 [==============================] - 1s 526ms/step - loss: 1.6280 - acc: 0.8334 - val_loss: 4.1625 - val_acc: 0.1190\n",
            "Epoch 307/500\n",
            "1/1 [==============================] - 1s 526ms/step - loss: 1.7270 - acc: 0.8323 - val_loss: 4.1616 - val_acc: 0.1190\n",
            "Epoch 308/500\n",
            "1/1 [==============================] - 1s 510ms/step - loss: 1.6456 - acc: 0.8318 - val_loss: 4.1606 - val_acc: 0.1190\n",
            "Epoch 309/500\n",
            "1/1 [==============================] - 1s 533ms/step - loss: 1.6544 - acc: 0.8318 - val_loss: 4.1593 - val_acc: 0.1190\n",
            "Epoch 310/500\n",
            "1/1 [==============================] - 1s 525ms/step - loss: 1.6343 - acc: 0.8298 - val_loss: 4.1580 - val_acc: 0.1190\n",
            "Epoch 311/500\n",
            "1/1 [==============================] - 1s 529ms/step - loss: 1.6331 - acc: 0.8316 - val_loss: 4.1564 - val_acc: 0.1190\n",
            "Epoch 312/500\n",
            "1/1 [==============================] - 1s 533ms/step - loss: 1.6383 - acc: 0.8334 - val_loss: 4.1548 - val_acc: 0.1190\n",
            "Epoch 313/500\n",
            "1/1 [==============================] - 1s 537ms/step - loss: 1.6226 - acc: 0.8328 - val_loss: 4.1536 - val_acc: 0.1190\n",
            "Epoch 314/500\n",
            "1/1 [==============================] - 1s 535ms/step - loss: 1.6619 - acc: 0.8293 - val_loss: 4.1522 - val_acc: 0.1190\n",
            "Epoch 315/500\n",
            "1/1 [==============================] - 1s 536ms/step - loss: 1.6708 - acc: 0.8318 - val_loss: 4.1506 - val_acc: 0.1190\n",
            "Epoch 316/500\n",
            "1/1 [==============================] - 1s 533ms/step - loss: 1.6192 - acc: 0.8332 - val_loss: 4.1492 - val_acc: 0.1190\n",
            "Epoch 317/500\n",
            "1/1 [==============================] - 1s 512ms/step - loss: 1.6461 - acc: 0.8320 - val_loss: 4.1473 - val_acc: 0.1190\n",
            "Epoch 318/500\n",
            "1/1 [==============================] - 1s 526ms/step - loss: 1.6466 - acc: 0.8317 - val_loss: 4.1455 - val_acc: 0.1214\n",
            "Epoch 319/500\n",
            "1/1 [==============================] - 1s 511ms/step - loss: 1.6406 - acc: 0.8324 - val_loss: 4.1437 - val_acc: 0.1214\n",
            "Epoch 320/500\n",
            "1/1 [==============================] - 1s 529ms/step - loss: 1.6627 - acc: 0.8322 - val_loss: 4.1421 - val_acc: 0.1214\n",
            "Epoch 321/500\n",
            "1/1 [==============================] - 1s 508ms/step - loss: 1.6567 - acc: 0.8300 - val_loss: 4.1399 - val_acc: 0.1214\n",
            "Epoch 322/500\n",
            "1/1 [==============================] - 1s 516ms/step - loss: 1.6630 - acc: 0.8329 - val_loss: 4.1380 - val_acc: 0.1214\n",
            "Epoch 323/500\n",
            "1/1 [==============================] - 1s 520ms/step - loss: 1.6669 - acc: 0.8326 - val_loss: 4.1362 - val_acc: 0.1214\n",
            "Epoch 324/500\n",
            "1/1 [==============================] - 1s 518ms/step - loss: 1.6516 - acc: 0.8330 - val_loss: 4.1341 - val_acc: 0.1214\n",
            "Epoch 325/500\n",
            "1/1 [==============================] - 1s 505ms/step - loss: 1.6174 - acc: 0.8333 - val_loss: 4.1319 - val_acc: 0.1214\n",
            "Epoch 326/500\n",
            "1/1 [==============================] - 1s 517ms/step - loss: 1.6463 - acc: 0.8326 - val_loss: 4.1295 - val_acc: 0.1214\n",
            "Epoch 327/500\n",
            "1/1 [==============================] - 1s 509ms/step - loss: 1.6099 - acc: 0.8334 - val_loss: 4.1268 - val_acc: 0.1214\n",
            "Epoch 328/500\n",
            "1/1 [==============================] - 1s 518ms/step - loss: 1.6067 - acc: 0.8316 - val_loss: 4.1243 - val_acc: 0.1214\n",
            "Epoch 329/500\n",
            "1/1 [==============================] - 1s 520ms/step - loss: 1.6305 - acc: 0.8309 - val_loss: 4.1212 - val_acc: 0.1214\n",
            "Epoch 330/500\n",
            "1/1 [==============================] - 1s 540ms/step - loss: 1.6492 - acc: 0.8324 - val_loss: 4.1183 - val_acc: 0.1214\n",
            "Epoch 331/500\n",
            "1/1 [==============================] - 1s 530ms/step - loss: 1.6662 - acc: 0.8312 - val_loss: 4.1155 - val_acc: 0.1214\n",
            "Epoch 332/500\n",
            "1/1 [==============================] - 1s 546ms/step - loss: 1.6273 - acc: 0.8322 - val_loss: 4.1127 - val_acc: 0.1214\n",
            "Epoch 333/500\n",
            "1/1 [==============================] - 1s 547ms/step - loss: 1.6315 - acc: 0.8318 - val_loss: 4.1100 - val_acc: 0.1214\n",
            "Epoch 334/500\n",
            "1/1 [==============================] - 1s 525ms/step - loss: 1.6291 - acc: 0.8309 - val_loss: 4.1070 - val_acc: 0.1214\n",
            "Epoch 335/500\n",
            "1/1 [==============================] - 1s 522ms/step - loss: 1.6045 - acc: 0.8326 - val_loss: 4.1043 - val_acc: 0.1214\n",
            "Epoch 336/500\n",
            "1/1 [==============================] - 1s 526ms/step - loss: 1.6210 - acc: 0.8318 - val_loss: 4.1017 - val_acc: 0.1214\n",
            "Epoch 337/500\n",
            "1/1 [==============================] - 1s 521ms/step - loss: 1.6396 - acc: 0.8330 - val_loss: 4.0996 - val_acc: 0.1214\n",
            "Epoch 338/500\n",
            "1/1 [==============================] - 1s 536ms/step - loss: 1.6185 - acc: 0.8327 - val_loss: 4.0982 - val_acc: 0.1214\n",
            "Epoch 339/500\n",
            "1/1 [==============================] - 1s 543ms/step - loss: 1.7043 - acc: 0.8338 - val_loss: 4.0967 - val_acc: 0.1214\n",
            "Epoch 340/500\n",
            "1/1 [==============================] - 1s 530ms/step - loss: 1.6300 - acc: 0.8320 - val_loss: 4.0955 - val_acc: 0.1214\n",
            "Epoch 341/500\n",
            "1/1 [==============================] - 1s 540ms/step - loss: 1.6240 - acc: 0.8317 - val_loss: 4.0947 - val_acc: 0.1214\n",
            "Epoch 342/500\n",
            "1/1 [==============================] - 1s 525ms/step - loss: 1.6311 - acc: 0.8316 - val_loss: 4.0939 - val_acc: 0.1214\n",
            "Epoch 343/500\n",
            "1/1 [==============================] - 1s 536ms/step - loss: 1.6207 - acc: 0.8328 - val_loss: 4.0926 - val_acc: 0.1214\n",
            "Epoch 344/500\n",
            "1/1 [==============================] - 1s 537ms/step - loss: 1.6064 - acc: 0.8332 - val_loss: 4.0909 - val_acc: 0.1214\n",
            "Epoch 345/500\n",
            "1/1 [==============================] - 1s 557ms/step - loss: 1.6013 - acc: 0.8318 - val_loss: 4.0888 - val_acc: 0.1214\n",
            "Epoch 346/500\n",
            "1/1 [==============================] - 1s 517ms/step - loss: 1.6233 - acc: 0.8336 - val_loss: 4.0871 - val_acc: 0.1214\n",
            "Epoch 347/500\n",
            "1/1 [==============================] - 1s 533ms/step - loss: 1.5895 - acc: 0.8324 - val_loss: 4.0853 - val_acc: 0.1214\n",
            "Epoch 348/500\n",
            "1/1 [==============================] - 1s 517ms/step - loss: 1.5953 - acc: 0.8328 - val_loss: 4.0835 - val_acc: 0.1214\n",
            "Epoch 349/500\n",
            "1/1 [==============================] - 1s 533ms/step - loss: 1.6136 - acc: 0.8325 - val_loss: 4.0816 - val_acc: 0.1214\n",
            "Epoch 350/500\n",
            "1/1 [==============================] - 1s 516ms/step - loss: 1.5892 - acc: 0.8327 - val_loss: 4.0798 - val_acc: 0.1214\n",
            "Epoch 351/500\n",
            "1/1 [==============================] - 1s 539ms/step - loss: 1.6237 - acc: 0.8325 - val_loss: 4.0785 - val_acc: 0.1214\n",
            "Epoch 352/500\n",
            "1/1 [==============================] - 1s 541ms/step - loss: 1.5813 - acc: 0.8328 - val_loss: 4.0770 - val_acc: 0.1214\n",
            "Epoch 353/500\n",
            "1/1 [==============================] - 1s 535ms/step - loss: 1.6308 - acc: 0.8319 - val_loss: 4.0753 - val_acc: 0.1214\n",
            "Epoch 354/500\n",
            "1/1 [==============================] - 1s 527ms/step - loss: 1.5890 - acc: 0.8317 - val_loss: 4.0741 - val_acc: 0.1214\n",
            "Epoch 355/500\n",
            "1/1 [==============================] - 1s 533ms/step - loss: 1.5867 - acc: 0.8330 - val_loss: 4.0727 - val_acc: 0.1214\n",
            "Epoch 356/500\n",
            "1/1 [==============================] - 1s 516ms/step - loss: 1.5982 - acc: 0.8338 - val_loss: 4.0710 - val_acc: 0.1214\n",
            "Epoch 357/500\n",
            "1/1 [==============================] - 1s 537ms/step - loss: 1.5913 - acc: 0.8322 - val_loss: 4.0690 - val_acc: 0.1214\n",
            "Epoch 358/500\n",
            "1/1 [==============================] - 1s 524ms/step - loss: 1.5798 - acc: 0.8323 - val_loss: 4.0667 - val_acc: 0.1214\n",
            "Epoch 359/500\n",
            "1/1 [==============================] - 1s 518ms/step - loss: 1.6093 - acc: 0.8330 - val_loss: 4.0644 - val_acc: 0.1214\n",
            "Epoch 360/500\n",
            "1/1 [==============================] - 1s 543ms/step - loss: 1.5918 - acc: 0.8345 - val_loss: 4.0621 - val_acc: 0.1214\n",
            "Epoch 361/500\n",
            "1/1 [==============================] - 1s 530ms/step - loss: 1.6188 - acc: 0.8327 - val_loss: 4.0600 - val_acc: 0.1214\n",
            "Epoch 362/500\n",
            "1/1 [==============================] - 1s 545ms/step - loss: 1.5939 - acc: 0.8328 - val_loss: 4.0577 - val_acc: 0.1214\n",
            "Epoch 363/500\n",
            "1/1 [==============================] - 1s 520ms/step - loss: 1.5889 - acc: 0.8337 - val_loss: 4.0540 - val_acc: 0.1214\n",
            "Epoch 364/500\n",
            "1/1 [==============================] - 1s 541ms/step - loss: 1.5884 - acc: 0.8339 - val_loss: 4.0507 - val_acc: 0.1214\n",
            "Epoch 365/500\n",
            "1/1 [==============================] - 1s 520ms/step - loss: 1.5786 - acc: 0.8330 - val_loss: 4.0475 - val_acc: 0.1238\n",
            "Epoch 366/500\n",
            "1/1 [==============================] - 1s 544ms/step - loss: 1.5767 - acc: 0.8322 - val_loss: 4.0446 - val_acc: 0.1238\n",
            "Epoch 367/500\n",
            "1/1 [==============================] - 1s 530ms/step - loss: 1.5793 - acc: 0.8325 - val_loss: 4.0418 - val_acc: 0.1238\n",
            "Epoch 368/500\n",
            "1/1 [==============================] - 1s 527ms/step - loss: 1.5794 - acc: 0.8318 - val_loss: 4.0395 - val_acc: 0.1238\n",
            "Epoch 369/500\n",
            "1/1 [==============================] - 1s 512ms/step - loss: 1.5899 - acc: 0.8324 - val_loss: 4.0371 - val_acc: 0.1238\n",
            "Epoch 370/500\n",
            "1/1 [==============================] - 1s 521ms/step - loss: 1.5820 - acc: 0.8326 - val_loss: 4.0347 - val_acc: 0.1238\n",
            "Epoch 371/500\n",
            "1/1 [==============================] - 1s 548ms/step - loss: 1.5855 - acc: 0.8339 - val_loss: 4.0324 - val_acc: 0.1238\n",
            "Epoch 372/500\n",
            "1/1 [==============================] - 1s 544ms/step - loss: 1.5555 - acc: 0.8319 - val_loss: 4.0304 - val_acc: 0.1238\n",
            "Epoch 373/500\n",
            "1/1 [==============================] - 1s 505ms/step - loss: 1.5810 - acc: 0.8325 - val_loss: 4.0286 - val_acc: 0.1238\n",
            "Epoch 374/500\n",
            "1/1 [==============================] - 1s 539ms/step - loss: 1.5832 - acc: 0.8338 - val_loss: 4.0264 - val_acc: 0.1262\n",
            "Epoch 375/500\n",
            "1/1 [==============================] - 1s 513ms/step - loss: 1.5787 - acc: 0.8341 - val_loss: 4.0241 - val_acc: 0.1286\n",
            "Epoch 376/500\n",
            "1/1 [==============================] - 1s 532ms/step - loss: 1.5810 - acc: 0.8333 - val_loss: 4.0219 - val_acc: 0.1310\n",
            "Epoch 377/500\n",
            "1/1 [==============================] - 1s 506ms/step - loss: 1.5512 - acc: 0.8322 - val_loss: 4.0198 - val_acc: 0.1333\n",
            "Epoch 378/500\n",
            "1/1 [==============================] - 1s 540ms/step - loss: 1.5633 - acc: 0.8345 - val_loss: 4.0178 - val_acc: 0.1333\n",
            "Epoch 379/500\n",
            "1/1 [==============================] - 1s 533ms/step - loss: 1.5854 - acc: 0.8343 - val_loss: 4.0155 - val_acc: 0.1333\n",
            "Epoch 380/500\n",
            "1/1 [==============================] - 1s 537ms/step - loss: 1.5640 - acc: 0.8322 - val_loss: 4.0122 - val_acc: 0.1333\n",
            "Epoch 381/500\n",
            "1/1 [==============================] - 1s 528ms/step - loss: 1.5754 - acc: 0.8338 - val_loss: 4.0094 - val_acc: 0.1333\n",
            "Epoch 382/500\n",
            "1/1 [==============================] - 1s 542ms/step - loss: 1.5737 - acc: 0.8321 - val_loss: 4.0069 - val_acc: 0.1333\n",
            "Epoch 383/500\n",
            "1/1 [==============================] - 1s 539ms/step - loss: 1.5700 - acc: 0.8331 - val_loss: 4.0045 - val_acc: 0.1333\n",
            "Epoch 384/500\n",
            "1/1 [==============================] - 1s 514ms/step - loss: 1.5799 - acc: 0.8319 - val_loss: 4.0022 - val_acc: 0.1357\n",
            "Epoch 385/500\n",
            "1/1 [==============================] - 1s 536ms/step - loss: 1.5835 - acc: 0.8340 - val_loss: 3.9999 - val_acc: 0.1357\n",
            "Epoch 386/500\n",
            "1/1 [==============================] - 1s 520ms/step - loss: 1.5570 - acc: 0.8324 - val_loss: 3.9972 - val_acc: 0.1357\n",
            "Epoch 387/500\n",
            "1/1 [==============================] - 1s 526ms/step - loss: 1.5624 - acc: 0.8337 - val_loss: 3.9946 - val_acc: 0.1357\n",
            "Epoch 388/500\n",
            "1/1 [==============================] - 1s 523ms/step - loss: 1.5567 - acc: 0.8326 - val_loss: 3.9917 - val_acc: 0.1357\n",
            "Epoch 389/500\n",
            "1/1 [==============================] - 1s 524ms/step - loss: 1.5848 - acc: 0.8321 - val_loss: 3.9884 - val_acc: 0.1357\n",
            "Epoch 390/500\n",
            "1/1 [==============================] - 1s 505ms/step - loss: 1.5648 - acc: 0.8351 - val_loss: 3.9854 - val_acc: 0.1357\n",
            "Epoch 391/500\n",
            "1/1 [==============================] - 1s 549ms/step - loss: 1.5673 - acc: 0.8340 - val_loss: 3.9825 - val_acc: 0.1357\n",
            "Epoch 392/500\n",
            "1/1 [==============================] - 1s 515ms/step - loss: 1.5575 - acc: 0.8324 - val_loss: 3.9799 - val_acc: 0.1357\n",
            "Epoch 393/500\n",
            "1/1 [==============================] - 1s 519ms/step - loss: 1.5801 - acc: 0.8332 - val_loss: 3.9777 - val_acc: 0.1357\n",
            "Epoch 394/500\n",
            "1/1 [==============================] - 1s 524ms/step - loss: 1.5800 - acc: 0.8329 - val_loss: 3.9760 - val_acc: 0.1357\n",
            "Epoch 395/500\n",
            "1/1 [==============================] - 1s 557ms/step - loss: 1.5540 - acc: 0.8335 - val_loss: 3.9746 - val_acc: 0.1357\n",
            "Epoch 396/500\n",
            "1/1 [==============================] - 1s 528ms/step - loss: 1.5501 - acc: 0.8334 - val_loss: 3.9731 - val_acc: 0.1381\n",
            "Epoch 397/500\n",
            "1/1 [==============================] - 1s 529ms/step - loss: 1.5282 - acc: 0.8338 - val_loss: 3.9713 - val_acc: 0.1405\n",
            "Epoch 398/500\n",
            "1/1 [==============================] - 1s 524ms/step - loss: 1.5434 - acc: 0.8341 - val_loss: 3.9693 - val_acc: 0.1405\n",
            "Epoch 399/500\n",
            "1/1 [==============================] - 1s 533ms/step - loss: 1.5352 - acc: 0.8330 - val_loss: 3.9678 - val_acc: 0.1405\n",
            "Epoch 400/500\n",
            "1/1 [==============================] - 1s 509ms/step - loss: 1.5624 - acc: 0.8319 - val_loss: 3.9662 - val_acc: 0.1405\n",
            "Epoch 401/500\n",
            "1/1 [==============================] - 1s 515ms/step - loss: 1.5254 - acc: 0.8355 - val_loss: 3.9647 - val_acc: 0.1405\n",
            "Epoch 402/500\n",
            "1/1 [==============================] - 1s 522ms/step - loss: 1.5597 - acc: 0.8335 - val_loss: 3.9639 - val_acc: 0.1405\n",
            "Epoch 403/500\n",
            "1/1 [==============================] - 1s 527ms/step - loss: 1.5513 - acc: 0.8351 - val_loss: 3.9627 - val_acc: 0.1405\n",
            "Epoch 404/500\n",
            "1/1 [==============================] - 1s 518ms/step - loss: 1.5325 - acc: 0.8343 - val_loss: 3.9618 - val_acc: 0.1405\n",
            "Epoch 405/500\n",
            "1/1 [==============================] - 1s 523ms/step - loss: 1.5236 - acc: 0.8338 - val_loss: 3.9613 - val_acc: 0.1429\n",
            "Epoch 406/500\n",
            "1/1 [==============================] - 1s 507ms/step - loss: 1.5263 - acc: 0.8336 - val_loss: 3.9605 - val_acc: 0.1429\n",
            "Epoch 407/500\n",
            "1/1 [==============================] - 1s 523ms/step - loss: 1.5204 - acc: 0.8327 - val_loss: 3.9595 - val_acc: 0.1429\n",
            "Epoch 408/500\n",
            "1/1 [==============================] - 1s 516ms/step - loss: 1.5440 - acc: 0.8340 - val_loss: 3.9583 - val_acc: 0.1452\n",
            "Epoch 409/500\n",
            "1/1 [==============================] - 1s 527ms/step - loss: 1.5229 - acc: 0.8345 - val_loss: 3.9571 - val_acc: 0.1476\n",
            "Epoch 410/500\n",
            "1/1 [==============================] - 1s 521ms/step - loss: 1.5402 - acc: 0.8336 - val_loss: 3.9560 - val_acc: 0.1500\n",
            "Epoch 411/500\n",
            "1/1 [==============================] - 1s 526ms/step - loss: 1.5448 - acc: 0.8326 - val_loss: 3.9541 - val_acc: 0.1500\n",
            "Epoch 412/500\n",
            "1/1 [==============================] - 1s 521ms/step - loss: 1.5410 - acc: 0.8338 - val_loss: 3.9522 - val_acc: 0.1500\n",
            "Epoch 413/500\n",
            "1/1 [==============================] - 1s 552ms/step - loss: 1.5331 - acc: 0.8341 - val_loss: 3.9502 - val_acc: 0.1500\n",
            "Epoch 414/500\n",
            "1/1 [==============================] - 1s 561ms/step - loss: 1.5411 - acc: 0.8326 - val_loss: 3.9488 - val_acc: 0.1524\n",
            "Epoch 415/500\n",
            "1/1 [==============================] - 1s 520ms/step - loss: 1.5592 - acc: 0.8331 - val_loss: 3.9474 - val_acc: 0.1524\n",
            "Epoch 416/500\n",
            "1/1 [==============================] - 1s 530ms/step - loss: 1.5352 - acc: 0.8328 - val_loss: 3.9460 - val_acc: 0.1524\n",
            "Epoch 417/500\n",
            "1/1 [==============================] - 1s 525ms/step - loss: 1.5162 - acc: 0.8340 - val_loss: 3.9441 - val_acc: 0.1524\n",
            "Epoch 418/500\n",
            "1/1 [==============================] - 1s 550ms/step - loss: 1.5436 - acc: 0.8332 - val_loss: 3.9426 - val_acc: 0.1548\n",
            "Epoch 419/500\n",
            "1/1 [==============================] - 1s 504ms/step - loss: 1.5301 - acc: 0.8340 - val_loss: 3.9410 - val_acc: 0.1571\n",
            "Epoch 420/500\n",
            "1/1 [==============================] - 1s 558ms/step - loss: 1.5151 - acc: 0.8348 - val_loss: 3.9395 - val_acc: 0.1571\n",
            "Epoch 421/500\n",
            "1/1 [==============================] - 1s 528ms/step - loss: 1.5178 - acc: 0.8336 - val_loss: 3.9385 - val_acc: 0.1571\n",
            "Epoch 422/500\n",
            "1/1 [==============================] - 1s 537ms/step - loss: 1.5152 - acc: 0.8325 - val_loss: 3.9384 - val_acc: 0.1571\n",
            "Epoch 423/500\n",
            "1/1 [==============================] - 1s 511ms/step - loss: 1.5521 - acc: 0.8333 - val_loss: 3.9380 - val_acc: 0.1619\n",
            "Epoch 424/500\n",
            "1/1 [==============================] - 1s 556ms/step - loss: 1.5139 - acc: 0.8352 - val_loss: 3.9381 - val_acc: 0.1619\n",
            "Epoch 425/500\n",
            "1/1 [==============================] - 1s 510ms/step - loss: 1.5191 - acc: 0.8328 - val_loss: 3.9382 - val_acc: 0.1619\n",
            "Epoch 426/500\n",
            "1/1 [==============================] - 1s 519ms/step - loss: 1.5042 - acc: 0.8345 - val_loss: 3.9384 - val_acc: 0.1619\n",
            "Epoch 427/500\n",
            "1/1 [==============================] - 1s 524ms/step - loss: 1.5086 - acc: 0.8341 - val_loss: 3.9378 - val_acc: 0.1619\n",
            "Epoch 428/500\n",
            "1/1 [==============================] - 1s 524ms/step - loss: 1.5238 - acc: 0.8332 - val_loss: 3.9363 - val_acc: 0.1619\n",
            "Epoch 429/500\n",
            "1/1 [==============================] - 1s 532ms/step - loss: 1.5145 - acc: 0.8339 - val_loss: 3.9346 - val_acc: 0.1619\n",
            "Epoch 430/500\n",
            "1/1 [==============================] - 1s 523ms/step - loss: 1.5061 - acc: 0.8337 - val_loss: 3.9331 - val_acc: 0.1619\n",
            "Epoch 431/500\n",
            "1/1 [==============================] - 1s 522ms/step - loss: 1.5068 - acc: 0.8336 - val_loss: 3.9314 - val_acc: 0.1619\n",
            "Epoch 432/500\n",
            "1/1 [==============================] - 1s 536ms/step - loss: 1.5212 - acc: 0.8338 - val_loss: 3.9300 - val_acc: 0.1619\n",
            "Epoch 433/500\n",
            "1/1 [==============================] - 1s 512ms/step - loss: 1.5067 - acc: 0.8332 - val_loss: 3.9287 - val_acc: 0.1643\n",
            "Epoch 434/500\n",
            "1/1 [==============================] - 1s 518ms/step - loss: 1.5198 - acc: 0.8326 - val_loss: 3.9287 - val_acc: 0.1643\n",
            "Epoch 435/500\n",
            "1/1 [==============================] - 1s 531ms/step - loss: 1.5054 - acc: 0.8338 - val_loss: 3.9284 - val_acc: 0.1643\n",
            "Epoch 436/500\n",
            "1/1 [==============================] - 1s 515ms/step - loss: 1.5283 - acc: 0.8343 - val_loss: 3.9276 - val_acc: 0.1643\n",
            "Epoch 437/500\n",
            "1/1 [==============================] - 1s 532ms/step - loss: 1.4944 - acc: 0.8351 - val_loss: 3.9269 - val_acc: 0.1643\n",
            "Epoch 438/500\n",
            "1/1 [==============================] - 1s 512ms/step - loss: 1.5138 - acc: 0.8343 - val_loss: 3.9267 - val_acc: 0.1643\n",
            "Epoch 439/500\n",
            "1/1 [==============================] - 1s 527ms/step - loss: 1.5262 - acc: 0.8341 - val_loss: 3.9262 - val_acc: 0.1643\n",
            "Epoch 440/500\n",
            "1/1 [==============================] - 1s 509ms/step - loss: 1.4809 - acc: 0.8339 - val_loss: 3.9256 - val_acc: 0.1643\n",
            "Epoch 441/500\n",
            "1/1 [==============================] - 1s 529ms/step - loss: 1.5265 - acc: 0.8331 - val_loss: 3.9250 - val_acc: 0.1643\n",
            "Epoch 442/500\n",
            "1/1 [==============================] - 1s 519ms/step - loss: 1.4942 - acc: 0.8326 - val_loss: 3.9243 - val_acc: 0.1643\n",
            "Epoch 443/500\n",
            "1/1 [==============================] - 1s 508ms/step - loss: 1.4887 - acc: 0.8345 - val_loss: 3.9233 - val_acc: 0.1643\n",
            "Epoch 444/500\n",
            "1/1 [==============================] - 1s 522ms/step - loss: 1.4902 - acc: 0.8341 - val_loss: 3.9222 - val_acc: 0.1667\n",
            "Epoch 445/500\n",
            "1/1 [==============================] - 1s 525ms/step - loss: 1.4818 - acc: 0.8353 - val_loss: 3.9210 - val_acc: 0.1667\n",
            "Epoch 446/500\n",
            "1/1 [==============================] - 1s 524ms/step - loss: 1.4850 - acc: 0.8347 - val_loss: 3.9200 - val_acc: 0.1667\n",
            "Epoch 447/500\n",
            "1/1 [==============================] - 1s 532ms/step - loss: 1.4986 - acc: 0.8334 - val_loss: 3.9185 - val_acc: 0.1690\n",
            "Epoch 448/500\n",
            "1/1 [==============================] - 1s 528ms/step - loss: 1.5071 - acc: 0.8343 - val_loss: 3.9165 - val_acc: 0.1690\n",
            "Epoch 449/500\n",
            "1/1 [==============================] - 1s 516ms/step - loss: 1.4777 - acc: 0.8341 - val_loss: 3.9142 - val_acc: 0.1690\n",
            "Epoch 450/500\n",
            "1/1 [==============================] - 1s 505ms/step - loss: 1.4877 - acc: 0.8332 - val_loss: 3.9117 - val_acc: 0.1690\n",
            "Epoch 451/500\n",
            "1/1 [==============================] - 1s 544ms/step - loss: 1.4526 - acc: 0.8353 - val_loss: 3.9096 - val_acc: 0.1690\n",
            "Epoch 452/500\n",
            "1/1 [==============================] - 1s 526ms/step - loss: 1.4897 - acc: 0.8334 - val_loss: 3.9070 - val_acc: 0.1690\n",
            "Epoch 453/500\n",
            "1/1 [==============================] - 1s 574ms/step - loss: 1.4866 - acc: 0.8349 - val_loss: 3.9047 - val_acc: 0.1714\n",
            "Epoch 454/500\n",
            "1/1 [==============================] - 1s 520ms/step - loss: 1.5069 - acc: 0.8332 - val_loss: 3.9026 - val_acc: 0.1714\n",
            "Epoch 455/500\n",
            "1/1 [==============================] - 1s 517ms/step - loss: 1.5033 - acc: 0.8361 - val_loss: 3.9001 - val_acc: 0.1714\n",
            "Epoch 456/500\n",
            "1/1 [==============================] - 1s 514ms/step - loss: 1.4874 - acc: 0.8343 - val_loss: 3.8983 - val_acc: 0.1714\n",
            "Epoch 457/500\n",
            "1/1 [==============================] - 1s 517ms/step - loss: 1.5235 - acc: 0.8351 - val_loss: 3.8964 - val_acc: 0.1714\n",
            "Epoch 458/500\n",
            "1/1 [==============================] - 1s 520ms/step - loss: 1.4657 - acc: 0.8336 - val_loss: 3.8940 - val_acc: 0.1714\n",
            "Epoch 459/500\n",
            "1/1 [==============================] - 1s 533ms/step - loss: 1.4721 - acc: 0.8350 - val_loss: 3.8921 - val_acc: 0.1714\n",
            "Epoch 460/500\n",
            "1/1 [==============================] - 1s 507ms/step - loss: 1.4858 - acc: 0.8357 - val_loss: 3.8906 - val_acc: 0.1714\n",
            "Epoch 461/500\n",
            "1/1 [==============================] - 1s 526ms/step - loss: 1.4822 - acc: 0.8336 - val_loss: 3.8890 - val_acc: 0.1714\n",
            "Epoch 462/500\n",
            "1/1 [==============================] - 1s 517ms/step - loss: 1.5027 - acc: 0.8355 - val_loss: 3.8870 - val_acc: 0.1714\n",
            "Epoch 463/500\n",
            "1/1 [==============================] - 1s 521ms/step - loss: 1.4802 - acc: 0.8332 - val_loss: 3.8848 - val_acc: 0.1714\n",
            "Epoch 464/500\n",
            "1/1 [==============================] - 1s 521ms/step - loss: 1.4981 - acc: 0.8345 - val_loss: 3.8823 - val_acc: 0.1762\n",
            "Epoch 465/500\n",
            "1/1 [==============================] - 1s 524ms/step - loss: 1.4942 - acc: 0.8338 - val_loss: 3.8792 - val_acc: 0.1762\n",
            "Epoch 466/500\n",
            "1/1 [==============================] - 1s 524ms/step - loss: 1.4791 - acc: 0.8333 - val_loss: 3.8752 - val_acc: 0.1762\n",
            "Epoch 467/500\n",
            "1/1 [==============================] - 1s 522ms/step - loss: 1.4857 - acc: 0.8337 - val_loss: 3.8719 - val_acc: 0.1762\n",
            "Epoch 468/500\n",
            "1/1 [==============================] - 1s 532ms/step - loss: 1.4539 - acc: 0.8343 - val_loss: 3.8686 - val_acc: 0.1786\n",
            "Epoch 469/500\n",
            "1/1 [==============================] - 1s 506ms/step - loss: 1.4506 - acc: 0.8343 - val_loss: 3.8659 - val_acc: 0.1786\n",
            "Epoch 470/500\n",
            "1/1 [==============================] - 1s 508ms/step - loss: 1.4955 - acc: 0.8330 - val_loss: 3.8630 - val_acc: 0.1786\n",
            "Epoch 471/500\n",
            "1/1 [==============================] - 1s 510ms/step - loss: 1.5005 - acc: 0.8355 - val_loss: 3.8604 - val_acc: 0.1786\n",
            "Epoch 472/500\n",
            "1/1 [==============================] - 1s 523ms/step - loss: 1.4712 - acc: 0.8345 - val_loss: 3.8576 - val_acc: 0.1786\n",
            "Epoch 473/500\n",
            "1/1 [==============================] - 1s 535ms/step - loss: 1.4710 - acc: 0.8343 - val_loss: 3.8543 - val_acc: 0.1786\n",
            "Epoch 474/500\n",
            "1/1 [==============================] - 1s 522ms/step - loss: 1.4546 - acc: 0.8340 - val_loss: 3.8510 - val_acc: 0.1786\n",
            "Epoch 475/500\n",
            "1/1 [==============================] - 1s 526ms/step - loss: 1.4970 - acc: 0.8341 - val_loss: 3.8472 - val_acc: 0.1786\n",
            "Epoch 476/500\n",
            "1/1 [==============================] - 1s 530ms/step - loss: 1.4571 - acc: 0.8341 - val_loss: 3.8435 - val_acc: 0.1786\n",
            "Epoch 477/500\n",
            "1/1 [==============================] - 1s 510ms/step - loss: 1.4783 - acc: 0.8349 - val_loss: 3.8396 - val_acc: 0.1786\n",
            "Epoch 478/500\n",
            "1/1 [==============================] - 1s 537ms/step - loss: 1.4626 - acc: 0.8338 - val_loss: 3.8348 - val_acc: 0.1786\n",
            "Epoch 479/500\n",
            "1/1 [==============================] - 1s 506ms/step - loss: 1.4743 - acc: 0.8345 - val_loss: 3.8297 - val_acc: 0.1810\n",
            "Epoch 480/500\n",
            "1/1 [==============================] - 1s 528ms/step - loss: 1.4670 - acc: 0.8337 - val_loss: 3.8241 - val_acc: 0.1810\n",
            "Epoch 481/500\n",
            "1/1 [==============================] - 1s 551ms/step - loss: 1.4642 - acc: 0.8347 - val_loss: 3.8184 - val_acc: 0.1810\n",
            "Epoch 482/500\n",
            "1/1 [==============================] - 1s 555ms/step - loss: 1.4844 - acc: 0.8341 - val_loss: 3.8131 - val_acc: 0.1833\n",
            "Epoch 483/500\n",
            "1/1 [==============================] - 1s 543ms/step - loss: 1.4438 - acc: 0.8357 - val_loss: 3.8078 - val_acc: 0.1881\n",
            "Epoch 484/500\n",
            "1/1 [==============================] - 1s 547ms/step - loss: 1.4552 - acc: 0.8334 - val_loss: 3.8030 - val_acc: 0.1881\n",
            "Epoch 485/500\n",
            "1/1 [==============================] - 1s 520ms/step - loss: 1.4466 - acc: 0.8351 - val_loss: 3.7981 - val_acc: 0.1952\n",
            "Epoch 486/500\n",
            "1/1 [==============================] - 1s 541ms/step - loss: 1.4412 - acc: 0.8347 - val_loss: 3.7933 - val_acc: 0.1952\n",
            "Epoch 487/500\n",
            "1/1 [==============================] - 1s 521ms/step - loss: 1.4507 - acc: 0.8347 - val_loss: 3.7892 - val_acc: 0.1952\n",
            "Epoch 488/500\n",
            "1/1 [==============================] - 1s 542ms/step - loss: 1.4359 - acc: 0.8347 - val_loss: 3.7855 - val_acc: 0.1952\n",
            "Epoch 489/500\n",
            "1/1 [==============================] - 1s 547ms/step - loss: 1.4442 - acc: 0.8340 - val_loss: 3.7814 - val_acc: 0.1952\n",
            "Epoch 490/500\n",
            "1/1 [==============================] - 1s 525ms/step - loss: 1.4890 - acc: 0.8340 - val_loss: 3.7774 - val_acc: 0.1952\n",
            "Epoch 491/500\n",
            "1/1 [==============================] - 1s 525ms/step - loss: 1.4701 - acc: 0.8338 - val_loss: 3.7731 - val_acc: 0.1952\n",
            "Epoch 492/500\n",
            "1/1 [==============================] - 1s 543ms/step - loss: 1.4523 - acc: 0.8347 - val_loss: 3.7694 - val_acc: 0.1952\n",
            "Epoch 493/500\n",
            "1/1 [==============================] - 1s 561ms/step - loss: 1.4437 - acc: 0.8345 - val_loss: 3.7653 - val_acc: 0.1952\n",
            "Epoch 494/500\n",
            "1/1 [==============================] - 1s 514ms/step - loss: 1.4382 - acc: 0.8341 - val_loss: 3.7602 - val_acc: 0.1952\n",
            "Epoch 495/500\n",
            "1/1 [==============================] - 1s 515ms/step - loss: 1.4566 - acc: 0.8342 - val_loss: 3.7553 - val_acc: 0.1976\n",
            "Epoch 496/500\n",
            "1/1 [==============================] - 1s 528ms/step - loss: 1.4504 - acc: 0.8345 - val_loss: 3.7512 - val_acc: 0.1976\n",
            "Epoch 497/500\n",
            "1/1 [==============================] - 1s 528ms/step - loss: 1.4350 - acc: 0.8353 - val_loss: 3.7469 - val_acc: 0.1976\n",
            "Epoch 498/500\n",
            "1/1 [==============================] - 1s 525ms/step - loss: 1.4588 - acc: 0.8341 - val_loss: 3.7430 - val_acc: 0.2000\n",
            "Epoch 499/500\n",
            "1/1 [==============================] - 0s 476ms/step - loss: 1.4515 - acc: 0.8331 - val_loss: 3.7396 - val_acc: 0.2000\n",
            "Epoch 500/500\n",
            "1/1 [==============================] - 0s 462ms/step - loss: 1.4558 - acc: 0.8351 - val_loss: 3.7359 - val_acc: 0.2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufohY7I5uvgE"
      },
      "source": [
        "# Plotting History"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "KfpF3PZZuxlE",
        "outputId": "51d46999-1acf-4d4f-f8c8-8d1bba297ff9"
      },
      "source": [
        "import keras\n",
        "from matplotlib import pyplot as plt\n",
        "# history = model1.fit(train_x, train_y,validation_split = 0.1, epochs=50, batch_size=4)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fe1631a3550>]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuJH45r-Z_Qq"
      },
      "source": [
        "# Saving Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ltSQGRiaBcB"
      },
      "source": [
        "model.save(\"/content/drive/MyDrive/Project GCN Dataset/IDS_Batch1_0.2000(validation_accuracy).h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wX-Aacvfhr-G"
      },
      "source": [
        "# Loading Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFDnUaB8hwXo",
        "outputId": "06b77fa0-fcf9-4bc9-c9ef-d66771c0fab3"
      },
      "source": [
        "from keras.models import load_model\n",
        "# config = model.get_config()\n",
        "# print(config)\n",
        "model = load_model(\"/content/drive/MyDrive/Project GCN Dataset/IDS_Batch1_0.2000(validation_accuracy).h5\", custom_objects = {\"GCNConv\": GCNConv})\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 8)]          0           []                               \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
            "                                                                                                  \n",
            " gcn_conv (GCNConv)             (None, 32)           288         ['input_1[0][0]',                \n",
            "                                                                  'input_2[0][0]']                \n",
            "                                                                                                  \n",
            " gcn_conv_1 (GCNConv)           (None, 32)           1056        ['gcn_conv[0][0]',               \n",
            "                                                                  'input_2[0][0]']                \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 32)           0           ['gcn_conv_1[0][0]']             \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 128)          4224        ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 128)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 32)           4128        ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 32)           0           ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 12)           396         ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 10,092\n",
            "Trainable params: 10,092\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EqCENe5Zexz"
      },
      "source": [
        "# Other Batches Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwzfLflxZiS7"
      },
      "source": [
        "b2_features = Node_Features(A2)\n",
        "b3_features = Node_Features(A3)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DKv_BLybPfb"
      },
      "source": [
        "# Retrainingh Second Batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxbQf6jfbSKD",
        "outputId": "c527719f-aa25-4591-8087-b8af5d1e6a62"
      },
      "source": [
        "N = np.shape(A2)[0]\n",
        "history = model.fit([b2_features, A2],\n",
        "          e2,\n",
        "          epochs=1000,\n",
        "          batch_size=N,\n",
        "          validation_data=([val_fea, A4], val_enc),\n",
        "          callbacks=[callback]\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 8.8373 - acc: 0.4079 - val_loss: 3.6044 - val_acc: 0.2048\n",
            "Epoch 2/1000\n",
            "1/1 [==============================] - 0s 495ms/step - loss: 7.3806 - acc: 0.4518 - val_loss: 3.4201 - val_acc: 0.2071\n",
            "Epoch 3/1000\n",
            "1/1 [==============================] - 1s 503ms/step - loss: 6.0397 - acc: 0.4819 - val_loss: 3.2042 - val_acc: 0.2333\n",
            "Epoch 4/1000\n",
            "1/1 [==============================] - 0s 457ms/step - loss: 4.8373 - acc: 0.5115 - val_loss: 2.9499 - val_acc: 0.2571\n",
            "Epoch 5/1000\n",
            "1/1 [==============================] - 0s 456ms/step - loss: 4.0627 - acc: 0.5190 - val_loss: 2.6968 - val_acc: 0.2786\n",
            "Epoch 6/1000\n",
            "1/1 [==============================] - 0s 469ms/step - loss: 3.5618 - acc: 0.5318 - val_loss: 2.4481 - val_acc: 0.3357\n",
            "Epoch 7/1000\n",
            "1/1 [==============================] - 0s 461ms/step - loss: 2.8907 - acc: 0.5438 - val_loss: 2.2542 - val_acc: 0.4214\n",
            "Epoch 8/1000\n",
            "1/1 [==============================] - 0s 465ms/step - loss: 2.7430 - acc: 0.5444 - val_loss: 2.1437 - val_acc: 0.4381\n",
            "Epoch 9/1000\n",
            "1/1 [==============================] - 0s 464ms/step - loss: 2.6374 - acc: 0.5547 - val_loss: 2.0647 - val_acc: 0.4810\n",
            "Epoch 10/1000\n",
            "1/1 [==============================] - 0s 483ms/step - loss: 2.5368 - acc: 0.5536 - val_loss: 1.9928 - val_acc: 0.5405\n",
            "Epoch 11/1000\n",
            "1/1 [==============================] - 0s 480ms/step - loss: 2.4079 - acc: 0.5534 - val_loss: 1.9303 - val_acc: 0.6286\n",
            "Epoch 12/1000\n",
            "1/1 [==============================] - 0s 466ms/step - loss: 2.3647 - acc: 0.5571 - val_loss: 1.8761 - val_acc: 0.6881\n",
            "Epoch 13/1000\n",
            "1/1 [==============================] - 0s 455ms/step - loss: 2.3325 - acc: 0.5614 - val_loss: 1.8318 - val_acc: 0.7667\n",
            "Epoch 14/1000\n",
            "1/1 [==============================] - 0s 490ms/step - loss: 2.3645 - acc: 0.5560 - val_loss: 1.7959 - val_acc: 0.8333\n",
            "Epoch 15/1000\n",
            "1/1 [==============================] - 0s 471ms/step - loss: 2.2698 - acc: 0.5579 - val_loss: 1.7682 - val_acc: 0.8643\n",
            "Epoch 16/1000\n",
            "1/1 [==============================] - 0s 488ms/step - loss: 2.3080 - acc: 0.5595 - val_loss: 1.7485 - val_acc: 0.8643\n",
            "Epoch 17/1000\n",
            "1/1 [==============================] - 0s 466ms/step - loss: 2.2315 - acc: 0.5643 - val_loss: 1.7364 - val_acc: 0.8690\n",
            "Epoch 18/1000\n",
            "1/1 [==============================] - 0s 471ms/step - loss: 2.2667 - acc: 0.5610 - val_loss: 1.7318 - val_acc: 0.8690\n",
            "Epoch 19/1000\n",
            "1/1 [==============================] - 0s 456ms/step - loss: 2.1661 - acc: 0.5647 - val_loss: 1.7309 - val_acc: 0.8690\n",
            "Epoch 20/1000\n",
            "1/1 [==============================] - 0s 497ms/step - loss: 2.1893 - acc: 0.5641 - val_loss: 1.7299 - val_acc: 0.8690\n",
            "Epoch 21/1000\n",
            "1/1 [==============================] - 0s 473ms/step - loss: 2.1771 - acc: 0.5630 - val_loss: 1.7290 - val_acc: 0.8690\n",
            "Epoch 22/1000\n",
            "1/1 [==============================] - 0s 459ms/step - loss: 2.2372 - acc: 0.5628 - val_loss: 1.7280 - val_acc: 0.8690\n",
            "Epoch 23/1000\n",
            "1/1 [==============================] - 0s 469ms/step - loss: 2.1389 - acc: 0.5654 - val_loss: 1.7271 - val_acc: 0.8690\n",
            "Epoch 24/1000\n",
            "1/1 [==============================] - 1s 509ms/step - loss: 2.1651 - acc: 0.5664 - val_loss: 1.7262 - val_acc: 0.8690\n",
            "Epoch 25/1000\n",
            "1/1 [==============================] - 0s 456ms/step - loss: 2.1051 - acc: 0.5654 - val_loss: 1.7252 - val_acc: 0.8690\n",
            "Epoch 26/1000\n",
            "1/1 [==============================] - 0s 472ms/step - loss: 2.1126 - acc: 0.5647 - val_loss: 1.7243 - val_acc: 0.8690\n",
            "Epoch 27/1000\n",
            "1/1 [==============================] - 0s 474ms/step - loss: 2.1216 - acc: 0.5656 - val_loss: 1.7233 - val_acc: 0.8690\n",
            "Epoch 28/1000\n",
            "1/1 [==============================] - 0s 461ms/step - loss: 2.1109 - acc: 0.5665 - val_loss: 1.7224 - val_acc: 0.8690\n",
            "Epoch 29/1000\n",
            "1/1 [==============================] - 0s 469ms/step - loss: 2.1465 - acc: 0.5678 - val_loss: 1.7215 - val_acc: 0.8690\n",
            "Epoch 30/1000\n",
            "1/1 [==============================] - 0s 481ms/step - loss: 2.1499 - acc: 0.5643 - val_loss: 1.7206 - val_acc: 0.8690\n",
            "Epoch 31/1000\n",
            "1/1 [==============================] - 0s 466ms/step - loss: 2.1045 - acc: 0.5675 - val_loss: 1.7196 - val_acc: 0.8690\n",
            "Epoch 32/1000\n",
            "1/1 [==============================] - 0s 468ms/step - loss: 2.0901 - acc: 0.5665 - val_loss: 1.7187 - val_acc: 0.8690\n",
            "Epoch 33/1000\n",
            "1/1 [==============================] - 0s 478ms/step - loss: 2.0962 - acc: 0.5682 - val_loss: 1.7178 - val_acc: 0.8690\n",
            "Epoch 34/1000\n",
            "1/1 [==============================] - 0s 485ms/step - loss: 2.1299 - acc: 0.5680 - val_loss: 1.7169 - val_acc: 0.8690\n",
            "Epoch 35/1000\n",
            "1/1 [==============================] - 0s 462ms/step - loss: 2.1098 - acc: 0.5675 - val_loss: 1.7159 - val_acc: 0.8690\n",
            "Epoch 36/1000\n",
            "1/1 [==============================] - 0s 473ms/step - loss: 2.0989 - acc: 0.5675 - val_loss: 1.7150 - val_acc: 0.8690\n",
            "Epoch 37/1000\n",
            "1/1 [==============================] - 0s 467ms/step - loss: 2.0939 - acc: 0.5658 - val_loss: 1.7141 - val_acc: 0.8690\n",
            "Epoch 38/1000\n",
            "1/1 [==============================] - 0s 445ms/step - loss: 2.0698 - acc: 0.5667 - val_loss: 1.7132 - val_acc: 0.8690\n",
            "Epoch 39/1000\n",
            "1/1 [==============================] - 0s 461ms/step - loss: 2.0637 - acc: 0.5673 - val_loss: 1.7123 - val_acc: 0.8690\n",
            "Epoch 40/1000\n",
            "1/1 [==============================] - 0s 451ms/step - loss: 2.0597 - acc: 0.5678 - val_loss: 1.7114 - val_acc: 0.8690\n",
            "Epoch 41/1000\n",
            "1/1 [==============================] - 0s 464ms/step - loss: 2.0612 - acc: 0.5689 - val_loss: 1.7104 - val_acc: 0.8690\n",
            "Epoch 42/1000\n",
            "1/1 [==============================] - 0s 470ms/step - loss: 2.0891 - acc: 0.5673 - val_loss: 1.7095 - val_acc: 0.8690\n",
            "Epoch 43/1000\n",
            "1/1 [==============================] - 0s 436ms/step - loss: 2.0556 - acc: 0.5684 - val_loss: 1.7086 - val_acc: 0.8690\n",
            "Epoch 44/1000\n",
            "1/1 [==============================] - 0s 438ms/step - loss: 2.0864 - acc: 0.5675 - val_loss: 1.7077 - val_acc: 0.8690\n",
            "Epoch 45/1000\n",
            "1/1 [==============================] - 0s 459ms/step - loss: 2.0774 - acc: 0.5675 - val_loss: 1.7068 - val_acc: 0.8690\n",
            "Epoch 46/1000\n",
            "1/1 [==============================] - 0s 481ms/step - loss: 2.0568 - acc: 0.5680 - val_loss: 1.7059 - val_acc: 0.8690\n",
            "Epoch 47/1000\n",
            "1/1 [==============================] - 0s 461ms/step - loss: 2.0691 - acc: 0.5677 - val_loss: 1.7050 - val_acc: 0.8690\n",
            "Epoch 48/1000\n",
            "1/1 [==============================] - 0s 490ms/step - loss: 2.0467 - acc: 0.5688 - val_loss: 1.7041 - val_acc: 0.8690\n",
            "Epoch 49/1000\n",
            "1/1 [==============================] - 0s 463ms/step - loss: 2.0423 - acc: 0.5689 - val_loss: 1.7032 - val_acc: 0.8690\n",
            "Epoch 50/1000\n",
            "1/1 [==============================] - 0s 441ms/step - loss: 2.0589 - acc: 0.5689 - val_loss: 1.7023 - val_acc: 0.8690\n",
            "Epoch 51/1000\n",
            "1/1 [==============================] - 0s 455ms/step - loss: 2.0599 - acc: 0.5665 - val_loss: 1.7014 - val_acc: 0.8690\n",
            "Epoch 52/1000\n",
            "1/1 [==============================] - 0s 471ms/step - loss: 2.0759 - acc: 0.5688 - val_loss: 1.7004 - val_acc: 0.8690\n",
            "Epoch 53/1000\n",
            "1/1 [==============================] - 0s 449ms/step - loss: 2.0358 - acc: 0.5693 - val_loss: 1.6995 - val_acc: 0.8690\n",
            "Epoch 54/1000\n",
            "1/1 [==============================] - 0s 473ms/step - loss: 2.0590 - acc: 0.5686 - val_loss: 1.6986 - val_acc: 0.8690\n",
            "Epoch 55/1000\n",
            "1/1 [==============================] - 0s 452ms/step - loss: 2.0537 - acc: 0.5682 - val_loss: 1.6977 - val_acc: 0.8690\n",
            "Epoch 56/1000\n",
            "1/1 [==============================] - 0s 447ms/step - loss: 2.0470 - acc: 0.5688 - val_loss: 1.6968 - val_acc: 0.8690\n",
            "Epoch 57/1000\n",
            "1/1 [==============================] - 0s 467ms/step - loss: 2.0512 - acc: 0.5680 - val_loss: 1.6959 - val_acc: 0.8690\n",
            "Epoch 58/1000\n",
            "1/1 [==============================] - 0s 483ms/step - loss: 2.0539 - acc: 0.5691 - val_loss: 1.6950 - val_acc: 0.8690\n",
            "Epoch 59/1000\n",
            "1/1 [==============================] - 0s 465ms/step - loss: 2.0385 - acc: 0.5686 - val_loss: 1.6941 - val_acc: 0.8690\n",
            "Epoch 60/1000\n",
            "1/1 [==============================] - 0s 469ms/step - loss: 2.0492 - acc: 0.5686 - val_loss: 1.6933 - val_acc: 0.8690\n",
            "Epoch 61/1000\n",
            "1/1 [==============================] - 0s 488ms/step - loss: 2.0406 - acc: 0.5689 - val_loss: 1.6924 - val_acc: 0.8690\n",
            "Epoch 62/1000\n",
            "1/1 [==============================] - 0s 459ms/step - loss: 2.0307 - acc: 0.5689 - val_loss: 1.6915 - val_acc: 0.8690\n",
            "Epoch 63/1000\n",
            "1/1 [==============================] - 0s 461ms/step - loss: 2.0375 - acc: 0.5693 - val_loss: 1.6906 - val_acc: 0.8690\n",
            "Epoch 64/1000\n",
            "1/1 [==============================] - 0s 467ms/step - loss: 2.0583 - acc: 0.5688 - val_loss: 1.6897 - val_acc: 0.8690\n",
            "Epoch 65/1000\n",
            "1/1 [==============================] - 0s 481ms/step - loss: 2.0422 - acc: 0.5684 - val_loss: 1.6888 - val_acc: 0.8690\n",
            "Epoch 66/1000\n",
            "1/1 [==============================] - 0s 454ms/step - loss: 2.0365 - acc: 0.5686 - val_loss: 1.6879 - val_acc: 0.8690\n",
            "Epoch 67/1000\n",
            "1/1 [==============================] - 0s 447ms/step - loss: 2.0420 - acc: 0.5693 - val_loss: 1.6870 - val_acc: 0.8690\n",
            "Epoch 68/1000\n",
            "1/1 [==============================] - 0s 450ms/step - loss: 2.0241 - acc: 0.5695 - val_loss: 1.6861 - val_acc: 0.8690\n",
            "Epoch 69/1000\n",
            "1/1 [==============================] - 0s 486ms/step - loss: 2.0430 - acc: 0.5691 - val_loss: 1.6852 - val_acc: 0.8690\n",
            "Epoch 70/1000\n",
            "1/1 [==============================] - 0s 456ms/step - loss: 2.0428 - acc: 0.5689 - val_loss: 1.6843 - val_acc: 0.8690\n",
            "Epoch 71/1000\n",
            "1/1 [==============================] - 0s 434ms/step - loss: 2.0328 - acc: 0.5688 - val_loss: 1.6835 - val_acc: 0.8690\n",
            "Epoch 72/1000\n",
            "1/1 [==============================] - 0s 451ms/step - loss: 2.0402 - acc: 0.5689 - val_loss: 1.6826 - val_acc: 0.8690\n",
            "Epoch 73/1000\n",
            "1/1 [==============================] - 0s 453ms/step - loss: 2.0534 - acc: 0.5677 - val_loss: 1.6817 - val_acc: 0.8690\n",
            "Epoch 74/1000\n",
            "1/1 [==============================] - 0s 467ms/step - loss: 2.0341 - acc: 0.5691 - val_loss: 1.6808 - val_acc: 0.8690\n",
            "Epoch 75/1000\n",
            "1/1 [==============================] - 0s 471ms/step - loss: 2.0255 - acc: 0.5693 - val_loss: 1.6799 - val_acc: 0.8690\n",
            "Epoch 76/1000\n",
            "1/1 [==============================] - 0s 450ms/step - loss: 2.0319 - acc: 0.5686 - val_loss: 1.6790 - val_acc: 0.8690\n",
            "Epoch 77/1000\n",
            "1/1 [==============================] - 0s 453ms/step - loss: 2.0391 - acc: 0.5689 - val_loss: 1.6782 - val_acc: 0.8690\n",
            "Epoch 78/1000\n",
            "1/1 [==============================] - 0s 466ms/step - loss: 2.0268 - acc: 0.5688 - val_loss: 1.6773 - val_acc: 0.8690\n",
            "Epoch 79/1000\n",
            "1/1 [==============================] - 0s 469ms/step - loss: 2.0197 - acc: 0.5688 - val_loss: 1.6764 - val_acc: 0.8690\n",
            "Epoch 80/1000\n",
            "1/1 [==============================] - 0s 466ms/step - loss: 2.0240 - acc: 0.5693 - val_loss: 1.6755 - val_acc: 0.8690\n",
            "Epoch 81/1000\n",
            "1/1 [==============================] - 0s 462ms/step - loss: 2.0279 - acc: 0.5693 - val_loss: 1.6747 - val_acc: 0.8690\n",
            "Epoch 82/1000\n",
            "1/1 [==============================] - 0s 464ms/step - loss: 2.0183 - acc: 0.5693 - val_loss: 1.6738 - val_acc: 0.8690\n",
            "Epoch 83/1000\n",
            "1/1 [==============================] - 0s 459ms/step - loss: 2.0403 - acc: 0.5695 - val_loss: 1.6729 - val_acc: 0.8690\n",
            "Epoch 84/1000\n",
            "1/1 [==============================] - 0s 473ms/step - loss: 2.0202 - acc: 0.5688 - val_loss: 1.6720 - val_acc: 0.8690\n",
            "Epoch 85/1000\n",
            "1/1 [==============================] - 0s 450ms/step - loss: 2.0186 - acc: 0.5693 - val_loss: 1.6712 - val_acc: 0.8690\n",
            "Epoch 86/1000\n",
            "1/1 [==============================] - 0s 463ms/step - loss: 2.0331 - acc: 0.5695 - val_loss: 1.6703 - val_acc: 0.8690\n",
            "Epoch 87/1000\n",
            "1/1 [==============================] - 0s 460ms/step - loss: 2.0149 - acc: 0.5691 - val_loss: 1.6694 - val_acc: 0.8690\n",
            "Epoch 88/1000\n",
            "1/1 [==============================] - 0s 460ms/step - loss: 2.0208 - acc: 0.5699 - val_loss: 1.6685 - val_acc: 0.8690\n",
            "Epoch 89/1000\n",
            "1/1 [==============================] - 0s 458ms/step - loss: 2.0178 - acc: 0.5689 - val_loss: 1.6677 - val_acc: 0.8690\n",
            "Epoch 90/1000\n",
            "1/1 [==============================] - 0s 491ms/step - loss: 2.0256 - acc: 0.5686 - val_loss: 1.6668 - val_acc: 0.8690\n",
            "Epoch 91/1000\n",
            "1/1 [==============================] - 0s 491ms/step - loss: 2.0131 - acc: 0.5693 - val_loss: 1.6659 - val_acc: 0.8690\n",
            "Epoch 92/1000\n",
            "1/1 [==============================] - 0s 448ms/step - loss: 2.0155 - acc: 0.5691 - val_loss: 1.6651 - val_acc: 0.8690\n",
            "Epoch 93/1000\n",
            "1/1 [==============================] - 0s 479ms/step - loss: 2.0214 - acc: 0.5688 - val_loss: 1.6642 - val_acc: 0.8690\n",
            "Epoch 94/1000\n",
            "1/1 [==============================] - 0s 467ms/step - loss: 2.0411 - acc: 0.5689 - val_loss: 1.6633 - val_acc: 0.8690\n",
            "Epoch 95/1000\n",
            "1/1 [==============================] - 0s 435ms/step - loss: 2.0448 - acc: 0.5686 - val_loss: 1.6625 - val_acc: 0.8690\n",
            "Epoch 96/1000\n",
            "1/1 [==============================] - 0s 453ms/step - loss: 2.0080 - acc: 0.5695 - val_loss: 1.6616 - val_acc: 0.8690\n",
            "Epoch 97/1000\n",
            "1/1 [==============================] - 0s 472ms/step - loss: 2.0245 - acc: 0.5689 - val_loss: 1.6607 - val_acc: 0.8690\n",
            "Epoch 98/1000\n",
            "1/1 [==============================] - 0s 470ms/step - loss: 2.0058 - acc: 0.5693 - val_loss: 1.6599 - val_acc: 0.8690\n",
            "Epoch 99/1000\n",
            "1/1 [==============================] - 0s 465ms/step - loss: 2.0075 - acc: 0.5697 - val_loss: 1.6590 - val_acc: 0.8690\n",
            "Epoch 100/1000\n",
            "1/1 [==============================] - 0s 474ms/step - loss: 2.0090 - acc: 0.5699 - val_loss: 1.6582 - val_acc: 0.8690\n",
            "Epoch 101/1000\n",
            "1/1 [==============================] - 0s 465ms/step - loss: 2.0099 - acc: 0.5699 - val_loss: 1.6573 - val_acc: 0.8690\n",
            "Epoch 102/1000\n",
            "1/1 [==============================] - 0s 487ms/step - loss: 2.0101 - acc: 0.5691 - val_loss: 1.6564 - val_acc: 0.8690\n",
            "Epoch 103/1000\n",
            "1/1 [==============================] - 0s 463ms/step - loss: 2.0177 - acc: 0.5695 - val_loss: 1.6556 - val_acc: 0.8690\n",
            "Epoch 104/1000\n",
            "1/1 [==============================] - 0s 470ms/step - loss: 2.0185 - acc: 0.5697 - val_loss: 1.6547 - val_acc: 0.8690\n",
            "Epoch 105/1000\n",
            "1/1 [==============================] - 0s 481ms/step - loss: 1.9987 - acc: 0.5699 - val_loss: 1.6539 - val_acc: 0.8690\n",
            "Epoch 106/1000\n",
            "1/1 [==============================] - 0s 454ms/step - loss: 1.9974 - acc: 0.5693 - val_loss: 1.6530 - val_acc: 0.8690\n",
            "Epoch 107/1000\n",
            "1/1 [==============================] - 0s 485ms/step - loss: 2.0222 - acc: 0.5691 - val_loss: 1.6522 - val_acc: 0.8690\n",
            "Epoch 108/1000\n",
            "1/1 [==============================] - 0s 461ms/step - loss: 2.0037 - acc: 0.5695 - val_loss: 1.6513 - val_acc: 0.8690\n",
            "Epoch 109/1000\n",
            "1/1 [==============================] - 0s 479ms/step - loss: 2.0076 - acc: 0.5695 - val_loss: 1.6505 - val_acc: 0.8690\n",
            "Epoch 110/1000\n",
            "1/1 [==============================] - 0s 458ms/step - loss: 1.9961 - acc: 0.5693 - val_loss: 1.6496 - val_acc: 0.8690\n",
            "Epoch 111/1000\n",
            "1/1 [==============================] - 0s 490ms/step - loss: 2.0143 - acc: 0.5695 - val_loss: 1.6488 - val_acc: 0.8690\n",
            "Epoch 112/1000\n",
            "1/1 [==============================] - 0s 452ms/step - loss: 1.9947 - acc: 0.5702 - val_loss: 1.6479 - val_acc: 0.8690\n",
            "Epoch 113/1000\n",
            "1/1 [==============================] - 0s 460ms/step - loss: 1.9911 - acc: 0.5699 - val_loss: 1.6471 - val_acc: 0.8690\n",
            "Epoch 114/1000\n",
            "1/1 [==============================] - 0s 442ms/step - loss: 1.9938 - acc: 0.5693 - val_loss: 1.6462 - val_acc: 0.8690\n",
            "Epoch 115/1000\n",
            "1/1 [==============================] - 0s 444ms/step - loss: 1.9973 - acc: 0.5695 - val_loss: 1.6454 - val_acc: 0.8690\n",
            "Epoch 116/1000\n",
            "1/1 [==============================] - 0s 485ms/step - loss: 1.9963 - acc: 0.5688 - val_loss: 1.6445 - val_acc: 0.8690\n",
            "Epoch 117/1000\n",
            "1/1 [==============================] - 0s 461ms/step - loss: 1.9893 - acc: 0.5699 - val_loss: 1.6437 - val_acc: 0.8690\n",
            "Epoch 118/1000\n",
            "1/1 [==============================] - 0s 463ms/step - loss: 2.0105 - acc: 0.5693 - val_loss: 1.6428 - val_acc: 0.8690\n",
            "Epoch 119/1000\n",
            "1/1 [==============================] - 0s 448ms/step - loss: 1.9925 - acc: 0.5697 - val_loss: 1.6420 - val_acc: 0.8690\n",
            "Epoch 120/1000\n",
            "1/1 [==============================] - 0s 474ms/step - loss: 1.9958 - acc: 0.5688 - val_loss: 1.6412 - val_acc: 0.8690\n",
            "Epoch 121/1000\n",
            "1/1 [==============================] - 0s 468ms/step - loss: 1.9958 - acc: 0.5697 - val_loss: 1.6403 - val_acc: 0.8690\n",
            "Epoch 122/1000\n",
            "1/1 [==============================] - 0s 466ms/step - loss: 1.9883 - acc: 0.5691 - val_loss: 1.6395 - val_acc: 0.8690\n",
            "Epoch 123/1000\n",
            "1/1 [==============================] - 0s 461ms/step - loss: 1.9901 - acc: 0.5699 - val_loss: 1.6386 - val_acc: 0.8690\n",
            "Epoch 124/1000\n",
            "1/1 [==============================] - 1s 500ms/step - loss: 1.9845 - acc: 0.5699 - val_loss: 1.6378 - val_acc: 0.8690\n",
            "Epoch 125/1000\n",
            "1/1 [==============================] - 0s 458ms/step - loss: 1.9849 - acc: 0.5699 - val_loss: 1.6370 - val_acc: 0.8690\n",
            "Epoch 126/1000\n",
            "1/1 [==============================] - 0s 474ms/step - loss: 1.9870 - acc: 0.5699 - val_loss: 1.6361 - val_acc: 0.8690\n",
            "Epoch 127/1000\n",
            "1/1 [==============================] - 0s 440ms/step - loss: 1.9849 - acc: 0.5693 - val_loss: 1.6353 - val_acc: 0.8690\n",
            "Epoch 128/1000\n",
            "1/1 [==============================] - 0s 465ms/step - loss: 1.9914 - acc: 0.5691 - val_loss: 1.6344 - val_acc: 0.8690\n",
            "Epoch 129/1000\n",
            "1/1 [==============================] - 0s 438ms/step - loss: 1.9825 - acc: 0.5699 - val_loss: 1.6336 - val_acc: 0.8690\n",
            "Epoch 130/1000\n",
            "1/1 [==============================] - 0s 443ms/step - loss: 2.0010 - acc: 0.5693 - val_loss: 1.6328 - val_acc: 0.8690\n",
            "Epoch 131/1000\n",
            "1/1 [==============================] - 0s 459ms/step - loss: 1.9926 - acc: 0.5691 - val_loss: 1.6319 - val_acc: 0.8690\n",
            "Epoch 132/1000\n",
            "1/1 [==============================] - 0s 456ms/step - loss: 1.9913 - acc: 0.5693 - val_loss: 1.6311 - val_acc: 0.8690\n",
            "Epoch 133/1000\n",
            "1/1 [==============================] - 0s 498ms/step - loss: 1.9853 - acc: 0.5695 - val_loss: 1.6303 - val_acc: 0.8690\n",
            "Epoch 134/1000\n",
            "1/1 [==============================] - 0s 472ms/step - loss: 1.9859 - acc: 0.5697 - val_loss: 1.6294 - val_acc: 0.8690\n",
            "Epoch 135/1000\n",
            "1/1 [==============================] - 0s 476ms/step - loss: 1.9841 - acc: 0.5689 - val_loss: 1.6286 - val_acc: 0.8690\n",
            "Epoch 136/1000\n",
            "1/1 [==============================] - 0s 451ms/step - loss: 1.9885 - acc: 0.5693 - val_loss: 1.6278 - val_acc: 0.8690\n",
            "Epoch 137/1000\n",
            "1/1 [==============================] - 0s 475ms/step - loss: 1.9815 - acc: 0.5693 - val_loss: 1.6270 - val_acc: 0.8690\n",
            "Epoch 138/1000\n",
            "1/1 [==============================] - 0s 447ms/step - loss: 2.0137 - acc: 0.5695 - val_loss: 1.6261 - val_acc: 0.8690\n",
            "Epoch 139/1000\n",
            "1/1 [==============================] - 0s 470ms/step - loss: 1.9783 - acc: 0.5686 - val_loss: 1.6253 - val_acc: 0.8690\n",
            "Epoch 140/1000\n",
            "1/1 [==============================] - 0s 467ms/step - loss: 1.9776 - acc: 0.5702 - val_loss: 1.6245 - val_acc: 0.8690\n",
            "Epoch 141/1000\n",
            "1/1 [==============================] - 0s 442ms/step - loss: 1.9914 - acc: 0.5693 - val_loss: 1.6236 - val_acc: 0.8690\n",
            "Epoch 142/1000\n",
            "1/1 [==============================] - 0s 459ms/step - loss: 1.9763 - acc: 0.5697 - val_loss: 1.6228 - val_acc: 0.8690\n",
            "Epoch 143/1000\n",
            "1/1 [==============================] - 0s 458ms/step - loss: 1.9762 - acc: 0.5697 - val_loss: 1.6220 - val_acc: 0.8690\n",
            "Epoch 144/1000\n",
            "1/1 [==============================] - 0s 456ms/step - loss: 1.9891 - acc: 0.5699 - val_loss: 1.6212 - val_acc: 0.8690\n",
            "Epoch 145/1000\n",
            "1/1 [==============================] - 0s 450ms/step - loss: 1.9826 - acc: 0.5691 - val_loss: 1.6203 - val_acc: 0.8690\n",
            "Epoch 146/1000\n",
            "1/1 [==============================] - 0s 465ms/step - loss: 1.9857 - acc: 0.5693 - val_loss: 1.6195 - val_acc: 0.8690\n",
            "Epoch 147/1000\n",
            "1/1 [==============================] - 0s 462ms/step - loss: 1.9737 - acc: 0.5693 - val_loss: 1.6187 - val_acc: 0.8690\n",
            "Epoch 148/1000\n",
            "1/1 [==============================] - 0s 467ms/step - loss: 1.9812 - acc: 0.5697 - val_loss: 1.6179 - val_acc: 0.8690\n",
            "Epoch 149/1000\n",
            "1/1 [==============================] - 0s 461ms/step - loss: 1.9749 - acc: 0.5695 - val_loss: 1.6171 - val_acc: 0.8690\n",
            "Epoch 150/1000\n",
            "1/1 [==============================] - 0s 452ms/step - loss: 1.9709 - acc: 0.5699 - val_loss: 1.6162 - val_acc: 0.8690\n",
            "Epoch 151/1000\n",
            "1/1 [==============================] - 0s 452ms/step - loss: 1.9858 - acc: 0.5695 - val_loss: 1.6154 - val_acc: 0.8690\n",
            "Epoch 152/1000\n",
            "1/1 [==============================] - 0s 475ms/step - loss: 1.9698 - acc: 0.5697 - val_loss: 1.6146 - val_acc: 0.8690\n",
            "Epoch 153/1000\n",
            "1/1 [==============================] - 0s 451ms/step - loss: 1.9703 - acc: 0.5699 - val_loss: 1.6138 - val_acc: 0.8690\n",
            "Epoch 154/1000\n",
            "1/1 [==============================] - 0s 444ms/step - loss: 1.9780 - acc: 0.5693 - val_loss: 1.6130 - val_acc: 0.8690\n",
            "Epoch 155/1000\n",
            "1/1 [==============================] - 0s 481ms/step - loss: 1.9675 - acc: 0.5693 - val_loss: 1.6122 - val_acc: 0.8690\n",
            "Epoch 156/1000\n",
            "1/1 [==============================] - 0s 457ms/step - loss: 1.9888 - acc: 0.5699 - val_loss: 1.6114 - val_acc: 0.8690\n",
            "Epoch 157/1000\n",
            "1/1 [==============================] - 0s 457ms/step - loss: 1.9718 - acc: 0.5695 - val_loss: 1.6105 - val_acc: 0.8690\n",
            "Epoch 158/1000\n",
            "1/1 [==============================] - 0s 474ms/step - loss: 1.9793 - acc: 0.5693 - val_loss: 1.6097 - val_acc: 0.8690\n",
            "Epoch 159/1000\n",
            "1/1 [==============================] - 0s 440ms/step - loss: 1.9700 - acc: 0.5695 - val_loss: 1.6089 - val_acc: 0.8690\n",
            "Epoch 160/1000\n",
            "1/1 [==============================] - 0s 445ms/step - loss: 1.9728 - acc: 0.5695 - val_loss: 1.6081 - val_acc: 0.8690\n",
            "Epoch 161/1000\n",
            "1/1 [==============================] - 0s 473ms/step - loss: 1.9627 - acc: 0.5699 - val_loss: 1.6073 - val_acc: 0.8690\n",
            "Epoch 162/1000\n",
            "1/1 [==============================] - 0s 470ms/step - loss: 1.9672 - acc: 0.5691 - val_loss: 1.6065 - val_acc: 0.8690\n",
            "Epoch 163/1000\n",
            "1/1 [==============================] - 0s 470ms/step - loss: 1.9682 - acc: 0.5695 - val_loss: 1.6057 - val_acc: 0.8690\n",
            "Epoch 164/1000\n",
            "1/1 [==============================] - 1s 518ms/step - loss: 1.9756 - acc: 0.5695 - val_loss: 1.6049 - val_acc: 0.8690\n",
            "Epoch 165/1000\n",
            "1/1 [==============================] - 0s 474ms/step - loss: 1.9669 - acc: 0.5697 - val_loss: 1.6041 - val_acc: 0.8690\n",
            "Epoch 166/1000\n",
            "1/1 [==============================] - 0s 448ms/step - loss: 1.9710 - acc: 0.5699 - val_loss: 1.6033 - val_acc: 0.8690\n",
            "Epoch 167/1000\n",
            "1/1 [==============================] - 0s 433ms/step - loss: 1.9615 - acc: 0.5697 - val_loss: 1.6025 - val_acc: 0.8690\n",
            "Epoch 168/1000\n",
            "1/1 [==============================] - 0s 460ms/step - loss: 1.9659 - acc: 0.5693 - val_loss: 1.6017 - val_acc: 0.8690\n",
            "Epoch 169/1000\n",
            "1/1 [==============================] - 0s 466ms/step - loss: 1.9617 - acc: 0.5697 - val_loss: 1.6008 - val_acc: 0.8690\n",
            "Epoch 170/1000\n",
            "1/1 [==============================] - 0s 477ms/step - loss: 1.9590 - acc: 0.5701 - val_loss: 1.6000 - val_acc: 0.8690\n",
            "Epoch 171/1000\n",
            "1/1 [==============================] - 0s 463ms/step - loss: 1.9665 - acc: 0.5691 - val_loss: 1.5992 - val_acc: 0.8690\n",
            "Epoch 172/1000\n",
            "1/1 [==============================] - 0s 466ms/step - loss: 1.9644 - acc: 0.5699 - val_loss: 1.5984 - val_acc: 0.8690\n",
            "Epoch 173/1000\n",
            "1/1 [==============================] - 0s 457ms/step - loss: 1.9633 - acc: 0.5697 - val_loss: 1.5976 - val_acc: 0.8690\n",
            "Epoch 174/1000\n",
            "1/1 [==============================] - 0s 483ms/step - loss: 1.9697 - acc: 0.5691 - val_loss: 1.5968 - val_acc: 0.8690\n",
            "Epoch 175/1000\n",
            "1/1 [==============================] - 0s 440ms/step - loss: 1.9749 - acc: 0.5693 - val_loss: 1.5960 - val_acc: 0.8690\n",
            "Epoch 176/1000\n",
            "1/1 [==============================] - 0s 447ms/step - loss: 1.9546 - acc: 0.5691 - val_loss: 1.5952 - val_acc: 0.8690\n",
            "Epoch 177/1000\n",
            "1/1 [==============================] - 0s 448ms/step - loss: 1.9709 - acc: 0.5695 - val_loss: 1.5944 - val_acc: 0.8690\n",
            "Epoch 178/1000\n",
            "1/1 [==============================] - 0s 456ms/step - loss: 1.9585 - acc: 0.5699 - val_loss: 1.5936 - val_acc: 0.8690\n",
            "Epoch 179/1000\n",
            "1/1 [==============================] - 0s 448ms/step - loss: 1.9552 - acc: 0.5697 - val_loss: 1.5929 - val_acc: 0.8690\n",
            "Epoch 180/1000\n",
            "1/1 [==============================] - 0s 448ms/step - loss: 1.9543 - acc: 0.5699 - val_loss: 1.5921 - val_acc: 0.8690\n",
            "Epoch 181/1000\n",
            "1/1 [==============================] - 0s 463ms/step - loss: 1.9681 - acc: 0.5691 - val_loss: 1.5913 - val_acc: 0.8690\n",
            "Epoch 182/1000\n",
            "1/1 [==============================] - 0s 453ms/step - loss: 1.9543 - acc: 0.5691 - val_loss: 1.5905 - val_acc: 0.8690\n",
            "Epoch 183/1000\n",
            "1/1 [==============================] - 0s 471ms/step - loss: 1.9525 - acc: 0.5695 - val_loss: 1.5897 - val_acc: 0.8690\n",
            "Epoch 184/1000\n",
            "1/1 [==============================] - 0s 448ms/step - loss: 1.9522 - acc: 0.5697 - val_loss: 1.5889 - val_acc: 0.8690\n",
            "Epoch 185/1000\n",
            "1/1 [==============================] - 0s 436ms/step - loss: 1.9497 - acc: 0.5697 - val_loss: 1.5881 - val_acc: 0.8690\n",
            "Epoch 186/1000\n",
            "1/1 [==============================] - 0s 478ms/step - loss: 1.9498 - acc: 0.5695 - val_loss: 1.5873 - val_acc: 0.8690\n",
            "Epoch 187/1000\n",
            "1/1 [==============================] - 0s 484ms/step - loss: 1.9512 - acc: 0.5699 - val_loss: 1.5865 - val_acc: 0.8690\n",
            "Epoch 188/1000\n",
            "1/1 [==============================] - 0s 455ms/step - loss: 1.9654 - acc: 0.5689 - val_loss: 1.5857 - val_acc: 0.8690\n",
            "Epoch 189/1000\n",
            "1/1 [==============================] - 0s 452ms/step - loss: 1.9560 - acc: 0.5697 - val_loss: 1.5849 - val_acc: 0.8690\n",
            "Epoch 190/1000\n",
            "1/1 [==============================] - 0s 459ms/step - loss: 1.9530 - acc: 0.5693 - val_loss: 1.5841 - val_acc: 0.8690\n",
            "Epoch 191/1000\n",
            "1/1 [==============================] - 0s 452ms/step - loss: 1.9484 - acc: 0.5695 - val_loss: 1.5834 - val_acc: 0.8690\n",
            "Epoch 192/1000\n",
            "1/1 [==============================] - 0s 471ms/step - loss: 1.9544 - acc: 0.5699 - val_loss: 1.5826 - val_acc: 0.8690\n",
            "Epoch 193/1000\n",
            "1/1 [==============================] - 0s 445ms/step - loss: 1.9474 - acc: 0.5699 - val_loss: 1.5818 - val_acc: 0.8690\n",
            "Epoch 194/1000\n",
            "1/1 [==============================] - 0s 471ms/step - loss: 1.9477 - acc: 0.5697 - val_loss: 1.5810 - val_acc: 0.8690\n",
            "Epoch 195/1000\n",
            "1/1 [==============================] - 0s 444ms/step - loss: 1.9516 - acc: 0.5699 - val_loss: 1.5802 - val_acc: 0.8690\n",
            "Epoch 196/1000\n",
            "1/1 [==============================] - 0s 458ms/step - loss: 1.9540 - acc: 0.5691 - val_loss: 1.5794 - val_acc: 0.8690\n",
            "Epoch 197/1000\n",
            "1/1 [==============================] - 0s 443ms/step - loss: 1.9437 - acc: 0.5697 - val_loss: 1.5787 - val_acc: 0.8690\n",
            "Epoch 198/1000\n",
            "1/1 [==============================] - 0s 438ms/step - loss: 1.9441 - acc: 0.5699 - val_loss: 1.5779 - val_acc: 0.8690\n",
            "Epoch 199/1000\n",
            "1/1 [==============================] - 0s 452ms/step - loss: 1.9404 - acc: 0.5701 - val_loss: 1.5771 - val_acc: 0.8690\n",
            "Epoch 200/1000\n",
            "1/1 [==============================] - 0s 457ms/step - loss: 1.9424 - acc: 0.5695 - val_loss: 1.5763 - val_acc: 0.8690\n",
            "Epoch 201/1000\n",
            "1/1 [==============================] - 0s 473ms/step - loss: 1.9499 - acc: 0.5695 - val_loss: 1.5755 - val_acc: 0.8690\n",
            "Epoch 202/1000\n",
            "1/1 [==============================] - 0s 457ms/step - loss: 1.9609 - acc: 0.5691 - val_loss: 1.5748 - val_acc: 0.8690\n",
            "Epoch 203/1000\n",
            "1/1 [==============================] - 0s 451ms/step - loss: 1.9470 - acc: 0.5693 - val_loss: 1.5740 - val_acc: 0.8690\n",
            "Epoch 204/1000\n",
            "1/1 [==============================] - 0s 458ms/step - loss: 1.9414 - acc: 0.5699 - val_loss: 1.5732 - val_acc: 0.8690\n",
            "Epoch 205/1000\n",
            "1/1 [==============================] - 0s 464ms/step - loss: 1.9401 - acc: 0.5699 - val_loss: 1.5724 - val_acc: 0.8690\n",
            "Epoch 206/1000\n",
            "1/1 [==============================] - 0s 479ms/step - loss: 1.9432 - acc: 0.5699 - val_loss: 1.5716 - val_acc: 0.8690\n",
            "Epoch 207/1000\n",
            "1/1 [==============================] - 0s 452ms/step - loss: 1.9405 - acc: 0.5693 - val_loss: 1.5709 - val_acc: 0.8690\n",
            "Epoch 208/1000\n",
            "1/1 [==============================] - 0s 478ms/step - loss: 1.9400 - acc: 0.5701 - val_loss: 1.5701 - val_acc: 0.8690\n",
            "Epoch 209/1000\n",
            "1/1 [==============================] - 0s 434ms/step - loss: 1.9408 - acc: 0.5697 - val_loss: 1.5693 - val_acc: 0.8690\n",
            "Epoch 210/1000\n",
            "1/1 [==============================] - 0s 492ms/step - loss: 1.9412 - acc: 0.5697 - val_loss: 1.5685 - val_acc: 0.8690\n",
            "Epoch 211/1000\n",
            "1/1 [==============================] - 0s 449ms/step - loss: 1.9378 - acc: 0.5695 - val_loss: 1.5678 - val_acc: 0.8690\n",
            "Epoch 212/1000\n",
            "1/1 [==============================] - 0s 468ms/step - loss: 1.9518 - acc: 0.5697 - val_loss: 1.5670 - val_acc: 0.8690\n",
            "Epoch 213/1000\n",
            "1/1 [==============================] - 0s 472ms/step - loss: 1.9345 - acc: 0.5693 - val_loss: 1.5662 - val_acc: 0.8690\n",
            "Epoch 214/1000\n",
            "1/1 [==============================] - 1s 504ms/step - loss: 1.9338 - acc: 0.5693 - val_loss: 1.5655 - val_acc: 0.8690\n",
            "Epoch 215/1000\n",
            "1/1 [==============================] - 0s 474ms/step - loss: 1.9394 - acc: 0.5701 - val_loss: 1.5647 - val_acc: 0.8690\n",
            "Epoch 216/1000\n",
            "1/1 [==============================] - 0s 473ms/step - loss: 1.9386 - acc: 0.5695 - val_loss: 1.5639 - val_acc: 0.8690\n",
            "Epoch 217/1000\n",
            "1/1 [==============================] - 0s 481ms/step - loss: 1.9461 - acc: 0.5695 - val_loss: 1.5632 - val_acc: 0.8690\n",
            "Epoch 218/1000\n",
            "1/1 [==============================] - 0s 484ms/step - loss: 1.9448 - acc: 0.5697 - val_loss: 1.5624 - val_acc: 0.8690\n",
            "Epoch 219/1000\n",
            "1/1 [==============================] - 0s 451ms/step - loss: 1.9417 - acc: 0.5699 - val_loss: 1.5616 - val_acc: 0.8690\n",
            "Epoch 220/1000\n",
            "1/1 [==============================] - 0s 474ms/step - loss: 1.9332 - acc: 0.5697 - val_loss: 1.5609 - val_acc: 0.8690\n",
            "Epoch 221/1000\n",
            "1/1 [==============================] - 0s 470ms/step - loss: 1.9302 - acc: 0.5697 - val_loss: 1.5601 - val_acc: 0.8690\n",
            "Epoch 222/1000\n",
            "1/1 [==============================] - 0s 455ms/step - loss: 1.9331 - acc: 0.5699 - val_loss: 1.5593 - val_acc: 0.8690\n",
            "Epoch 223/1000\n",
            "1/1 [==============================] - 0s 466ms/step - loss: 1.9333 - acc: 0.5695 - val_loss: 1.5586 - val_acc: 0.8690\n",
            "Epoch 224/1000\n",
            "1/1 [==============================] - 0s 480ms/step - loss: 1.9349 - acc: 0.5689 - val_loss: 1.5578 - val_acc: 0.8690\n",
            "Epoch 225/1000\n",
            "1/1 [==============================] - 0s 458ms/step - loss: 1.9284 - acc: 0.5702 - val_loss: 1.5570 - val_acc: 0.8690\n",
            "Epoch 226/1000\n",
            "1/1 [==============================] - 0s 431ms/step - loss: 1.9311 - acc: 0.5699 - val_loss: 1.5563 - val_acc: 0.8690\n",
            "Epoch 227/1000\n",
            "1/1 [==============================] - 0s 450ms/step - loss: 1.9335 - acc: 0.5695 - val_loss: 1.5555 - val_acc: 0.8690\n",
            "Epoch 228/1000\n",
            "1/1 [==============================] - 0s 444ms/step - loss: 1.9335 - acc: 0.5693 - val_loss: 1.5548 - val_acc: 0.8690\n",
            "Epoch 229/1000\n",
            "1/1 [==============================] - 0s 476ms/step - loss: 1.9290 - acc: 0.5701 - val_loss: 1.5540 - val_acc: 0.8690\n",
            "Epoch 230/1000\n",
            "1/1 [==============================] - 0s 439ms/step - loss: 1.9252 - acc: 0.5695 - val_loss: 1.5532 - val_acc: 0.8690\n",
            "Epoch 231/1000\n",
            "1/1 [==============================] - 0s 442ms/step - loss: 1.9325 - acc: 0.5691 - val_loss: 1.5525 - val_acc: 0.8690\n",
            "Epoch 232/1000\n",
            "1/1 [==============================] - 0s 460ms/step - loss: 1.9234 - acc: 0.5697 - val_loss: 1.5517 - val_acc: 0.8690\n",
            "Epoch 233/1000\n",
            "1/1 [==============================] - 0s 461ms/step - loss: 1.9260 - acc: 0.5697 - val_loss: 1.5510 - val_acc: 0.8690\n",
            "Epoch 234/1000\n",
            "1/1 [==============================] - 0s 487ms/step - loss: 1.9319 - acc: 0.5697 - val_loss: 1.5502 - val_acc: 0.8690\n",
            "Epoch 235/1000\n",
            "1/1 [==============================] - 0s 452ms/step - loss: 1.9342 - acc: 0.5699 - val_loss: 1.5495 - val_acc: 0.8690\n",
            "Epoch 236/1000\n",
            "1/1 [==============================] - 0s 476ms/step - loss: 1.9289 - acc: 0.5695 - val_loss: 1.5487 - val_acc: 0.8690\n",
            "Epoch 237/1000\n",
            "1/1 [==============================] - 0s 473ms/step - loss: 1.9309 - acc: 0.5702 - val_loss: 1.5480 - val_acc: 0.8690\n",
            "Epoch 238/1000\n",
            "1/1 [==============================] - 0s 465ms/step - loss: 1.9239 - acc: 0.5699 - val_loss: 1.5472 - val_acc: 0.8690\n",
            "Epoch 239/1000\n",
            "1/1 [==============================] - 0s 464ms/step - loss: 1.9243 - acc: 0.5697 - val_loss: 1.5464 - val_acc: 0.8690\n",
            "Epoch 240/1000\n",
            "1/1 [==============================] - 0s 476ms/step - loss: 1.9275 - acc: 0.5693 - val_loss: 1.5457 - val_acc: 0.8690\n",
            "Epoch 241/1000\n",
            "1/1 [==============================] - 0s 453ms/step - loss: 1.9191 - acc: 0.5701 - val_loss: 1.5449 - val_acc: 0.8690\n",
            "Epoch 242/1000\n",
            "1/1 [==============================] - 0s 473ms/step - loss: 1.9180 - acc: 0.5697 - val_loss: 1.5442 - val_acc: 0.8690\n",
            "Epoch 243/1000\n",
            "1/1 [==============================] - 0s 464ms/step - loss: 1.9205 - acc: 0.5701 - val_loss: 1.5434 - val_acc: 0.8690\n",
            "Epoch 244/1000\n",
            "1/1 [==============================] - 0s 492ms/step - loss: 1.9191 - acc: 0.5697 - val_loss: 1.5427 - val_acc: 0.8690\n",
            "Epoch 245/1000\n",
            "1/1 [==============================] - 0s 488ms/step - loss: 1.9191 - acc: 0.5699 - val_loss: 1.5419 - val_acc: 0.8690\n",
            "Epoch 246/1000\n",
            "1/1 [==============================] - 1s 501ms/step - loss: 1.9211 - acc: 0.5695 - val_loss: 1.5412 - val_acc: 0.8690\n",
            "Epoch 247/1000\n",
            "1/1 [==============================] - 0s 468ms/step - loss: 1.9181 - acc: 0.5697 - val_loss: 1.5405 - val_acc: 0.8690\n",
            "Epoch 248/1000\n",
            "1/1 [==============================] - 0s 469ms/step - loss: 1.9154 - acc: 0.5702 - val_loss: 1.5397 - val_acc: 0.8690\n",
            "Epoch 249/1000\n",
            "1/1 [==============================] - 0s 495ms/step - loss: 1.9185 - acc: 0.5693 - val_loss: 1.5390 - val_acc: 0.8690\n",
            "Epoch 250/1000\n",
            "1/1 [==============================] - 0s 477ms/step - loss: 1.9205 - acc: 0.5697 - val_loss: 1.5382 - val_acc: 0.8690\n",
            "Epoch 251/1000\n",
            "1/1 [==============================] - 0s 490ms/step - loss: 1.9175 - acc: 0.5699 - val_loss: 1.5375 - val_acc: 0.8690\n",
            "Epoch 252/1000\n",
            "1/1 [==============================] - 0s 498ms/step - loss: 1.9130 - acc: 0.5701 - val_loss: 1.5367 - val_acc: 0.8690\n",
            "Epoch 253/1000\n",
            "1/1 [==============================] - 0s 471ms/step - loss: 1.9139 - acc: 0.5702 - val_loss: 1.5360 - val_acc: 0.8690\n",
            "Epoch 254/1000\n",
            "1/1 [==============================] - 0s 468ms/step - loss: 1.9321 - acc: 0.5701 - val_loss: 1.5352 - val_acc: 0.8690\n",
            "Epoch 255/1000\n",
            "1/1 [==============================] - 0s 455ms/step - loss: 1.9261 - acc: 0.5697 - val_loss: 1.5345 - val_acc: 0.8690\n",
            "Epoch 256/1000\n",
            "1/1 [==============================] - 0s 492ms/step - loss: 1.9165 - acc: 0.5699 - val_loss: 1.5338 - val_acc: 0.8690\n",
            "Epoch 257/1000\n",
            "1/1 [==============================] - 0s 476ms/step - loss: 1.9107 - acc: 0.5699 - val_loss: 1.5330 - val_acc: 0.8690\n",
            "Epoch 258/1000\n",
            "1/1 [==============================] - 0s 460ms/step - loss: 1.9178 - acc: 0.5695 - val_loss: 1.5323 - val_acc: 0.8690\n",
            "Epoch 259/1000\n",
            "1/1 [==============================] - 1s 505ms/step - loss: 1.9124 - acc: 0.5699 - val_loss: 1.5315 - val_acc: 0.8690\n",
            "Epoch 260/1000\n",
            "1/1 [==============================] - 0s 479ms/step - loss: 1.9146 - acc: 0.5695 - val_loss: 1.5308 - val_acc: 0.8690\n",
            "Epoch 261/1000\n",
            "1/1 [==============================] - 0s 494ms/step - loss: 1.9119 - acc: 0.5699 - val_loss: 1.5301 - val_acc: 0.8690\n",
            "Epoch 262/1000\n",
            "1/1 [==============================] - 0s 463ms/step - loss: 1.9172 - acc: 0.5697 - val_loss: 1.5293 - val_acc: 0.8690\n",
            "Epoch 263/1000\n",
            "1/1 [==============================] - 0s 481ms/step - loss: 1.9136 - acc: 0.5699 - val_loss: 1.5286 - val_acc: 0.8690\n",
            "Epoch 264/1000\n",
            "1/1 [==============================] - 0s 462ms/step - loss: 1.9146 - acc: 0.5699 - val_loss: 1.5279 - val_acc: 0.8690\n",
            "Epoch 265/1000\n",
            "1/1 [==============================] - 0s 462ms/step - loss: 1.9085 - acc: 0.5697 - val_loss: 1.5271 - val_acc: 0.8690\n",
            "Epoch 266/1000\n",
            "1/1 [==============================] - 0s 496ms/step - loss: 1.9135 - acc: 0.5697 - val_loss: 1.5264 - val_acc: 0.8690\n",
            "Epoch 267/1000\n",
            "1/1 [==============================] - 0s 465ms/step - loss: 1.9172 - acc: 0.5691 - val_loss: 1.5257 - val_acc: 0.8690\n",
            "Epoch 268/1000\n",
            "1/1 [==============================] - 0s 478ms/step - loss: 1.9078 - acc: 0.5695 - val_loss: 1.5249 - val_acc: 0.8690\n",
            "Epoch 269/1000\n",
            "1/1 [==============================] - 0s 468ms/step - loss: 1.9060 - acc: 0.5697 - val_loss: 1.5242 - val_acc: 0.8690\n",
            "Epoch 270/1000\n",
            "1/1 [==============================] - 1s 509ms/step - loss: 1.9063 - acc: 0.5699 - val_loss: 1.5235 - val_acc: 0.8690\n",
            "Epoch 271/1000\n",
            "1/1 [==============================] - 0s 445ms/step - loss: 1.9097 - acc: 0.5699 - val_loss: 1.5227 - val_acc: 0.8690\n",
            "Epoch 272/1000\n",
            "1/1 [==============================] - 0s 454ms/step - loss: 1.9075 - acc: 0.5699 - val_loss: 1.5220 - val_acc: 0.8690\n",
            "Epoch 273/1000\n",
            "1/1 [==============================] - 0s 465ms/step - loss: 1.9022 - acc: 0.5702 - val_loss: 1.5213 - val_acc: 0.8690\n",
            "Epoch 274/1000\n",
            "1/1 [==============================] - 0s 465ms/step - loss: 1.9018 - acc: 0.5697 - val_loss: 1.5206 - val_acc: 0.8690\n",
            "Epoch 275/1000\n",
            "1/1 [==============================] - 0s 463ms/step - loss: 1.9056 - acc: 0.5699 - val_loss: 1.5198 - val_acc: 0.8690\n",
            "Epoch 276/1000\n",
            "1/1 [==============================] - 0s 477ms/step - loss: 1.9049 - acc: 0.5697 - val_loss: 1.5191 - val_acc: 0.8690\n",
            "Epoch 277/1000\n",
            "1/1 [==============================] - 0s 466ms/step - loss: 1.9131 - acc: 0.5695 - val_loss: 1.5184 - val_acc: 0.8690\n",
            "Epoch 278/1000\n",
            "1/1 [==============================] - 0s 494ms/step - loss: 1.8999 - acc: 0.5701 - val_loss: 1.5177 - val_acc: 0.8690\n",
            "Epoch 279/1000\n",
            "1/1 [==============================] - 0s 467ms/step - loss: 1.9015 - acc: 0.5695 - val_loss: 1.5169 - val_acc: 0.8690\n",
            "Epoch 280/1000\n",
            "1/1 [==============================] - 0s 486ms/step - loss: 1.9016 - acc: 0.5701 - val_loss: 1.5162 - val_acc: 0.8690\n",
            "Epoch 281/1000\n",
            "1/1 [==============================] - 0s 455ms/step - loss: 1.9087 - acc: 0.5695 - val_loss: 1.5155 - val_acc: 0.8690\n",
            "Epoch 282/1000\n",
            "1/1 [==============================] - 0s 481ms/step - loss: 1.9023 - acc: 0.5697 - val_loss: 1.5148 - val_acc: 0.8690\n",
            "Epoch 283/1000\n",
            "1/1 [==============================] - 0s 488ms/step - loss: 1.9116 - acc: 0.5691 - val_loss: 1.5140 - val_acc: 0.8690\n",
            "Epoch 284/1000\n",
            "1/1 [==============================] - 0s 465ms/step - loss: 1.9098 - acc: 0.5699 - val_loss: 1.5133 - val_acc: 0.8690\n",
            "Epoch 285/1000\n",
            "1/1 [==============================] - 0s 448ms/step - loss: 1.8991 - acc: 0.5697 - val_loss: 1.5126 - val_acc: 0.8690\n",
            "Epoch 286/1000\n",
            "1/1 [==============================] - 0s 447ms/step - loss: 1.9005 - acc: 0.5701 - val_loss: 1.5119 - val_acc: 0.8690\n",
            "Epoch 287/1000\n",
            "1/1 [==============================] - 0s 482ms/step - loss: 1.8961 - acc: 0.5699 - val_loss: 1.5112 - val_acc: 0.8690\n",
            "Epoch 288/1000\n",
            "1/1 [==============================] - 0s 485ms/step - loss: 1.9002 - acc: 0.5701 - val_loss: 1.5104 - val_acc: 0.8690\n",
            "Epoch 289/1000\n",
            "1/1 [==============================] - 0s 479ms/step - loss: 1.9122 - acc: 0.5695 - val_loss: 1.5097 - val_acc: 0.8690\n",
            "Epoch 290/1000\n",
            "1/1 [==============================] - 0s 494ms/step - loss: 1.8962 - acc: 0.5699 - val_loss: 1.5090 - val_acc: 0.8690\n",
            "Epoch 291/1000\n",
            "1/1 [==============================] - 0s 457ms/step - loss: 1.8975 - acc: 0.5697 - val_loss: 1.5083 - val_acc: 0.8690\n",
            "Epoch 292/1000\n",
            "1/1 [==============================] - 0s 458ms/step - loss: 1.8931 - acc: 0.5699 - val_loss: 1.5076 - val_acc: 0.8690\n",
            "Epoch 293/1000\n",
            "1/1 [==============================] - 0s 480ms/step - loss: 1.8935 - acc: 0.5701 - val_loss: 1.5069 - val_acc: 0.8690\n",
            "Epoch 294/1000\n",
            "1/1 [==============================] - 0s 477ms/step - loss: 1.8931 - acc: 0.5697 - val_loss: 1.5061 - val_acc: 0.8690\n",
            "Epoch 295/1000\n",
            "1/1 [==============================] - 0s 488ms/step - loss: 1.8949 - acc: 0.5699 - val_loss: 1.5054 - val_acc: 0.8690\n",
            "Epoch 296/1000\n",
            "1/1 [==============================] - 0s 469ms/step - loss: 1.8939 - acc: 0.5701 - val_loss: 1.5047 - val_acc: 0.8690\n",
            "Epoch 297/1000\n",
            "1/1 [==============================] - 0s 464ms/step - loss: 1.8936 - acc: 0.5697 - val_loss: 1.5040 - val_acc: 0.8690\n",
            "Epoch 298/1000\n",
            "1/1 [==============================] - 0s 457ms/step - loss: 1.8925 - acc: 0.5697 - val_loss: 1.5033 - val_acc: 0.8690\n",
            "Epoch 299/1000\n",
            "1/1 [==============================] - 0s 465ms/step - loss: 1.8924 - acc: 0.5699 - val_loss: 1.5026 - val_acc: 0.8690\n",
            "Epoch 300/1000\n",
            "1/1 [==============================] - 0s 495ms/step - loss: 1.8916 - acc: 0.5697 - val_loss: 1.5019 - val_acc: 0.8690\n",
            "Epoch 301/1000\n",
            "1/1 [==============================] - 0s 476ms/step - loss: 1.9056 - acc: 0.5689 - val_loss: 1.5012 - val_acc: 0.8690\n",
            "Epoch 302/1000\n",
            "1/1 [==============================] - 0s 470ms/step - loss: 1.8922 - acc: 0.5699 - val_loss: 1.5004 - val_acc: 0.8690\n",
            "Epoch 303/1000\n",
            "1/1 [==============================] - 0s 464ms/step - loss: 1.8884 - acc: 0.5701 - val_loss: 1.4997 - val_acc: 0.8690\n",
            "Epoch 304/1000\n",
            "1/1 [==============================] - 1s 515ms/step - loss: 1.8903 - acc: 0.5699 - val_loss: 1.4990 - val_acc: 0.8690\n",
            "Epoch 305/1000\n",
            "1/1 [==============================] - 0s 449ms/step - loss: 1.8910 - acc: 0.5699 - val_loss: 1.4983 - val_acc: 0.8690\n",
            "Epoch 306/1000\n",
            "1/1 [==============================] - 0s 480ms/step - loss: 1.8921 - acc: 0.5697 - val_loss: 1.4976 - val_acc: 0.8690\n",
            "Epoch 307/1000\n",
            "1/1 [==============================] - 0s 471ms/step - loss: 1.9008 - acc: 0.5701 - val_loss: 1.4969 - val_acc: 0.8690\n",
            "Epoch 308/1000\n",
            "1/1 [==============================] - 0s 470ms/step - loss: 1.8885 - acc: 0.5695 - val_loss: 1.4962 - val_acc: 0.8690\n",
            "Epoch 309/1000\n",
            "1/1 [==============================] - 0s 458ms/step - loss: 1.8905 - acc: 0.5699 - val_loss: 1.4955 - val_acc: 0.8690\n",
            "Epoch 310/1000\n",
            "1/1 [==============================] - 0s 473ms/step - loss: 1.9095 - acc: 0.5691 - val_loss: 1.4948 - val_acc: 0.8690\n",
            "Epoch 311/1000\n",
            "1/1 [==============================] - 0s 465ms/step - loss: 1.8848 - acc: 0.5701 - val_loss: 1.4941 - val_acc: 0.8690\n",
            "Epoch 312/1000\n",
            "1/1 [==============================] - 0s 457ms/step - loss: 1.8899 - acc: 0.5699 - val_loss: 1.4934 - val_acc: 0.8690\n",
            "Epoch 313/1000\n",
            "1/1 [==============================] - 0s 480ms/step - loss: 1.8903 - acc: 0.5697 - val_loss: 1.4927 - val_acc: 0.8690\n",
            "Epoch 314/1000\n",
            "1/1 [==============================] - 0s 481ms/step - loss: 1.8880 - acc: 0.5697 - val_loss: 1.4920 - val_acc: 0.8690\n",
            "Epoch 315/1000\n",
            "1/1 [==============================] - 0s 486ms/step - loss: 1.8839 - acc: 0.5699 - val_loss: 1.4913 - val_acc: 0.8690\n",
            "Epoch 316/1000\n",
            "1/1 [==============================] - 0s 471ms/step - loss: 1.8843 - acc: 0.5701 - val_loss: 1.4906 - val_acc: 0.8690\n",
            "Epoch 317/1000\n",
            "1/1 [==============================] - 0s 481ms/step - loss: 1.8883 - acc: 0.5697 - val_loss: 1.4899 - val_acc: 0.8690\n",
            "Epoch 318/1000\n",
            "1/1 [==============================] - 0s 470ms/step - loss: 1.8832 - acc: 0.5701 - val_loss: 1.4892 - val_acc: 0.8690\n",
            "Epoch 319/1000\n",
            "1/1 [==============================] - 0s 484ms/step - loss: 1.8865 - acc: 0.5697 - val_loss: 1.4885 - val_acc: 0.8690\n",
            "Epoch 320/1000\n",
            "1/1 [==============================] - 0s 490ms/step - loss: 1.8831 - acc: 0.5699 - val_loss: 1.4878 - val_acc: 0.8690\n",
            "Epoch 321/1000\n",
            "1/1 [==============================] - 1s 504ms/step - loss: 1.8804 - acc: 0.5699 - val_loss: 1.4871 - val_acc: 0.8690\n",
            "Epoch 322/1000\n",
            "1/1 [==============================] - 1s 511ms/step - loss: 1.8926 - acc: 0.5699 - val_loss: 1.4864 - val_acc: 0.8690\n",
            "Epoch 323/1000\n",
            "1/1 [==============================] - 1s 511ms/step - loss: 1.8821 - acc: 0.5701 - val_loss: 1.4857 - val_acc: 0.8690\n",
            "Epoch 324/1000\n",
            "1/1 [==============================] - 0s 468ms/step - loss: 1.8780 - acc: 0.5699 - val_loss: 1.4850 - val_acc: 0.8690\n",
            "Epoch 325/1000\n",
            "1/1 [==============================] - 0s 449ms/step - loss: 1.8827 - acc: 0.5697 - val_loss: 1.4843 - val_acc: 0.8690\n",
            "Epoch 326/1000\n",
            "1/1 [==============================] - 0s 464ms/step - loss: 1.8802 - acc: 0.5701 - val_loss: 1.4836 - val_acc: 0.8690\n",
            "Epoch 327/1000\n",
            "1/1 [==============================] - 0s 462ms/step - loss: 1.8780 - acc: 0.5699 - val_loss: 1.4829 - val_acc: 0.8690\n",
            "Epoch 328/1000\n",
            "1/1 [==============================] - 0s 482ms/step - loss: 1.8781 - acc: 0.5697 - val_loss: 1.4822 - val_acc: 0.8690\n",
            "Epoch 329/1000\n",
            "1/1 [==============================] - 0s 489ms/step - loss: 1.8826 - acc: 0.5697 - val_loss: 1.4815 - val_acc: 0.8690\n",
            "Epoch 330/1000\n",
            "1/1 [==============================] - 0s 448ms/step - loss: 1.8762 - acc: 0.5699 - val_loss: 1.4808 - val_acc: 0.8690\n",
            "Epoch 331/1000\n",
            "1/1 [==============================] - 0s 468ms/step - loss: 1.8756 - acc: 0.5701 - val_loss: 1.4802 - val_acc: 0.8690\n",
            "Epoch 332/1000\n",
            "1/1 [==============================] - 1s 508ms/step - loss: 1.8957 - acc: 0.5693 - val_loss: 1.4795 - val_acc: 0.8690\n",
            "Epoch 333/1000\n",
            "1/1 [==============================] - 0s 443ms/step - loss: 1.8759 - acc: 0.5699 - val_loss: 1.4788 - val_acc: 0.8690\n",
            "Epoch 334/1000\n",
            "1/1 [==============================] - 0s 494ms/step - loss: 1.8744 - acc: 0.5701 - val_loss: 1.4781 - val_acc: 0.8690\n",
            "Epoch 335/1000\n",
            "1/1 [==============================] - 0s 471ms/step - loss: 1.8758 - acc: 0.5699 - val_loss: 1.4774 - val_acc: 0.8690\n",
            "Epoch 336/1000\n",
            "1/1 [==============================] - 0s 474ms/step - loss: 1.8756 - acc: 0.5699 - val_loss: 1.4767 - val_acc: 0.8690\n",
            "Epoch 337/1000\n",
            "1/1 [==============================] - 0s 470ms/step - loss: 1.8756 - acc: 0.5699 - val_loss: 1.4760 - val_acc: 0.8690\n",
            "Epoch 338/1000\n",
            "1/1 [==============================] - 0s 484ms/step - loss: 1.8748 - acc: 0.5701 - val_loss: 1.4753 - val_acc: 0.8690\n",
            "Epoch 339/1000\n",
            "1/1 [==============================] - 0s 469ms/step - loss: 1.8721 - acc: 0.5701 - val_loss: 1.4747 - val_acc: 0.8690\n",
            "Epoch 340/1000\n",
            "1/1 [==============================] - 0s 467ms/step - loss: 1.8743 - acc: 0.5701 - val_loss: 1.4740 - val_acc: 0.8690\n",
            "Epoch 341/1000\n",
            "1/1 [==============================] - 0s 485ms/step - loss: 1.8760 - acc: 0.5695 - val_loss: 1.4733 - val_acc: 0.8690\n",
            "Epoch 342/1000\n",
            "1/1 [==============================] - 0s 483ms/step - loss: 1.8723 - acc: 0.5701 - val_loss: 1.4726 - val_acc: 0.8690\n",
            "Epoch 343/1000\n",
            "1/1 [==============================] - 0s 467ms/step - loss: 1.8726 - acc: 0.5701 - val_loss: 1.4719 - val_acc: 0.8690\n",
            "Epoch 344/1000\n",
            "1/1 [==============================] - 0s 465ms/step - loss: 1.8764 - acc: 0.5699 - val_loss: 1.4712 - val_acc: 0.8690\n",
            "Epoch 345/1000\n",
            "1/1 [==============================] - 1s 503ms/step - loss: 1.8786 - acc: 0.5695 - val_loss: 1.4706 - val_acc: 0.8690\n",
            "Epoch 346/1000\n",
            "1/1 [==============================] - 0s 466ms/step - loss: 1.8683 - acc: 0.5702 - val_loss: 1.4699 - val_acc: 0.8690\n",
            "Epoch 347/1000\n",
            "1/1 [==============================] - 0s 476ms/step - loss: 1.8700 - acc: 0.5699 - val_loss: 1.4692 - val_acc: 0.8690\n",
            "Epoch 348/1000\n",
            "1/1 [==============================] - 0s 469ms/step - loss: 1.8740 - acc: 0.5693 - val_loss: 1.4685 - val_acc: 0.8690\n",
            "Epoch 349/1000\n",
            "1/1 [==============================] - 0s 467ms/step - loss: 1.8685 - acc: 0.5695 - val_loss: 1.4678 - val_acc: 0.8690\n",
            "Epoch 350/1000\n",
            "1/1 [==============================] - 0s 469ms/step - loss: 1.8812 - acc: 0.5695 - val_loss: 1.4672 - val_acc: 0.8690\n",
            "Epoch 351/1000\n",
            "1/1 [==============================] - 0s 490ms/step - loss: 1.8700 - acc: 0.5695 - val_loss: 1.4665 - val_acc: 0.8690\n",
            "Epoch 352/1000\n",
            "1/1 [==============================] - 0s 464ms/step - loss: 1.8689 - acc: 0.5701 - val_loss: 1.4658 - val_acc: 0.8690\n",
            "Epoch 353/1000\n",
            "1/1 [==============================] - 0s 466ms/step - loss: 1.8665 - acc: 0.5699 - val_loss: 1.4651 - val_acc: 0.8690\n",
            "Epoch 354/1000\n",
            "1/1 [==============================] - 0s 467ms/step - loss: 1.8676 - acc: 0.5695 - val_loss: 1.4645 - val_acc: 0.8690\n",
            "Epoch 355/1000\n",
            "1/1 [==============================] - 0s 472ms/step - loss: 1.8729 - acc: 0.5697 - val_loss: 1.4638 - val_acc: 0.8690\n",
            "Epoch 356/1000\n",
            "1/1 [==============================] - 0s 452ms/step - loss: 1.8726 - acc: 0.5699 - val_loss: 1.4631 - val_acc: 0.8690\n",
            "Epoch 357/1000\n",
            "1/1 [==============================] - 0s 486ms/step - loss: 1.8808 - acc: 0.5701 - val_loss: 1.4624 - val_acc: 0.8690\n",
            "Epoch 358/1000\n",
            "1/1 [==============================] - 0s 455ms/step - loss: 1.8661 - acc: 0.5695 - val_loss: 1.4618 - val_acc: 0.8690\n",
            "Epoch 359/1000\n",
            "1/1 [==============================] - 0s 475ms/step - loss: 1.8666 - acc: 0.5693 - val_loss: 1.4611 - val_acc: 0.8690\n",
            "Epoch 360/1000\n",
            "1/1 [==============================] - 0s 470ms/step - loss: 1.8658 - acc: 0.5697 - val_loss: 1.4604 - val_acc: 0.8690\n",
            "Epoch 361/1000\n",
            "1/1 [==============================] - 0s 472ms/step - loss: 1.8638 - acc: 0.5701 - val_loss: 1.4597 - val_acc: 0.8690\n",
            "Epoch 362/1000\n",
            "1/1 [==============================] - 0s 457ms/step - loss: 1.8653 - acc: 0.5699 - val_loss: 1.4591 - val_acc: 0.8690\n",
            "Epoch 363/1000\n",
            "1/1 [==============================] - 0s 459ms/step - loss: 1.8611 - acc: 0.5699 - val_loss: 1.4584 - val_acc: 0.8690\n",
            "Epoch 364/1000\n",
            "1/1 [==============================] - 0s 480ms/step - loss: 1.8639 - acc: 0.5697 - val_loss: 1.4577 - val_acc: 0.8690\n",
            "Epoch 365/1000\n",
            "1/1 [==============================] - 0s 480ms/step - loss: 1.8604 - acc: 0.5701 - val_loss: 1.4571 - val_acc: 0.8690\n",
            "Epoch 366/1000\n",
            "1/1 [==============================] - 0s 500ms/step - loss: 1.8628 - acc: 0.5699 - val_loss: 1.4564 - val_acc: 0.8690\n",
            "Epoch 367/1000\n",
            "1/1 [==============================] - 0s 455ms/step - loss: 1.8597 - acc: 0.5699 - val_loss: 1.4557 - val_acc: 0.8690\n",
            "Epoch 368/1000\n",
            "1/1 [==============================] - 0s 499ms/step - loss: 1.8661 - acc: 0.5699 - val_loss: 1.4551 - val_acc: 0.8690\n",
            "Epoch 369/1000\n",
            "1/1 [==============================] - 0s 494ms/step - loss: 1.8604 - acc: 0.5699 - val_loss: 1.4544 - val_acc: 0.8690\n",
            "Epoch 370/1000\n",
            "1/1 [==============================] - 0s 469ms/step - loss: 1.8586 - acc: 0.5701 - val_loss: 1.4537 - val_acc: 0.8690\n",
            "Epoch 371/1000\n",
            "1/1 [==============================] - 0s 451ms/step - loss: 1.8606 - acc: 0.5701 - val_loss: 1.4531 - val_acc: 0.8690\n",
            "Epoch 372/1000\n",
            "1/1 [==============================] - 0s 456ms/step - loss: 1.8593 - acc: 0.5701 - val_loss: 1.4524 - val_acc: 0.8690\n",
            "Epoch 373/1000\n",
            "1/1 [==============================] - 0s 455ms/step - loss: 1.8642 - acc: 0.5697 - val_loss: 1.4517 - val_acc: 0.8690\n",
            "Epoch 374/1000\n",
            "1/1 [==============================] - 0s 480ms/step - loss: 1.8633 - acc: 0.5697 - val_loss: 1.4511 - val_acc: 0.8690\n",
            "Epoch 375/1000\n",
            "1/1 [==============================] - 0s 481ms/step - loss: 1.8589 - acc: 0.5701 - val_loss: 1.4504 - val_acc: 0.8690\n",
            "Epoch 376/1000\n",
            "1/1 [==============================] - 0s 481ms/step - loss: 1.8577 - acc: 0.5695 - val_loss: 1.4497 - val_acc: 0.8690\n",
            "Epoch 377/1000\n",
            "1/1 [==============================] - 0s 482ms/step - loss: 1.8569 - acc: 0.5697 - val_loss: 1.4491 - val_acc: 0.8690\n",
            "Epoch 378/1000\n",
            "1/1 [==============================] - 0s 486ms/step - loss: 1.8549 - acc: 0.5701 - val_loss: 1.4484 - val_acc: 0.8690\n",
            "Epoch 379/1000\n",
            "1/1 [==============================] - 0s 471ms/step - loss: 1.8619 - acc: 0.5691 - val_loss: 1.4478 - val_acc: 0.8690\n",
            "Epoch 380/1000\n",
            "1/1 [==============================] - 0s 458ms/step - loss: 1.8549 - acc: 0.5699 - val_loss: 1.4471 - val_acc: 0.8690\n",
            "Epoch 381/1000\n",
            "1/1 [==============================] - 0s 456ms/step - loss: 1.8539 - acc: 0.5701 - val_loss: 1.4464 - val_acc: 0.8690\n",
            "Epoch 382/1000\n",
            "1/1 [==============================] - 0s 450ms/step - loss: 1.8557 - acc: 0.5699 - val_loss: 1.4458 - val_acc: 0.8690\n",
            "Epoch 383/1000\n",
            "1/1 [==============================] - 0s 485ms/step - loss: 1.8533 - acc: 0.5699 - val_loss: 1.4451 - val_acc: 0.8690\n",
            "Epoch 384/1000\n",
            "1/1 [==============================] - 0s 457ms/step - loss: 1.8572 - acc: 0.5699 - val_loss: 1.4445 - val_acc: 0.8690\n",
            "Epoch 385/1000\n",
            "1/1 [==============================] - 0s 482ms/step - loss: 1.8551 - acc: 0.5697 - val_loss: 1.4438 - val_acc: 0.8690\n",
            "Epoch 386/1000\n",
            "1/1 [==============================] - 0s 463ms/step - loss: 1.8520 - acc: 0.5697 - val_loss: 1.4431 - val_acc: 0.8690\n",
            "Epoch 387/1000\n",
            "1/1 [==============================] - 0s 476ms/step - loss: 1.8518 - acc: 0.5699 - val_loss: 1.4425 - val_acc: 0.8690\n",
            "Epoch 388/1000\n",
            "1/1 [==============================] - 0s 475ms/step - loss: 1.8517 - acc: 0.5699 - val_loss: 1.4418 - val_acc: 0.8690\n",
            "Epoch 389/1000\n",
            "1/1 [==============================] - 0s 488ms/step - loss: 1.8497 - acc: 0.5701 - val_loss: 1.4412 - val_acc: 0.8690\n",
            "Epoch 390/1000\n",
            "1/1 [==============================] - 0s 488ms/step - loss: 1.8530 - acc: 0.5699 - val_loss: 1.4405 - val_acc: 0.8690\n",
            "Epoch 391/1000\n",
            "1/1 [==============================] - 0s 449ms/step - loss: 1.8569 - acc: 0.5701 - val_loss: 1.4399 - val_acc: 0.8690\n",
            "Epoch 392/1000\n",
            "1/1 [==============================] - 0s 452ms/step - loss: 1.8500 - acc: 0.5699 - val_loss: 1.4392 - val_acc: 0.8690\n",
            "Epoch 393/1000\n",
            "1/1 [==============================] - 0s 444ms/step - loss: 1.8485 - acc: 0.5699 - val_loss: 1.4386 - val_acc: 0.8690\n",
            "Epoch 394/1000\n",
            "1/1 [==============================] - 0s 478ms/step - loss: 1.8495 - acc: 0.5701 - val_loss: 1.4379 - val_acc: 0.8690\n",
            "Epoch 395/1000\n",
            "1/1 [==============================] - 0s 471ms/step - loss: 1.8512 - acc: 0.5699 - val_loss: 1.4373 - val_acc: 0.8690\n",
            "Epoch 396/1000\n",
            "1/1 [==============================] - 0s 499ms/step - loss: 1.8556 - acc: 0.5695 - val_loss: 1.4366 - val_acc: 0.8690\n",
            "Epoch 397/1000\n",
            "1/1 [==============================] - 0s 497ms/step - loss: 1.8472 - acc: 0.5701 - val_loss: 1.4360 - val_acc: 0.8690\n",
            "Epoch 398/1000\n",
            "1/1 [==============================] - 0s 490ms/step - loss: 1.8576 - acc: 0.5697 - val_loss: 1.4353 - val_acc: 0.8690\n",
            "Epoch 399/1000\n",
            "1/1 [==============================] - 0s 453ms/step - loss: 1.8515 - acc: 0.5699 - val_loss: 1.4347 - val_acc: 0.8690\n",
            "Epoch 400/1000\n",
            "1/1 [==============================] - 0s 479ms/step - loss: 1.8463 - acc: 0.5701 - val_loss: 1.4340 - val_acc: 0.8690\n",
            "Epoch 401/1000\n",
            "1/1 [==============================] - 0s 468ms/step - loss: 1.8592 - acc: 0.5697 - val_loss: 1.4334 - val_acc: 0.8690\n",
            "Epoch 402/1000\n",
            "1/1 [==============================] - 0s 482ms/step - loss: 1.8505 - acc: 0.5701 - val_loss: 1.4327 - val_acc: 0.8690\n",
            "Epoch 403/1000\n",
            "1/1 [==============================] - 0s 483ms/step - loss: 1.8469 - acc: 0.5699 - val_loss: 1.4321 - val_acc: 0.8690\n",
            "Epoch 404/1000\n",
            "1/1 [==============================] - 0s 465ms/step - loss: 1.8496 - acc: 0.5699 - val_loss: 1.4314 - val_acc: 0.8690\n",
            "Epoch 405/1000\n",
            "1/1 [==============================] - 0s 463ms/step - loss: 1.8459 - acc: 0.5697 - val_loss: 1.4308 - val_acc: 0.8690\n",
            "Epoch 406/1000\n",
            "1/1 [==============================] - 0s 482ms/step - loss: 1.8436 - acc: 0.5701 - val_loss: 1.4302 - val_acc: 0.8690\n",
            "Epoch 407/1000\n",
            "1/1 [==============================] - 0s 465ms/step - loss: 1.8503 - acc: 0.5691 - val_loss: 1.4295 - val_acc: 0.8690\n",
            "Epoch 408/1000\n",
            "1/1 [==============================] - 0s 486ms/step - loss: 1.8452 - acc: 0.5702 - val_loss: 1.4289 - val_acc: 0.8690\n",
            "Epoch 409/1000\n",
            "1/1 [==============================] - 0s 482ms/step - loss: 1.8432 - acc: 0.5701 - val_loss: 1.4282 - val_acc: 0.8690\n",
            "Epoch 410/1000\n",
            "1/1 [==============================] - 0s 446ms/step - loss: 1.8434 - acc: 0.5701 - val_loss: 1.4276 - val_acc: 0.8690\n",
            "Epoch 411/1000\n",
            "1/1 [==============================] - 0s 482ms/step - loss: 1.8413 - acc: 0.5697 - val_loss: 1.4269 - val_acc: 0.8690\n",
            "Epoch 412/1000\n",
            "1/1 [==============================] - 0s 480ms/step - loss: 1.8448 - acc: 0.5697 - val_loss: 1.4263 - val_acc: 0.8690\n",
            "Epoch 413/1000\n",
            "1/1 [==============================] - 0s 482ms/step - loss: 1.8470 - acc: 0.5695 - val_loss: 1.4257 - val_acc: 0.8690\n",
            "Epoch 414/1000\n",
            "1/1 [==============================] - 0s 445ms/step - loss: 1.8558 - acc: 0.5699 - val_loss: 1.4250 - val_acc: 0.8690\n",
            "Epoch 415/1000\n",
            "1/1 [==============================] - 0s 474ms/step - loss: 1.8412 - acc: 0.5697 - val_loss: 1.4244 - val_acc: 0.8690\n",
            "Epoch 416/1000\n",
            "1/1 [==============================] - 0s 448ms/step - loss: 1.8394 - acc: 0.5701 - val_loss: 1.4238 - val_acc: 0.8690\n",
            "Epoch 417/1000\n",
            "1/1 [==============================] - 0s 465ms/step - loss: 1.8413 - acc: 0.5699 - val_loss: 1.4231 - val_acc: 0.8690\n",
            "Epoch 418/1000\n",
            "1/1 [==============================] - 0s 463ms/step - loss: 1.8445 - acc: 0.5701 - val_loss: 1.4225 - val_acc: 0.8690\n",
            "Epoch 419/1000\n",
            "1/1 [==============================] - 0s 464ms/step - loss: 1.8382 - acc: 0.5701 - val_loss: 1.4218 - val_acc: 0.8690\n",
            "Epoch 420/1000\n",
            "1/1 [==============================] - 0s 482ms/step - loss: 1.8394 - acc: 0.5701 - val_loss: 1.4212 - val_acc: 0.8690\n",
            "Epoch 421/1000\n",
            "1/1 [==============================] - 0s 468ms/step - loss: 1.8377 - acc: 0.5699 - val_loss: 1.4206 - val_acc: 0.8690\n",
            "Epoch 422/1000\n",
            "1/1 [==============================] - 0s 453ms/step - loss: 1.8501 - acc: 0.5699 - val_loss: 1.4199 - val_acc: 0.8690\n",
            "Epoch 423/1000\n",
            "1/1 [==============================] - 0s 452ms/step - loss: 1.8442 - acc: 0.5701 - val_loss: 1.4193 - val_acc: 0.8690\n",
            "Epoch 424/1000\n",
            "1/1 [==============================] - 0s 464ms/step - loss: 1.8364 - acc: 0.5701 - val_loss: 1.4187 - val_acc: 0.8690\n",
            "Epoch 425/1000\n",
            "1/1 [==============================] - 0s 482ms/step - loss: 1.8399 - acc: 0.5699 - val_loss: 1.4180 - val_acc: 0.8690\n",
            "Epoch 426/1000\n",
            "1/1 [==============================] - 0s 458ms/step - loss: 1.8362 - acc: 0.5699 - val_loss: 1.4174 - val_acc: 0.8690\n",
            "Epoch 427/1000\n",
            "1/1 [==============================] - 0s 473ms/step - loss: 1.8357 - acc: 0.5697 - val_loss: 1.4168 - val_acc: 0.8690\n",
            "Epoch 428/1000\n",
            "1/1 [==============================] - 0s 480ms/step - loss: 1.8355 - acc: 0.5701 - val_loss: 1.4162 - val_acc: 0.8690\n",
            "Epoch 429/1000\n",
            "1/1 [==============================] - 0s 463ms/step - loss: 1.8377 - acc: 0.5697 - val_loss: 1.4155 - val_acc: 0.8690\n",
            "Epoch 430/1000\n",
            "1/1 [==============================] - 1s 509ms/step - loss: 1.8395 - acc: 0.5702 - val_loss: 1.4149 - val_acc: 0.8690\n",
            "Epoch 431/1000\n",
            "1/1 [==============================] - 0s 482ms/step - loss: 1.8421 - acc: 0.5697 - val_loss: 1.4143 - val_acc: 0.8690\n",
            "Epoch 432/1000\n",
            "1/1 [==============================] - 0s 488ms/step - loss: 1.8347 - acc: 0.5697 - val_loss: 1.4136 - val_acc: 0.8690\n",
            "Epoch 433/1000\n",
            "1/1 [==============================] - 0s 464ms/step - loss: 1.8376 - acc: 0.5701 - val_loss: 1.4130 - val_acc: 0.8690\n",
            "Epoch 434/1000\n",
            "1/1 [==============================] - 0s 491ms/step - loss: 1.8334 - acc: 0.5701 - val_loss: 1.4124 - val_acc: 0.8690\n",
            "Epoch 435/1000\n",
            "1/1 [==============================] - 1s 516ms/step - loss: 1.8397 - acc: 0.5697 - val_loss: 1.4118 - val_acc: 0.8690\n",
            "Epoch 436/1000\n",
            "1/1 [==============================] - 0s 492ms/step - loss: 1.8383 - acc: 0.5695 - val_loss: 1.4111 - val_acc: 0.8690\n",
            "Epoch 437/1000\n",
            "1/1 [==============================] - 0s 494ms/step - loss: 1.8329 - acc: 0.5699 - val_loss: 1.4105 - val_acc: 0.8690\n",
            "Epoch 438/1000\n",
            "1/1 [==============================] - 0s 489ms/step - loss: 1.8405 - acc: 0.5697 - val_loss: 1.4099 - val_acc: 0.8690\n",
            "Epoch 439/1000\n",
            "1/1 [==============================] - 1s 500ms/step - loss: 1.8305 - acc: 0.5701 - val_loss: 1.4093 - val_acc: 0.8690\n",
            "Epoch 440/1000\n",
            "1/1 [==============================] - 1s 510ms/step - loss: 1.8339 - acc: 0.5701 - val_loss: 1.4086 - val_acc: 0.8690\n",
            "Epoch 441/1000\n",
            "1/1 [==============================] - 0s 486ms/step - loss: 1.8310 - acc: 0.5701 - val_loss: 1.4080 - val_acc: 0.8690\n",
            "Epoch 442/1000\n",
            "1/1 [==============================] - 0s 477ms/step - loss: 1.8342 - acc: 0.5701 - val_loss: 1.4074 - val_acc: 0.8690\n",
            "Epoch 443/1000\n",
            "1/1 [==============================] - 0s 469ms/step - loss: 1.8325 - acc: 0.5699 - val_loss: 1.4068 - val_acc: 0.8690\n",
            "Epoch 444/1000\n",
            "1/1 [==============================] - 0s 480ms/step - loss: 1.8315 - acc: 0.5697 - val_loss: 1.4061 - val_acc: 0.8690\n",
            "Epoch 445/1000\n",
            "1/1 [==============================] - 0s 460ms/step - loss: 1.8319 - acc: 0.5697 - val_loss: 1.4055 - val_acc: 0.8690\n",
            "Epoch 446/1000\n",
            "1/1 [==============================] - 0s 457ms/step - loss: 1.8287 - acc: 0.5701 - val_loss: 1.4049 - val_acc: 0.8690\n",
            "Epoch 447/1000\n",
            "1/1 [==============================] - 0s 471ms/step - loss: 1.8295 - acc: 0.5699 - val_loss: 1.4043 - val_acc: 0.8690\n",
            "Epoch 448/1000\n",
            "1/1 [==============================] - 0s 498ms/step - loss: 1.8280 - acc: 0.5701 - val_loss: 1.4037 - val_acc: 0.8690\n",
            "Epoch 449/1000\n",
            "1/1 [==============================] - 0s 483ms/step - loss: 1.8266 - acc: 0.5701 - val_loss: 1.4031 - val_acc: 0.8690\n",
            "Epoch 450/1000\n",
            "1/1 [==============================] - 0s 462ms/step - loss: 1.8318 - acc: 0.5699 - val_loss: 1.4024 - val_acc: 0.8690\n",
            "Epoch 451/1000\n",
            "1/1 [==============================] - 0s 480ms/step - loss: 1.8277 - acc: 0.5699 - val_loss: 1.4018 - val_acc: 0.8690\n",
            "Epoch 452/1000\n",
            "1/1 [==============================] - 1s 501ms/step - loss: 1.8267 - acc: 0.5701 - val_loss: 1.4012 - val_acc: 0.8690\n",
            "Epoch 453/1000\n",
            "1/1 [==============================] - 0s 482ms/step - loss: 1.8284 - acc: 0.5699 - val_loss: 1.4006 - val_acc: 0.8690\n",
            "Epoch 454/1000\n",
            "1/1 [==============================] - 0s 471ms/step - loss: 1.8282 - acc: 0.5699 - val_loss: 1.4000 - val_acc: 0.8690\n",
            "Epoch 455/1000\n",
            "1/1 [==============================] - 0s 478ms/step - loss: 1.8248 - acc: 0.5699 - val_loss: 1.3994 - val_acc: 0.8690\n",
            "Epoch 456/1000\n",
            "1/1 [==============================] - 0s 465ms/step - loss: 1.8306 - acc: 0.5693 - val_loss: 1.3987 - val_acc: 0.8690\n",
            "Epoch 457/1000\n",
            "1/1 [==============================] - 0s 482ms/step - loss: 1.8252 - acc: 0.5699 - val_loss: 1.3981 - val_acc: 0.8690\n",
            "Epoch 458/1000\n",
            "1/1 [==============================] - 0s 465ms/step - loss: 1.8250 - acc: 0.5699 - val_loss: 1.3975 - val_acc: 0.8690\n",
            "Epoch 459/1000\n",
            "1/1 [==============================] - 0s 470ms/step - loss: 1.8236 - acc: 0.5699 - val_loss: 1.3969 - val_acc: 0.8690\n",
            "Epoch 460/1000\n",
            "1/1 [==============================] - 0s 457ms/step - loss: 1.8227 - acc: 0.5701 - val_loss: 1.3963 - val_acc: 0.8690\n",
            "Epoch 461/1000\n",
            "1/1 [==============================] - 0s 486ms/step - loss: 1.8225 - acc: 0.5701 - val_loss: 1.3957 - val_acc: 0.8690\n",
            "Epoch 462/1000\n",
            "1/1 [==============================] - 0s 471ms/step - loss: 1.8247 - acc: 0.5699 - val_loss: 1.3951 - val_acc: 0.8690\n",
            "Epoch 463/1000\n",
            "1/1 [==============================] - 0s 460ms/step - loss: 1.8216 - acc: 0.5701 - val_loss: 1.3945 - val_acc: 0.8690\n",
            "Epoch 464/1000\n",
            "1/1 [==============================] - 0s 487ms/step - loss: 1.8211 - acc: 0.5701 - val_loss: 1.3939 - val_acc: 0.8690\n",
            "Epoch 465/1000\n",
            "1/1 [==============================] - 0s 466ms/step - loss: 1.8225 - acc: 0.5699 - val_loss: 1.3933 - val_acc: 0.8690\n",
            "Epoch 466/1000\n",
            "1/1 [==============================] - 0s 478ms/step - loss: 1.8384 - acc: 0.5695 - val_loss: 1.3926 - val_acc: 0.8690\n",
            "Epoch 467/1000\n",
            "1/1 [==============================] - 0s 473ms/step - loss: 1.8205 - acc: 0.5699 - val_loss: 1.3920 - val_acc: 0.8690\n",
            "Epoch 468/1000\n",
            "1/1 [==============================] - 0s 489ms/step - loss: 1.8225 - acc: 0.5699 - val_loss: 1.3914 - val_acc: 0.8690\n",
            "Epoch 469/1000\n",
            "1/1 [==============================] - 0s 478ms/step - loss: 1.8191 - acc: 0.5701 - val_loss: 1.3908 - val_acc: 0.8690\n",
            "Epoch 470/1000\n",
            "1/1 [==============================] - 1s 502ms/step - loss: 1.8249 - acc: 0.5699 - val_loss: 1.3902 - val_acc: 0.8690\n",
            "Epoch 471/1000\n",
            "1/1 [==============================] - 1s 517ms/step - loss: 1.8188 - acc: 0.5701 - val_loss: 1.3896 - val_acc: 0.8690\n",
            "Epoch 472/1000\n",
            "1/1 [==============================] - 0s 485ms/step - loss: 1.8180 - acc: 0.5701 - val_loss: 1.3890 - val_acc: 0.8690\n",
            "Epoch 473/1000\n",
            "1/1 [==============================] - 0s 462ms/step - loss: 1.8177 - acc: 0.5699 - val_loss: 1.3884 - val_acc: 0.8690\n",
            "Epoch 474/1000\n",
            "1/1 [==============================] - 0s 490ms/step - loss: 1.8199 - acc: 0.5699 - val_loss: 1.3878 - val_acc: 0.8690\n",
            "Epoch 475/1000\n",
            "1/1 [==============================] - 0s 463ms/step - loss: 1.8215 - acc: 0.5695 - val_loss: 1.3872 - val_acc: 0.8690\n",
            "Epoch 476/1000\n",
            "1/1 [==============================] - 0s 484ms/step - loss: 1.8179 - acc: 0.5701 - val_loss: 1.3866 - val_acc: 0.8690\n",
            "Epoch 477/1000\n",
            "1/1 [==============================] - 0s 454ms/step - loss: 1.8173 - acc: 0.5699 - val_loss: 1.3860 - val_acc: 0.8690\n",
            "Epoch 478/1000\n",
            "1/1 [==============================] - 0s 492ms/step - loss: 1.8231 - acc: 0.5697 - val_loss: 1.3854 - val_acc: 0.8690\n",
            "Epoch 479/1000\n",
            "1/1 [==============================] - 0s 474ms/step - loss: 1.8166 - acc: 0.5699 - val_loss: 1.3848 - val_acc: 0.8690\n",
            "Epoch 480/1000\n",
            "1/1 [==============================] - 0s 463ms/step - loss: 1.8150 - acc: 0.5701 - val_loss: 1.3842 - val_acc: 0.8690\n",
            "Epoch 481/1000\n",
            "1/1 [==============================] - 0s 497ms/step - loss: 1.8160 - acc: 0.5699 - val_loss: 1.3836 - val_acc: 0.8690\n",
            "Epoch 482/1000\n",
            "1/1 [==============================] - 0s 476ms/step - loss: 1.8192 - acc: 0.5699 - val_loss: 1.3830 - val_acc: 0.8690\n",
            "Epoch 483/1000\n",
            "1/1 [==============================] - 0s 484ms/step - loss: 1.8165 - acc: 0.5693 - val_loss: 1.3824 - val_acc: 0.8690\n",
            "Epoch 484/1000\n",
            "1/1 [==============================] - 0s 471ms/step - loss: 1.8143 - acc: 0.5701 - val_loss: 1.3818 - val_acc: 0.8690\n",
            "Epoch 485/1000\n",
            "1/1 [==============================] - 0s 484ms/step - loss: 1.8138 - acc: 0.5697 - val_loss: 1.3812 - val_acc: 0.8690\n",
            "Epoch 486/1000\n",
            "1/1 [==============================] - 0s 476ms/step - loss: 1.8144 - acc: 0.5699 - val_loss: 1.3806 - val_acc: 0.8690\n",
            "Epoch 487/1000\n",
            "1/1 [==============================] - 0s 472ms/step - loss: 1.8152 - acc: 0.5697 - val_loss: 1.3800 - val_acc: 0.8690\n",
            "Epoch 488/1000\n",
            "1/1 [==============================] - 0s 478ms/step - loss: 1.8153 - acc: 0.5697 - val_loss: 1.3794 - val_acc: 0.8690\n",
            "Epoch 489/1000\n",
            "1/1 [==============================] - 1s 500ms/step - loss: 1.8143 - acc: 0.5697 - val_loss: 1.3788 - val_acc: 0.8690\n",
            "Epoch 490/1000\n",
            "1/1 [==============================] - 0s 468ms/step - loss: 1.8137 - acc: 0.5699 - val_loss: 1.3782 - val_acc: 0.8690\n",
            "Epoch 491/1000\n",
            "1/1 [==============================] - 0s 476ms/step - loss: 1.8121 - acc: 0.5699 - val_loss: 1.3776 - val_acc: 0.8690\n",
            "Epoch 492/1000\n",
            "1/1 [==============================] - 0s 466ms/step - loss: 1.8150 - acc: 0.5699 - val_loss: 1.3770 - val_acc: 0.8690\n",
            "Epoch 493/1000\n",
            "1/1 [==============================] - 0s 492ms/step - loss: 1.8102 - acc: 0.5701 - val_loss: 1.3765 - val_acc: 0.8690\n",
            "Epoch 494/1000\n",
            "1/1 [==============================] - 0s 467ms/step - loss: 1.8140 - acc: 0.5701 - val_loss: 1.3759 - val_acc: 0.8690\n",
            "Epoch 495/1000\n",
            "1/1 [==============================] - 0s 486ms/step - loss: 1.8133 - acc: 0.5699 - val_loss: 1.3753 - val_acc: 0.8690\n",
            "Epoch 496/1000\n",
            "1/1 [==============================] - 0s 471ms/step - loss: 1.8094 - acc: 0.5701 - val_loss: 1.3747 - val_acc: 0.8690\n",
            "Epoch 497/1000\n",
            "1/1 [==============================] - 0s 479ms/step - loss: 1.8102 - acc: 0.5701 - val_loss: 1.3741 - val_acc: 0.8690\n",
            "Epoch 498/1000\n",
            "1/1 [==============================] - 0s 479ms/step - loss: 1.8120 - acc: 0.5697 - val_loss: 1.3735 - val_acc: 0.8690\n",
            "Epoch 499/1000\n",
            "1/1 [==============================] - 1s 504ms/step - loss: 1.8111 - acc: 0.5699 - val_loss: 1.3729 - val_acc: 0.8690\n",
            "Epoch 500/1000\n",
            "1/1 [==============================] - 0s 462ms/step - loss: 1.8086 - acc: 0.5695 - val_loss: 1.3723 - val_acc: 0.8690\n",
            "Epoch 501/1000\n",
            "1/1 [==============================] - 0s 477ms/step - loss: 1.8114 - acc: 0.5699 - val_loss: 1.3717 - val_acc: 0.8690\n",
            "Epoch 502/1000\n",
            "1/1 [==============================] - 1s 520ms/step - loss: 1.8115 - acc: 0.5699 - val_loss: 1.3712 - val_acc: 0.8690\n",
            "Epoch 503/1000\n",
            "1/1 [==============================] - 0s 489ms/step - loss: 1.8085 - acc: 0.5699 - val_loss: 1.3706 - val_acc: 0.8690\n",
            "Epoch 504/1000\n",
            "1/1 [==============================] - 0s 489ms/step - loss: 1.8086 - acc: 0.5697 - val_loss: 1.3700 - val_acc: 0.8690\n",
            "Epoch 505/1000\n",
            "1/1 [==============================] - 0s 455ms/step - loss: 1.8086 - acc: 0.5701 - val_loss: 1.3694 - val_acc: 0.8690\n",
            "Epoch 506/1000\n",
            "1/1 [==============================] - 0s 469ms/step - loss: 1.8070 - acc: 0.5701 - val_loss: 1.3688 - val_acc: 0.8690\n",
            "Epoch 507/1000\n",
            "1/1 [==============================] - 0s 474ms/step - loss: 1.8056 - acc: 0.5701 - val_loss: 1.3682 - val_acc: 0.8690\n",
            "Epoch 508/1000\n",
            "1/1 [==============================] - 0s 481ms/step - loss: 1.8080 - acc: 0.5701 - val_loss: 1.3676 - val_acc: 0.8690\n",
            "Epoch 509/1000\n",
            "1/1 [==============================] - 0s 478ms/step - loss: 1.8091 - acc: 0.5697 - val_loss: 1.3671 - val_acc: 0.8690\n",
            "Epoch 510/1000\n",
            "1/1 [==============================] - 1s 503ms/step - loss: 1.8092 - acc: 0.5699 - val_loss: 1.3665 - val_acc: 0.8690\n",
            "Epoch 511/1000\n",
            "1/1 [==============================] - 0s 462ms/step - loss: 1.8140 - acc: 0.5697 - val_loss: 1.3659 - val_acc: 0.8690\n",
            "Epoch 512/1000\n",
            "1/1 [==============================] - 1s 501ms/step - loss: 1.8064 - acc: 0.5701 - val_loss: 1.3653 - val_acc: 0.8690\n",
            "Epoch 513/1000\n",
            "1/1 [==============================] - 0s 471ms/step - loss: 1.8048 - acc: 0.5697 - val_loss: 1.3647 - val_acc: 0.8690\n",
            "Epoch 514/1000\n",
            "1/1 [==============================] - 0s 496ms/step - loss: 1.8050 - acc: 0.5699 - val_loss: 1.3642 - val_acc: 0.8690\n",
            "Epoch 515/1000\n",
            "1/1 [==============================] - 1s 501ms/step - loss: 1.8036 - acc: 0.5699 - val_loss: 1.3636 - val_acc: 0.8690\n",
            "Epoch 516/1000\n",
            "1/1 [==============================] - 0s 484ms/step - loss: 1.8066 - acc: 0.5693 - val_loss: 1.3630 - val_acc: 0.8690\n",
            "Epoch 517/1000\n",
            "1/1 [==============================] - 0s 453ms/step - loss: 1.8043 - acc: 0.5699 - val_loss: 1.3624 - val_acc: 0.8690\n",
            "Epoch 518/1000\n",
            "1/1 [==============================] - 0s 467ms/step - loss: 1.8041 - acc: 0.5699 - val_loss: 1.3618 - val_acc: 0.8690\n",
            "Epoch 519/1000\n",
            "1/1 [==============================] - 0s 470ms/step - loss: 1.8026 - acc: 0.5697 - val_loss: 1.3613 - val_acc: 0.8690\n",
            "Epoch 520/1000\n",
            "1/1 [==============================] - 0s 486ms/step - loss: 1.8010 - acc: 0.5699 - val_loss: 1.3607 - val_acc: 0.8690\n",
            "Epoch 521/1000\n",
            "1/1 [==============================] - 0s 477ms/step - loss: 1.8011 - acc: 0.5699 - val_loss: 1.3601 - val_acc: 0.8690\n",
            "Epoch 522/1000\n",
            "1/1 [==============================] - 0s 476ms/step - loss: 1.8049 - acc: 0.5697 - val_loss: 1.3595 - val_acc: 0.8690\n",
            "Epoch 523/1000\n",
            "1/1 [==============================] - 0s 461ms/step - loss: 1.8039 - acc: 0.5701 - val_loss: 1.3590 - val_acc: 0.8690\n",
            "Epoch 524/1000\n",
            "1/1 [==============================] - 1s 501ms/step - loss: 1.7993 - acc: 0.5701 - val_loss: 1.3584 - val_acc: 0.8690\n",
            "Epoch 525/1000\n",
            "1/1 [==============================] - 0s 491ms/step - loss: 1.8035 - acc: 0.5701 - val_loss: 1.3578 - val_acc: 0.8690\n",
            "Epoch 526/1000\n",
            "1/1 [==============================] - 0s 477ms/step - loss: 1.8003 - acc: 0.5699 - val_loss: 1.3572 - val_acc: 0.8690\n",
            "Epoch 527/1000\n",
            "1/1 [==============================] - 0s 483ms/step - loss: 1.8049 - acc: 0.5701 - val_loss: 1.3567 - val_acc: 0.8690\n",
            "Epoch 528/1000\n",
            "1/1 [==============================] - 0s 458ms/step - loss: 1.7991 - acc: 0.5699 - val_loss: 1.3561 - val_acc: 0.8690\n",
            "Epoch 529/1000\n",
            "1/1 [==============================] - 0s 488ms/step - loss: 1.8025 - acc: 0.5699 - val_loss: 1.3555 - val_acc: 0.8690\n",
            "Epoch 530/1000\n",
            "1/1 [==============================] - 0s 476ms/step - loss: 1.8031 - acc: 0.5697 - val_loss: 1.3550 - val_acc: 0.8690\n",
            "Epoch 531/1000\n",
            "1/1 [==============================] - 0s 470ms/step - loss: 1.7970 - acc: 0.5701 - val_loss: 1.3544 - val_acc: 0.8690\n",
            "Epoch 532/1000\n",
            "1/1 [==============================] - 0s 461ms/step - loss: 1.8014 - acc: 0.5697 - val_loss: 1.3538 - val_acc: 0.8690\n",
            "Epoch 533/1000\n",
            "1/1 [==============================] - 0s 483ms/step - loss: 1.8026 - acc: 0.5697 - val_loss: 1.3532 - val_acc: 0.8690\n",
            "Epoch 534/1000\n",
            "1/1 [==============================] - 0s 468ms/step - loss: 1.7974 - acc: 0.5701 - val_loss: 1.3527 - val_acc: 0.8690\n",
            "Epoch 535/1000\n",
            "1/1 [==============================] - 0s 487ms/step - loss: 1.8024 - acc: 0.5699 - val_loss: 1.3521 - val_acc: 0.8690\n",
            "Epoch 536/1000\n",
            "1/1 [==============================] - 0s 473ms/step - loss: 1.7964 - acc: 0.5697 - val_loss: 1.3515 - val_acc: 0.8690\n",
            "Epoch 537/1000\n",
            "1/1 [==============================] - 0s 492ms/step - loss: 1.7963 - acc: 0.5701 - val_loss: 1.3510 - val_acc: 0.8690\n",
            "Epoch 538/1000\n",
            "1/1 [==============================] - 0s 457ms/step - loss: 1.7980 - acc: 0.5699 - val_loss: 1.3504 - val_acc: 0.8690\n",
            "Epoch 539/1000\n",
            "1/1 [==============================] - 0s 452ms/step - loss: 1.7966 - acc: 0.5697 - val_loss: 1.3498 - val_acc: 0.8690\n",
            "Epoch 540/1000\n",
            "1/1 [==============================] - 0s 462ms/step - loss: 1.7955 - acc: 0.5701 - val_loss: 1.3493 - val_acc: 0.8690\n",
            "Epoch 541/1000\n",
            "1/1 [==============================] - 0s 453ms/step - loss: 1.7973 - acc: 0.5697 - val_loss: 1.3487 - val_acc: 0.8690\n",
            "Epoch 542/1000\n",
            "1/1 [==============================] - 0s 464ms/step - loss: 1.7991 - acc: 0.5699 - val_loss: 1.3481 - val_acc: 0.8690\n",
            "Epoch 543/1000\n",
            "1/1 [==============================] - 0s 469ms/step - loss: 1.7995 - acc: 0.5697 - val_loss: 1.3476 - val_acc: 0.8690\n",
            "Epoch 544/1000\n",
            "1/1 [==============================] - 1s 502ms/step - loss: 1.7976 - acc: 0.5695 - val_loss: 1.3470 - val_acc: 0.8690\n",
            "Epoch 545/1000\n",
            "1/1 [==============================] - 0s 486ms/step - loss: 1.7955 - acc: 0.5695 - val_loss: 1.3465 - val_acc: 0.8690\n",
            "Epoch 546/1000\n",
            "1/1 [==============================] - 0s 480ms/step - loss: 1.7965 - acc: 0.5699 - val_loss: 1.3459 - val_acc: 0.8690\n",
            "Epoch 547/1000\n",
            "1/1 [==============================] - 1s 516ms/step - loss: 1.7934 - acc: 0.5702 - val_loss: 1.3453 - val_acc: 0.8690\n",
            "Epoch 548/1000\n",
            "1/1 [==============================] - 0s 470ms/step - loss: 1.7946 - acc: 0.5699 - val_loss: 1.3448 - val_acc: 0.8690\n",
            "Epoch 549/1000\n",
            "1/1 [==============================] - 0s 463ms/step - loss: 1.7952 - acc: 0.5695 - val_loss: 1.3442 - val_acc: 0.8690\n",
            "Epoch 550/1000\n",
            "1/1 [==============================] - 0s 466ms/step - loss: 1.7953 - acc: 0.5701 - val_loss: 1.3437 - val_acc: 0.8690\n",
            "Epoch 551/1000\n",
            "1/1 [==============================] - 0s 476ms/step - loss: 1.7919 - acc: 0.5701 - val_loss: 1.3431 - val_acc: 0.8690\n",
            "Epoch 552/1000\n",
            "1/1 [==============================] - 0s 474ms/step - loss: 1.8019 - acc: 0.5697 - val_loss: 1.3425 - val_acc: 0.8690\n",
            "Epoch 553/1000\n",
            "1/1 [==============================] - 0s 444ms/step - loss: 1.7900 - acc: 0.5699 - val_loss: 1.3420 - val_acc: 0.8690\n",
            "Epoch 554/1000\n",
            "1/1 [==============================] - 0s 481ms/step - loss: 1.8027 - acc: 0.5701 - val_loss: 1.3414 - val_acc: 0.8690\n",
            "Epoch 555/1000\n",
            "1/1 [==============================] - 0s 468ms/step - loss: 1.7896 - acc: 0.5699 - val_loss: 1.3409 - val_acc: 0.8690\n",
            "Epoch 556/1000\n",
            "1/1 [==============================] - 0s 474ms/step - loss: 1.7897 - acc: 0.5699 - val_loss: 1.3403 - val_acc: 0.8690\n",
            "Epoch 557/1000\n",
            "1/1 [==============================] - 1s 506ms/step - loss: 1.7904 - acc: 0.5701 - val_loss: 1.3398 - val_acc: 0.8690\n",
            "Epoch 558/1000\n",
            "1/1 [==============================] - 0s 469ms/step - loss: 1.7884 - acc: 0.5699 - val_loss: 1.3392 - val_acc: 0.8690\n",
            "Epoch 559/1000\n",
            "1/1 [==============================] - 0s 469ms/step - loss: 1.7931 - acc: 0.5699 - val_loss: 1.3386 - val_acc: 0.8690\n",
            "Epoch 560/1000\n",
            "1/1 [==============================] - 0s 494ms/step - loss: 1.7877 - acc: 0.5699 - val_loss: 1.3381 - val_acc: 0.8690\n",
            "Epoch 561/1000\n",
            "1/1 [==============================] - 0s 458ms/step - loss: 1.7879 - acc: 0.5701 - val_loss: 1.3375 - val_acc: 0.8690\n",
            "Epoch 562/1000\n",
            "1/1 [==============================] - 0s 472ms/step - loss: 1.7879 - acc: 0.5699 - val_loss: 1.3370 - val_acc: 0.8690\n",
            "Epoch 563/1000\n",
            "1/1 [==============================] - 0s 489ms/step - loss: 1.7921 - acc: 0.5699 - val_loss: 1.3364 - val_acc: 0.8690\n",
            "Epoch 564/1000\n",
            "1/1 [==============================] - 0s 482ms/step - loss: 1.7884 - acc: 0.5697 - val_loss: 1.3359 - val_acc: 0.8690\n",
            "Epoch 565/1000\n",
            "1/1 [==============================] - 0s 469ms/step - loss: 1.7988 - acc: 0.5701 - val_loss: 1.3353 - val_acc: 0.8690\n",
            "Epoch 566/1000\n",
            "1/1 [==============================] - 0s 477ms/step - loss: 1.7903 - acc: 0.5699 - val_loss: 1.3348 - val_acc: 0.8690\n",
            "Epoch 567/1000\n",
            "1/1 [==============================] - 0s 470ms/step - loss: 1.7853 - acc: 0.5701 - val_loss: 1.3342 - val_acc: 0.8690\n",
            "Epoch 568/1000\n",
            "1/1 [==============================] - 0s 461ms/step - loss: 1.7883 - acc: 0.5697 - val_loss: 1.3337 - val_acc: 0.8690\n",
            "Epoch 569/1000\n",
            "1/1 [==============================] - 0s 494ms/step - loss: 1.7899 - acc: 0.5697 - val_loss: 1.3331 - val_acc: 0.8690\n",
            "Epoch 570/1000\n",
            "1/1 [==============================] - 0s 477ms/step - loss: 1.7903 - acc: 0.5697 - val_loss: 1.3326 - val_acc: 0.8690\n",
            "Epoch 571/1000\n",
            "1/1 [==============================] - 0s 467ms/step - loss: 1.7853 - acc: 0.5701 - val_loss: 1.3320 - val_acc: 0.8690\n",
            "Epoch 572/1000\n",
            "1/1 [==============================] - 0s 466ms/step - loss: 1.7845 - acc: 0.5699 - val_loss: 1.3315 - val_acc: 0.8690\n",
            "Epoch 573/1000\n",
            "1/1 [==============================] - 0s 482ms/step - loss: 1.7936 - acc: 0.5699 - val_loss: 1.3309 - val_acc: 0.8690\n",
            "Epoch 574/1000\n",
            "1/1 [==============================] - 0s 470ms/step - loss: 1.7849 - acc: 0.5701 - val_loss: 1.3304 - val_acc: 0.8690\n",
            "Epoch 575/1000\n",
            "1/1 [==============================] - 0s 476ms/step - loss: 1.7836 - acc: 0.5701 - val_loss: 1.3299 - val_acc: 0.8690\n",
            "Epoch 576/1000\n",
            "1/1 [==============================] - 0s 475ms/step - loss: 1.7823 - acc: 0.5701 - val_loss: 1.3293 - val_acc: 0.8690\n",
            "Epoch 577/1000\n",
            "1/1 [==============================] - 0s 480ms/step - loss: 1.7927 - acc: 0.5701 - val_loss: 1.3288 - val_acc: 0.8690\n",
            "Epoch 578/1000\n",
            "1/1 [==============================] - 1s 508ms/step - loss: 1.7948 - acc: 0.5695 - val_loss: 1.3282 - val_acc: 0.8690\n",
            "Epoch 579/1000\n",
            "1/1 [==============================] - 0s 449ms/step - loss: 1.7821 - acc: 0.5699 - val_loss: 1.3277 - val_acc: 0.8690\n",
            "Epoch 580/1000\n",
            "1/1 [==============================] - 0s 473ms/step - loss: 1.7839 - acc: 0.5701 - val_loss: 1.3271 - val_acc: 0.8690\n",
            "Epoch 581/1000\n",
            "1/1 [==============================] - 0s 459ms/step - loss: 1.7812 - acc: 0.5702 - val_loss: 1.3266 - val_acc: 0.8690\n",
            "Epoch 582/1000\n",
            "1/1 [==============================] - 0s 457ms/step - loss: 1.7830 - acc: 0.5699 - val_loss: 1.3261 - val_acc: 0.8690\n",
            "Epoch 583/1000\n",
            "1/1 [==============================] - 0s 458ms/step - loss: 1.7804 - acc: 0.5699 - val_loss: 1.3255 - val_acc: 0.8690\n",
            "Epoch 584/1000\n",
            "1/1 [==============================] - 1s 502ms/step - loss: 1.7801 - acc: 0.5699 - val_loss: 1.3250 - val_acc: 0.8690\n",
            "Epoch 585/1000\n",
            "1/1 [==============================] - 0s 474ms/step - loss: 1.7832 - acc: 0.5699 - val_loss: 1.3244 - val_acc: 0.8690\n",
            "Epoch 586/1000\n",
            "1/1 [==============================] - 0s 485ms/step - loss: 1.7795 - acc: 0.5701 - val_loss: 1.3239 - val_acc: 0.8690\n",
            "Epoch 587/1000\n",
            "1/1 [==============================] - 0s 494ms/step - loss: 1.7875 - acc: 0.5701 - val_loss: 1.3234 - val_acc: 0.8690\n",
            "Epoch 588/1000\n",
            "1/1 [==============================] - 0s 496ms/step - loss: 1.7795 - acc: 0.5697 - val_loss: 1.3228 - val_acc: 0.8690\n",
            "Epoch 589/1000\n",
            "1/1 [==============================] - 0s 476ms/step - loss: 1.7791 - acc: 0.5699 - val_loss: 1.3223 - val_acc: 0.8690\n",
            "Epoch 590/1000\n",
            "1/1 [==============================] - 0s 496ms/step - loss: 1.7786 - acc: 0.5699 - val_loss: 1.3217 - val_acc: 0.8690\n",
            "Epoch 591/1000\n",
            "1/1 [==============================] - 1s 501ms/step - loss: 1.7778 - acc: 0.5701 - val_loss: 1.3212 - val_acc: 0.8690\n",
            "Epoch 592/1000\n",
            "1/1 [==============================] - 0s 494ms/step - loss: 1.7790 - acc: 0.5701 - val_loss: 1.3207 - val_acc: 0.8690\n",
            "Epoch 593/1000\n",
            "1/1 [==============================] - 0s 483ms/step - loss: 1.7772 - acc: 0.5701 - val_loss: 1.3201 - val_acc: 0.8690\n",
            "Epoch 594/1000\n",
            "1/1 [==============================] - 1s 503ms/step - loss: 1.7778 - acc: 0.5699 - val_loss: 1.3196 - val_acc: 0.8690\n",
            "Epoch 595/1000\n",
            "1/1 [==============================] - 0s 479ms/step - loss: 1.7764 - acc: 0.5699 - val_loss: 1.3191 - val_acc: 0.8690\n",
            "Epoch 596/1000\n",
            "1/1 [==============================] - 0s 488ms/step - loss: 1.7774 - acc: 0.5697 - val_loss: 1.3185 - val_acc: 0.8690\n",
            "Epoch 597/1000\n",
            "1/1 [==============================] - 0s 494ms/step - loss: 1.7789 - acc: 0.5697 - val_loss: 1.3180 - val_acc: 0.8690\n",
            "Epoch 598/1000\n",
            "1/1 [==============================] - 0s 487ms/step - loss: 1.7765 - acc: 0.5699 - val_loss: 1.3175 - val_acc: 0.8690\n",
            "Epoch 599/1000\n",
            "1/1 [==============================] - 0s 480ms/step - loss: 1.7784 - acc: 0.5693 - val_loss: 1.3169 - val_acc: 0.8690\n",
            "Epoch 600/1000\n",
            "1/1 [==============================] - 0s 457ms/step - loss: 1.7751 - acc: 0.5699 - val_loss: 1.3164 - val_acc: 0.8690\n",
            "Epoch 601/1000\n",
            "1/1 [==============================] - 0s 482ms/step - loss: 1.7758 - acc: 0.5699 - val_loss: 1.3159 - val_acc: 0.8690\n",
            "Epoch 602/1000\n",
            "1/1 [==============================] - 0s 466ms/step - loss: 1.7751 - acc: 0.5699 - val_loss: 1.3153 - val_acc: 0.8690\n",
            "Epoch 603/1000\n",
            "1/1 [==============================] - 0s 489ms/step - loss: 1.7766 - acc: 0.5697 - val_loss: 1.3148 - val_acc: 0.8690\n",
            "Epoch 604/1000\n",
            "1/1 [==============================] - 0s 474ms/step - loss: 1.7733 - acc: 0.5702 - val_loss: 1.3143 - val_acc: 0.8690\n",
            "Epoch 605/1000\n",
            "1/1 [==============================] - 0s 483ms/step - loss: 1.7747 - acc: 0.5701 - val_loss: 1.3137 - val_acc: 0.8690\n",
            "Epoch 606/1000\n",
            "1/1 [==============================] - 0s 466ms/step - loss: 1.7727 - acc: 0.5702 - val_loss: 1.3132 - val_acc: 0.8690\n",
            "Epoch 607/1000\n",
            "1/1 [==============================] - 0s 488ms/step - loss: 1.7729 - acc: 0.5701 - val_loss: 1.3127 - val_acc: 0.8690\n",
            "Epoch 608/1000\n",
            "1/1 [==============================] - 0s 472ms/step - loss: 1.7729 - acc: 0.5699 - val_loss: 1.3122 - val_acc: 0.8690\n",
            "Epoch 609/1000\n",
            "1/1 [==============================] - 0s 481ms/step - loss: 1.7726 - acc: 0.5701 - val_loss: 1.3116 - val_acc: 0.8690\n",
            "Epoch 610/1000\n",
            "1/1 [==============================] - 0s 492ms/step - loss: 1.7731 - acc: 0.5701 - val_loss: 1.3111 - val_acc: 0.8690\n",
            "Epoch 611/1000\n",
            "1/1 [==============================] - 0s 481ms/step - loss: 1.7785 - acc: 0.5699 - val_loss: 1.3106 - val_acc: 0.8690\n",
            "Epoch 612/1000\n",
            "1/1 [==============================] - 0s 451ms/step - loss: 1.7765 - acc: 0.5701 - val_loss: 1.3101 - val_acc: 0.8690\n",
            "Epoch 613/1000\n",
            "1/1 [==============================] - 0s 487ms/step - loss: 1.7715 - acc: 0.5697 - val_loss: 1.3095 - val_acc: 0.8690\n",
            "Epoch 614/1000\n",
            "1/1 [==============================] - 0s 484ms/step - loss: 1.7708 - acc: 0.5701 - val_loss: 1.3090 - val_acc: 0.8690\n",
            "Epoch 615/1000\n",
            "1/1 [==============================] - 0s 482ms/step - loss: 1.7771 - acc: 0.5699 - val_loss: 1.3085 - val_acc: 0.8690\n",
            "Epoch 616/1000\n",
            "1/1 [==============================] - 0s 483ms/step - loss: 1.7707 - acc: 0.5701 - val_loss: 1.3080 - val_acc: 0.8690\n",
            "Epoch 617/1000\n",
            "1/1 [==============================] - 0s 466ms/step - loss: 1.7698 - acc: 0.5701 - val_loss: 1.3074 - val_acc: 0.8690\n",
            "Epoch 618/1000\n",
            "1/1 [==============================] - 0s 497ms/step - loss: 1.7789 - acc: 0.5701 - val_loss: 1.3069 - val_acc: 0.8690\n",
            "Epoch 619/1000\n",
            "1/1 [==============================] - 1s 507ms/step - loss: 1.7724 - acc: 0.5699 - val_loss: 1.3064 - val_acc: 0.8690\n",
            "Epoch 620/1000\n",
            "1/1 [==============================] - 0s 500ms/step - loss: 1.7749 - acc: 0.5701 - val_loss: 1.3059 - val_acc: 0.8690\n",
            "Epoch 621/1000\n",
            "1/1 [==============================] - 0s 487ms/step - loss: 1.7700 - acc: 0.5701 - val_loss: 1.3054 - val_acc: 0.8690\n",
            "Epoch 622/1000\n",
            "1/1 [==============================] - 1s 504ms/step - loss: 1.7721 - acc: 0.5697 - val_loss: 1.3048 - val_acc: 0.8690\n",
            "Epoch 623/1000\n",
            "1/1 [==============================] - 0s 484ms/step - loss: 1.7681 - acc: 0.5701 - val_loss: 1.3043 - val_acc: 0.8690\n",
            "Epoch 624/1000\n",
            "1/1 [==============================] - 0s 480ms/step - loss: 1.7678 - acc: 0.5701 - val_loss: 1.3038 - val_acc: 0.8690\n",
            "Epoch 625/1000\n",
            "1/1 [==============================] - 0s 476ms/step - loss: 1.7674 - acc: 0.5701 - val_loss: 1.3033 - val_acc: 0.8690\n",
            "Epoch 626/1000\n",
            "1/1 [==============================] - 1s 504ms/step - loss: 1.7666 - acc: 0.5701 - val_loss: 1.3028 - val_acc: 0.8690\n",
            "Epoch 627/1000\n",
            "1/1 [==============================] - 0s 484ms/step - loss: 1.7700 - acc: 0.5697 - val_loss: 1.3023 - val_acc: 0.8690\n",
            "Epoch 628/1000\n",
            "1/1 [==============================] - 0s 488ms/step - loss: 1.7688 - acc: 0.5701 - val_loss: 1.3017 - val_acc: 0.8690\n",
            "Epoch 629/1000\n",
            "1/1 [==============================] - 0s 478ms/step - loss: 1.7687 - acc: 0.5701 - val_loss: 1.3012 - val_acc: 0.8690\n",
            "Epoch 630/1000\n",
            "1/1 [==============================] - 0s 487ms/step - loss: 1.7684 - acc: 0.5697 - val_loss: 1.3007 - val_acc: 0.8690\n",
            "Epoch 631/1000\n",
            "1/1 [==============================] - 0s 482ms/step - loss: 1.7658 - acc: 0.5701 - val_loss: 1.3002 - val_acc: 0.8690\n",
            "Epoch 632/1000\n",
            "1/1 [==============================] - 0s 494ms/step - loss: 1.7670 - acc: 0.5699 - val_loss: 1.2997 - val_acc: 0.8690\n",
            "Epoch 633/1000\n",
            "1/1 [==============================] - 0s 493ms/step - loss: 1.7653 - acc: 0.5701 - val_loss: 1.2992 - val_acc: 0.8690\n",
            "Epoch 634/1000\n",
            "1/1 [==============================] - 0s 485ms/step - loss: 1.7649 - acc: 0.5701 - val_loss: 1.2987 - val_acc: 0.8690\n",
            "Epoch 635/1000\n",
            "1/1 [==============================] - 0s 500ms/step - loss: 1.7791 - acc: 0.5695 - val_loss: 1.2981 - val_acc: 0.8690\n",
            "Epoch 636/1000\n",
            "1/1 [==============================] - 0s 486ms/step - loss: 1.7684 - acc: 0.5699 - val_loss: 1.2976 - val_acc: 0.8690\n",
            "Epoch 637/1000\n",
            "1/1 [==============================] - 0s 479ms/step - loss: 1.7641 - acc: 0.5701 - val_loss: 1.2971 - val_acc: 0.8690\n",
            "Epoch 638/1000\n",
            "1/1 [==============================] - 0s 478ms/step - loss: 1.7639 - acc: 0.5701 - val_loss: 1.2966 - val_acc: 0.8690\n",
            "Epoch 639/1000\n",
            "1/1 [==============================] - 0s 448ms/step - loss: 1.7635 - acc: 0.5701 - val_loss: 1.2961 - val_acc: 0.8690\n",
            "Epoch 640/1000\n",
            "1/1 [==============================] - 0s 483ms/step - loss: 1.7664 - acc: 0.5701 - val_loss: 1.2956 - val_acc: 0.8690\n",
            "Epoch 641/1000\n",
            "1/1 [==============================] - 0s 477ms/step - loss: 1.7636 - acc: 0.5699 - val_loss: 1.2951 - val_acc: 0.8690\n",
            "Epoch 642/1000\n",
            "1/1 [==============================] - 0s 467ms/step - loss: 1.7630 - acc: 0.5699 - val_loss: 1.2946 - val_acc: 0.8690\n",
            "Epoch 643/1000\n",
            "1/1 [==============================] - 0s 474ms/step - loss: 1.7736 - acc: 0.5697 - val_loss: 1.2941 - val_acc: 0.8690\n",
            "Epoch 644/1000\n",
            "1/1 [==============================] - 0s 465ms/step - loss: 1.7649 - acc: 0.5701 - val_loss: 1.2936 - val_acc: 0.8690\n",
            "Epoch 645/1000\n",
            "1/1 [==============================] - 0s 482ms/step - loss: 1.7695 - acc: 0.5699 - val_loss: 1.2930 - val_acc: 0.8690\n",
            "Epoch 646/1000\n",
            "1/1 [==============================] - 0s 472ms/step - loss: 1.7659 - acc: 0.5701 - val_loss: 1.2925 - val_acc: 0.8690\n",
            "Epoch 647/1000\n",
            "1/1 [==============================] - 0s 485ms/step - loss: 1.7624 - acc: 0.5699 - val_loss: 1.2920 - val_acc: 0.8690\n",
            "Epoch 648/1000\n",
            "1/1 [==============================] - 0s 476ms/step - loss: 1.7620 - acc: 0.5699 - val_loss: 1.2915 - val_acc: 0.8690\n",
            "Epoch 649/1000\n",
            "1/1 [==============================] - 0s 489ms/step - loss: 1.7605 - acc: 0.5701 - val_loss: 1.2910 - val_acc: 0.8690\n",
            "Epoch 650/1000\n",
            "1/1 [==============================] - 0s 473ms/step - loss: 1.7626 - acc: 0.5701 - val_loss: 1.2905 - val_acc: 0.8690\n",
            "Epoch 651/1000\n",
            "1/1 [==============================] - 0s 483ms/step - loss: 1.7717 - acc: 0.5701 - val_loss: 1.2900 - val_acc: 0.8690\n",
            "Epoch 652/1000\n",
            "1/1 [==============================] - 0s 487ms/step - loss: 1.7738 - acc: 0.5699 - val_loss: 1.2895 - val_acc: 0.8690\n",
            "Epoch 653/1000\n",
            "1/1 [==============================] - 0s 483ms/step - loss: 1.7626 - acc: 0.5699 - val_loss: 1.2890 - val_acc: 0.8690\n",
            "Epoch 654/1000\n",
            "1/1 [==============================] - 0s 452ms/step - loss: 1.7591 - acc: 0.5701 - val_loss: 1.2885 - val_acc: 0.8690\n",
            "Epoch 655/1000\n",
            "1/1 [==============================] - 0s 468ms/step - loss: 1.7639 - acc: 0.5699 - val_loss: 1.2880 - val_acc: 0.8690\n",
            "Epoch 656/1000\n",
            "1/1 [==============================] - 0s 476ms/step - loss: 1.7648 - acc: 0.5697 - val_loss: 1.2875 - val_acc: 0.8690\n",
            "Epoch 657/1000\n",
            "1/1 [==============================] - 0s 498ms/step - loss: 1.7682 - acc: 0.5697 - val_loss: 1.2870 - val_acc: 0.8690\n",
            "Epoch 658/1000\n",
            "1/1 [==============================] - 0s 489ms/step - loss: 1.7622 - acc: 0.5699 - val_loss: 1.2865 - val_acc: 0.8690\n",
            "Epoch 659/1000\n",
            "1/1 [==============================] - 0s 465ms/step - loss: 1.7594 - acc: 0.5699 - val_loss: 1.2860 - val_acc: 0.8690\n",
            "Epoch 660/1000\n",
            "1/1 [==============================] - 1s 512ms/step - loss: 1.7590 - acc: 0.5701 - val_loss: 1.2855 - val_acc: 0.8690\n",
            "Epoch 661/1000\n",
            "1/1 [==============================] - 0s 481ms/step - loss: 1.7571 - acc: 0.5701 - val_loss: 1.2850 - val_acc: 0.8690\n",
            "Epoch 662/1000\n",
            "1/1 [==============================] - 0s 491ms/step - loss: 1.7602 - acc: 0.5699 - val_loss: 1.2845 - val_acc: 0.8690\n",
            "Epoch 663/1000\n",
            "1/1 [==============================] - 1s 844ms/step - loss: 1.7566 - acc: 0.5701 - val_loss: 1.2840 - val_acc: 0.8690\n",
            "Epoch 664/1000\n",
            "1/1 [==============================] - 1s 553ms/step - loss: 1.7568 - acc: 0.5702 - val_loss: 1.2835 - val_acc: 0.8690\n",
            "Epoch 665/1000\n",
            "1/1 [==============================] - 1s 561ms/step - loss: 1.7562 - acc: 0.5701 - val_loss: 1.2830 - val_acc: 0.8690\n",
            "Epoch 666/1000\n",
            "1/1 [==============================] - 1s 543ms/step - loss: 1.7565 - acc: 0.5699 - val_loss: 1.2825 - val_acc: 0.8690\n",
            "Epoch 667/1000\n",
            "1/1 [==============================] - 1s 568ms/step - loss: 1.7590 - acc: 0.5701 - val_loss: 1.2820 - val_acc: 0.8690\n",
            "Epoch 668/1000\n",
            "1/1 [==============================] - 1s 648ms/step - loss: 1.7555 - acc: 0.5701 - val_loss: 1.2815 - val_acc: 0.8690\n",
            "Epoch 669/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.7552 - acc: 0.5701 - val_loss: 1.2810 - val_acc: 0.8690\n",
            "Epoch 670/1000\n",
            "1/1 [==============================] - 1s 957ms/step - loss: 1.7585 - acc: 0.5699 - val_loss: 1.2805 - val_acc: 0.8690\n",
            "Epoch 671/1000\n",
            "1/1 [==============================] - 1s 652ms/step - loss: 1.7547 - acc: 0.5701 - val_loss: 1.2800 - val_acc: 0.8690\n",
            "Epoch 672/1000\n",
            "1/1 [==============================] - 0s 494ms/step - loss: 1.7695 - acc: 0.5701 - val_loss: 1.2796 - val_acc: 0.8690\n",
            "Epoch 673/1000\n",
            "1/1 [==============================] - 0s 469ms/step - loss: 1.7583 - acc: 0.5697 - val_loss: 1.2791 - val_acc: 0.8690\n",
            "Epoch 674/1000\n",
            "1/1 [==============================] - 1s 506ms/step - loss: 1.7532 - acc: 0.5701 - val_loss: 1.2786 - val_acc: 0.8690\n",
            "Epoch 675/1000\n",
            "1/1 [==============================] - 0s 476ms/step - loss: 1.7541 - acc: 0.5699 - val_loss: 1.2781 - val_acc: 0.8690\n",
            "Epoch 676/1000\n",
            "1/1 [==============================] - 0s 478ms/step - loss: 1.7531 - acc: 0.5701 - val_loss: 1.2776 - val_acc: 0.8690\n",
            "Epoch 677/1000\n",
            "1/1 [==============================] - 0s 470ms/step - loss: 1.7537 - acc: 0.5701 - val_loss: 1.2771 - val_acc: 0.8690\n",
            "Epoch 678/1000\n",
            "1/1 [==============================] - 1s 506ms/step - loss: 1.7521 - acc: 0.5701 - val_loss: 1.2766 - val_acc: 0.8690\n",
            "Epoch 679/1000\n",
            "1/1 [==============================] - 0s 469ms/step - loss: 1.7590 - acc: 0.5695 - val_loss: 1.2761 - val_acc: 0.8690\n",
            "Epoch 680/1000\n",
            "1/1 [==============================] - 0s 479ms/step - loss: 1.7535 - acc: 0.5699 - val_loss: 1.2756 - val_acc: 0.8690\n",
            "Epoch 681/1000\n",
            "1/1 [==============================] - 0s 465ms/step - loss: 1.7571 - acc: 0.5699 - val_loss: 1.2751 - val_acc: 0.8690\n",
            "Epoch 682/1000\n",
            "1/1 [==============================] - 0s 458ms/step - loss: 1.7530 - acc: 0.5699 - val_loss: 1.2747 - val_acc: 0.8690\n",
            "Epoch 683/1000\n",
            "1/1 [==============================] - 0s 478ms/step - loss: 1.7520 - acc: 0.5699 - val_loss: 1.2742 - val_acc: 0.8690\n",
            "Epoch 684/1000\n",
            "1/1 [==============================] - 0s 472ms/step - loss: 1.7542 - acc: 0.5699 - val_loss: 1.2737 - val_acc: 0.8690\n",
            "Epoch 685/1000\n",
            "1/1 [==============================] - 0s 481ms/step - loss: 1.7516 - acc: 0.5702 - val_loss: 1.2732 - val_acc: 0.8690\n",
            "Epoch 686/1000\n",
            "1/1 [==============================] - 0s 483ms/step - loss: 1.7500 - acc: 0.5701 - val_loss: 1.2727 - val_acc: 0.8690\n",
            "Epoch 687/1000\n",
            "1/1 [==============================] - 1s 500ms/step - loss: 1.7505 - acc: 0.5699 - val_loss: 1.2722 - val_acc: 0.8690\n",
            "Epoch 688/1000\n",
            "1/1 [==============================] - 0s 465ms/step - loss: 1.7524 - acc: 0.5701 - val_loss: 1.2717 - val_acc: 0.8690\n",
            "Epoch 689/1000\n",
            "1/1 [==============================] - 0s 497ms/step - loss: 1.7494 - acc: 0.5701 - val_loss: 1.2713 - val_acc: 0.8690\n",
            "Epoch 690/1000\n",
            "1/1 [==============================] - 1s 525ms/step - loss: 1.7511 - acc: 0.5699 - val_loss: 1.2708 - val_acc: 0.8690\n",
            "Epoch 691/1000\n",
            "1/1 [==============================] - 0s 493ms/step - loss: 1.7559 - acc: 0.5697 - val_loss: 1.2703 - val_acc: 0.8690\n",
            "Epoch 692/1000\n",
            "1/1 [==============================] - 0s 499ms/step - loss: 1.7498 - acc: 0.5701 - val_loss: 1.2698 - val_acc: 0.8690\n",
            "Epoch 693/1000\n",
            "1/1 [==============================] - 0s 481ms/step - loss: 1.7479 - acc: 0.5701 - val_loss: 1.2693 - val_acc: 0.8690\n",
            "Epoch 694/1000\n",
            "1/1 [==============================] - 0s 468ms/step - loss: 1.7509 - acc: 0.5699 - val_loss: 1.2689 - val_acc: 0.8690\n",
            "Epoch 695/1000\n",
            "1/1 [==============================] - 0s 474ms/step - loss: 1.7519 - acc: 0.5699 - val_loss: 1.2684 - val_acc: 0.8690\n",
            "Epoch 696/1000\n",
            "1/1 [==============================] - 1s 500ms/step - loss: 1.7575 - acc: 0.5701 - val_loss: 1.2679 - val_acc: 0.8690\n",
            "Epoch 697/1000\n",
            "1/1 [==============================] - 0s 482ms/step - loss: 1.7480 - acc: 0.5699 - val_loss: 1.2674 - val_acc: 0.8690\n",
            "Epoch 698/1000\n",
            "1/1 [==============================] - 0s 466ms/step - loss: 1.7469 - acc: 0.5701 - val_loss: 1.2669 - val_acc: 0.8690\n",
            "Epoch 699/1000\n",
            "1/1 [==============================] - 0s 484ms/step - loss: 1.7514 - acc: 0.5699 - val_loss: 1.2665 - val_acc: 0.8690\n",
            "Epoch 700/1000\n",
            "1/1 [==============================] - 0s 493ms/step - loss: 1.7476 - acc: 0.5699 - val_loss: 1.2660 - val_acc: 0.8690\n",
            "Epoch 701/1000\n",
            "1/1 [==============================] - 0s 472ms/step - loss: 1.7471 - acc: 0.5701 - val_loss: 1.2655 - val_acc: 0.8690\n",
            "Epoch 702/1000\n",
            "1/1 [==============================] - 0s 483ms/step - loss: 1.7497 - acc: 0.5701 - val_loss: 1.2650 - val_acc: 0.8690\n",
            "Epoch 703/1000\n",
            "1/1 [==============================] - 1s 502ms/step - loss: 1.7477 - acc: 0.5699 - val_loss: 1.2646 - val_acc: 0.8690\n",
            "Epoch 704/1000\n",
            "1/1 [==============================] - 0s 454ms/step - loss: 1.7457 - acc: 0.5699 - val_loss: 1.2641 - val_acc: 0.8690\n",
            "Epoch 705/1000\n",
            "1/1 [==============================] - 0s 492ms/step - loss: 1.7459 - acc: 0.5701 - val_loss: 1.2636 - val_acc: 0.8690\n",
            "Epoch 706/1000\n",
            "1/1 [==============================] - 0s 482ms/step - loss: 1.7463 - acc: 0.5701 - val_loss: 1.2631 - val_acc: 0.8690\n",
            "Epoch 707/1000\n",
            "1/1 [==============================] - 0s 479ms/step - loss: 1.7463 - acc: 0.5701 - val_loss: 1.2627 - val_acc: 0.8690\n",
            "Epoch 708/1000\n",
            "1/1 [==============================] - 0s 469ms/step - loss: 1.7445 - acc: 0.5701 - val_loss: 1.2622 - val_acc: 0.8690\n",
            "Epoch 709/1000\n",
            "1/1 [==============================] - 0s 489ms/step - loss: 1.7444 - acc: 0.5701 - val_loss: 1.2617 - val_acc: 0.8690\n",
            "Epoch 710/1000\n",
            "1/1 [==============================] - 0s 496ms/step - loss: 1.7444 - acc: 0.5699 - val_loss: 1.2612 - val_acc: 0.8690\n",
            "Epoch 711/1000\n",
            "1/1 [==============================] - 0s 462ms/step - loss: 1.7436 - acc: 0.5701 - val_loss: 1.2608 - val_acc: 0.8690\n",
            "Epoch 712/1000\n",
            "1/1 [==============================] - 0s 481ms/step - loss: 1.7479 - acc: 0.5697 - val_loss: 1.2603 - val_acc: 0.8690\n",
            "Epoch 713/1000\n",
            "1/1 [==============================] - 0s 472ms/step - loss: 1.7431 - acc: 0.5701 - val_loss: 1.2598 - val_acc: 0.8690\n",
            "Epoch 714/1000\n",
            "1/1 [==============================] - 0s 488ms/step - loss: 1.7443 - acc: 0.5699 - val_loss: 1.2593 - val_acc: 0.8690\n",
            "Epoch 715/1000\n",
            "1/1 [==============================] - 0s 469ms/step - loss: 1.7432 - acc: 0.5699 - val_loss: 1.2589 - val_acc: 0.8690\n",
            "Epoch 716/1000\n",
            "1/1 [==============================] - 0s 480ms/step - loss: 1.7427 - acc: 0.5701 - val_loss: 1.2584 - val_acc: 0.8690\n",
            "Epoch 717/1000\n",
            "1/1 [==============================] - 0s 484ms/step - loss: 1.7519 - acc: 0.5699 - val_loss: 1.2579 - val_acc: 0.8690\n",
            "Epoch 718/1000\n",
            "1/1 [==============================] - 1s 515ms/step - loss: 1.7438 - acc: 0.5701 - val_loss: 1.2575 - val_acc: 0.8690\n",
            "Epoch 719/1000\n",
            "1/1 [==============================] - 0s 479ms/step - loss: 1.7429 - acc: 0.5697 - val_loss: 1.2570 - val_acc: 0.8690\n",
            "Epoch 720/1000\n",
            "1/1 [==============================] - 0s 487ms/step - loss: 1.7413 - acc: 0.5701 - val_loss: 1.2565 - val_acc: 0.8690\n",
            "Epoch 721/1000\n",
            "1/1 [==============================] - 0s 483ms/step - loss: 1.7414 - acc: 0.5701 - val_loss: 1.2561 - val_acc: 0.8690\n",
            "Epoch 722/1000\n",
            "1/1 [==============================] - 0s 485ms/step - loss: 1.7413 - acc: 0.5701 - val_loss: 1.2556 - val_acc: 0.8690\n",
            "Epoch 723/1000\n",
            "1/1 [==============================] - 1s 504ms/step - loss: 1.7442 - acc: 0.5699 - val_loss: 1.2551 - val_acc: 0.8690\n",
            "Epoch 724/1000\n",
            "1/1 [==============================] - 0s 468ms/step - loss: 1.7434 - acc: 0.5699 - val_loss: 1.2547 - val_acc: 0.8690\n",
            "Epoch 725/1000\n",
            "1/1 [==============================] - 0s 487ms/step - loss: 1.7406 - acc: 0.5701 - val_loss: 1.2542 - val_acc: 0.8690\n",
            "Epoch 726/1000\n",
            "1/1 [==============================] - 0s 471ms/step - loss: 1.7401 - acc: 0.5701 - val_loss: 1.2537 - val_acc: 0.8690\n",
            "Epoch 727/1000\n",
            "1/1 [==============================] - 0s 498ms/step - loss: 1.7431 - acc: 0.5701 - val_loss: 1.2533 - val_acc: 0.8690\n",
            "Epoch 728/1000\n",
            "1/1 [==============================] - 0s 488ms/step - loss: 1.7451 - acc: 0.5693 - val_loss: 1.2528 - val_acc: 0.8690\n",
            "Epoch 729/1000\n",
            "1/1 [==============================] - 0s 496ms/step - loss: 1.7408 - acc: 0.5701 - val_loss: 1.2524 - val_acc: 0.8690\n",
            "Epoch 730/1000\n",
            "1/1 [==============================] - 0s 474ms/step - loss: 1.7396 - acc: 0.5699 - val_loss: 1.2519 - val_acc: 0.8690\n",
            "Epoch 731/1000\n",
            "1/1 [==============================] - 0s 491ms/step - loss: 1.7384 - acc: 0.5701 - val_loss: 1.2514 - val_acc: 0.8690\n",
            "Epoch 732/1000\n",
            "1/1 [==============================] - 0s 463ms/step - loss: 1.7385 - acc: 0.5699 - val_loss: 1.2510 - val_acc: 0.8690\n",
            "Epoch 733/1000\n",
            "1/1 [==============================] - 0s 498ms/step - loss: 1.7381 - acc: 0.5699 - val_loss: 1.2505 - val_acc: 0.8690\n",
            "Epoch 734/1000\n",
            "1/1 [==============================] - 1s 505ms/step - loss: 1.7378 - acc: 0.5701 - val_loss: 1.2501 - val_acc: 0.8690\n",
            "Epoch 735/1000\n",
            "1/1 [==============================] - 0s 467ms/step - loss: 1.7377 - acc: 0.5699 - val_loss: 1.2496 - val_acc: 0.8690\n",
            "Epoch 736/1000\n",
            "1/1 [==============================] - 0s 488ms/step - loss: 1.7431 - acc: 0.5699 - val_loss: 1.2491 - val_acc: 0.8690\n",
            "Epoch 737/1000\n",
            "1/1 [==============================] - 1s 502ms/step - loss: 1.7409 - acc: 0.5699 - val_loss: 1.2487 - val_acc: 0.8690\n",
            "Epoch 738/1000\n",
            "1/1 [==============================] - 0s 472ms/step - loss: 1.7381 - acc: 0.5699 - val_loss: 1.2482 - val_acc: 0.8690\n",
            "Epoch 739/1000\n",
            "1/1 [==============================] - 1s 504ms/step - loss: 1.7367 - acc: 0.5701 - val_loss: 1.2478 - val_acc: 0.8690\n",
            "Epoch 740/1000\n",
            "1/1 [==============================] - 0s 486ms/step - loss: 1.7360 - acc: 0.5701 - val_loss: 1.2473 - val_acc: 0.8690\n",
            "Epoch 741/1000\n",
            "1/1 [==============================] - 0s 484ms/step - loss: 1.7361 - acc: 0.5701 - val_loss: 1.2468 - val_acc: 0.8690\n",
            "Epoch 742/1000\n",
            "1/1 [==============================] - 0s 495ms/step - loss: 1.7372 - acc: 0.5699 - val_loss: 1.2464 - val_acc: 0.8690\n",
            "Epoch 743/1000\n",
            "1/1 [==============================] - 1s 509ms/step - loss: 1.7375 - acc: 0.5701 - val_loss: 1.2459 - val_acc: 0.8690\n",
            "Epoch 744/1000\n",
            "1/1 [==============================] - 1s 503ms/step - loss: 1.7354 - acc: 0.5701 - val_loss: 1.2455 - val_acc: 0.8690\n",
            "Epoch 745/1000\n",
            "1/1 [==============================] - 0s 484ms/step - loss: 1.7349 - acc: 0.5701 - val_loss: 1.2450 - val_acc: 0.8690\n",
            "Epoch 746/1000\n",
            "1/1 [==============================] - 0s 477ms/step - loss: 1.7349 - acc: 0.5701 - val_loss: 1.2446 - val_acc: 0.8690\n",
            "Epoch 747/1000\n",
            "1/1 [==============================] - 0s 496ms/step - loss: 1.7404 - acc: 0.5699 - val_loss: 1.2441 - val_acc: 0.8690\n",
            "Epoch 748/1000\n",
            "1/1 [==============================] - 0s 474ms/step - loss: 1.7367 - acc: 0.5699 - val_loss: 1.2437 - val_acc: 0.8690\n",
            "Epoch 749/1000\n",
            "1/1 [==============================] - 0s 494ms/step - loss: 1.7347 - acc: 0.5701 - val_loss: 1.2432 - val_acc: 0.8690\n",
            "Epoch 750/1000\n",
            "1/1 [==============================] - 0s 464ms/step - loss: 1.7361 - acc: 0.5697 - val_loss: 1.2428 - val_acc: 0.8690\n",
            "Epoch 751/1000\n",
            "1/1 [==============================] - 0s 486ms/step - loss: 1.7350 - acc: 0.5697 - val_loss: 1.2423 - val_acc: 0.8690\n",
            "Epoch 752/1000\n",
            "1/1 [==============================] - 0s 476ms/step - loss: 1.7334 - acc: 0.5701 - val_loss: 1.2419 - val_acc: 0.8690\n",
            "Epoch 753/1000\n",
            "1/1 [==============================] - 0s 479ms/step - loss: 1.7332 - acc: 0.5699 - val_loss: 1.2414 - val_acc: 0.8690\n",
            "Epoch 754/1000\n",
            "1/1 [==============================] - 0s 478ms/step - loss: 1.7428 - acc: 0.5695 - val_loss: 1.2410 - val_acc: 0.8690\n",
            "Epoch 755/1000\n",
            "1/1 [==============================] - 0s 480ms/step - loss: 1.7346 - acc: 0.5699 - val_loss: 1.2405 - val_acc: 0.8690\n",
            "Epoch 756/1000\n",
            "1/1 [==============================] - 0s 480ms/step - loss: 1.7350 - acc: 0.5697 - val_loss: 1.2401 - val_acc: 0.8690\n",
            "Epoch 757/1000\n",
            "1/1 [==============================] - 0s 487ms/step - loss: 1.7333 - acc: 0.5695 - val_loss: 1.2396 - val_acc: 0.8690\n",
            "Epoch 758/1000\n",
            "1/1 [==============================] - 0s 470ms/step - loss: 1.7335 - acc: 0.5699 - val_loss: 1.2392 - val_acc: 0.8690\n",
            "Epoch 759/1000\n",
            "1/1 [==============================] - 0s 474ms/step - loss: 1.7344 - acc: 0.5699 - val_loss: 1.2387 - val_acc: 0.8690\n",
            "Epoch 760/1000\n",
            "1/1 [==============================] - 0s 482ms/step - loss: 1.7343 - acc: 0.5697 - val_loss: 1.2383 - val_acc: 0.8690\n",
            "Epoch 761/1000\n",
            "1/1 [==============================] - 0s 472ms/step - loss: 1.7316 - acc: 0.5701 - val_loss: 1.2378 - val_acc: 0.8690\n",
            "Epoch 762/1000\n",
            "1/1 [==============================] - 1s 526ms/step - loss: 1.7391 - acc: 0.5701 - val_loss: 1.2374 - val_acc: 0.8690\n",
            "Epoch 763/1000\n",
            "1/1 [==============================] - 0s 473ms/step - loss: 1.7328 - acc: 0.5697 - val_loss: 1.2369 - val_acc: 0.8690\n",
            "Epoch 764/1000\n",
            "1/1 [==============================] - 1s 505ms/step - loss: 1.7321 - acc: 0.5701 - val_loss: 1.2365 - val_acc: 0.8690\n",
            "Epoch 765/1000\n",
            "1/1 [==============================] - 1s 508ms/step - loss: 1.7326 - acc: 0.5701 - val_loss: 1.2360 - val_acc: 0.8690\n",
            "Epoch 766/1000\n",
            "1/1 [==============================] - 0s 477ms/step - loss: 1.7305 - acc: 0.5701 - val_loss: 1.2356 - val_acc: 0.8690\n",
            "Epoch 767/1000\n",
            "1/1 [==============================] - 0s 470ms/step - loss: 1.7315 - acc: 0.5701 - val_loss: 1.2352 - val_acc: 0.8690\n",
            "Epoch 768/1000\n",
            "1/1 [==============================] - 0s 484ms/step - loss: 1.7307 - acc: 0.5701 - val_loss: 1.2347 - val_acc: 0.8690\n",
            "Epoch 769/1000\n",
            "1/1 [==============================] - 0s 478ms/step - loss: 1.7300 - acc: 0.5697 - val_loss: 1.2343 - val_acc: 0.8690\n",
            "Epoch 770/1000\n",
            "1/1 [==============================] - 0s 482ms/step - loss: 1.7547 - acc: 0.5699 - val_loss: 1.2338 - val_acc: 0.8690\n",
            "Epoch 771/1000\n",
            "1/1 [==============================] - 0s 498ms/step - loss: 1.7298 - acc: 0.5697 - val_loss: 1.2334 - val_acc: 0.8690\n",
            "Epoch 772/1000\n",
            "1/1 [==============================] - 1s 504ms/step - loss: 1.7293 - acc: 0.5701 - val_loss: 1.2329 - val_acc: 0.8690\n",
            "Epoch 773/1000\n",
            "1/1 [==============================] - 0s 483ms/step - loss: 1.7299 - acc: 0.5699 - val_loss: 1.2325 - val_acc: 0.8690\n",
            "Epoch 774/1000\n",
            "1/1 [==============================] - 0s 499ms/step - loss: 1.7281 - acc: 0.5701 - val_loss: 1.2321 - val_acc: 0.8690\n",
            "Epoch 775/1000\n",
            "1/1 [==============================] - 0s 488ms/step - loss: 1.7298 - acc: 0.5697 - val_loss: 1.2316 - val_acc: 0.8690\n",
            "Epoch 776/1000\n",
            "1/1 [==============================] - 0s 471ms/step - loss: 1.7277 - acc: 0.5701 - val_loss: 1.2312 - val_acc: 0.8690\n",
            "Epoch 777/1000\n",
            "1/1 [==============================] - 0s 475ms/step - loss: 1.7358 - acc: 0.5699 - val_loss: 1.2307 - val_acc: 0.8690\n",
            "Epoch 778/1000\n",
            "1/1 [==============================] - 1s 513ms/step - loss: 1.7283 - acc: 0.5697 - val_loss: 1.2303 - val_acc: 0.8690\n",
            "Epoch 779/1000\n",
            "1/1 [==============================] - 0s 468ms/step - loss: 1.7310 - acc: 0.5697 - val_loss: 1.2299 - val_acc: 0.8690\n",
            "Epoch 780/1000\n",
            "1/1 [==============================] - 0s 495ms/step - loss: 1.7267 - acc: 0.5701 - val_loss: 1.2294 - val_acc: 0.8690\n",
            "Epoch 781/1000\n",
            "1/1 [==============================] - 0s 475ms/step - loss: 1.7265 - acc: 0.5701 - val_loss: 1.2290 - val_acc: 0.8690\n",
            "Epoch 782/1000\n",
            "1/1 [==============================] - 0s 488ms/step - loss: 1.7281 - acc: 0.5699 - val_loss: 1.2286 - val_acc: 0.8690\n",
            "Epoch 783/1000\n",
            "1/1 [==============================] - 0s 486ms/step - loss: 1.7270 - acc: 0.5699 - val_loss: 1.2281 - val_acc: 0.8690\n",
            "Epoch 784/1000\n",
            "1/1 [==============================] - 0s 468ms/step - loss: 1.7267 - acc: 0.5699 - val_loss: 1.2277 - val_acc: 0.8690\n",
            "Epoch 785/1000\n",
            "1/1 [==============================] - 0s 494ms/step - loss: 1.7251 - acc: 0.5702 - val_loss: 1.2273 - val_acc: 0.8690\n",
            "Epoch 786/1000\n",
            "1/1 [==============================] - 0s 465ms/step - loss: 1.7264 - acc: 0.5701 - val_loss: 1.2268 - val_acc: 0.8690\n",
            "Epoch 787/1000\n",
            "1/1 [==============================] - 0s 463ms/step - loss: 1.7269 - acc: 0.5699 - val_loss: 1.2264 - val_acc: 0.8690\n",
            "Epoch 788/1000\n",
            "1/1 [==============================] - 0s 480ms/step - loss: 1.7248 - acc: 0.5701 - val_loss: 1.2260 - val_acc: 0.8690\n",
            "Epoch 789/1000\n",
            "1/1 [==============================] - 0s 486ms/step - loss: 1.7239 - acc: 0.5702 - val_loss: 1.2255 - val_acc: 0.8690\n",
            "Epoch 790/1000\n",
            "1/1 [==============================] - 0s 474ms/step - loss: 1.7245 - acc: 0.5701 - val_loss: 1.2251 - val_acc: 0.8690\n",
            "Epoch 791/1000\n",
            "1/1 [==============================] - 0s 476ms/step - loss: 1.7240 - acc: 0.5701 - val_loss: 1.2247 - val_acc: 0.8690\n",
            "Epoch 792/1000\n",
            "1/1 [==============================] - 0s 489ms/step - loss: 1.7239 - acc: 0.5701 - val_loss: 1.2242 - val_acc: 0.8690\n",
            "Epoch 793/1000\n",
            "1/1 [==============================] - 0s 490ms/step - loss: 1.7311 - acc: 0.5699 - val_loss: 1.2238 - val_acc: 0.8690\n",
            "Epoch 794/1000\n",
            "1/1 [==============================] - 0s 493ms/step - loss: 1.7245 - acc: 0.5697 - val_loss: 1.2234 - val_acc: 0.8690\n",
            "Epoch 795/1000\n",
            "1/1 [==============================] - 0s 493ms/step - loss: 1.7256 - acc: 0.5693 - val_loss: 1.2229 - val_acc: 0.8690\n",
            "Epoch 796/1000\n",
            "1/1 [==============================] - 0s 485ms/step - loss: 1.7242 - acc: 0.5699 - val_loss: 1.2225 - val_acc: 0.8690\n",
            "Epoch 797/1000\n",
            "1/1 [==============================] - 0s 483ms/step - loss: 1.7234 - acc: 0.5699 - val_loss: 1.2221 - val_acc: 0.8690\n",
            "Epoch 798/1000\n",
            "1/1 [==============================] - 0s 478ms/step - loss: 1.7236 - acc: 0.5701 - val_loss: 1.2217 - val_acc: 0.8690\n",
            "Epoch 799/1000\n",
            "1/1 [==============================] - 1s 508ms/step - loss: 1.7228 - acc: 0.5697 - val_loss: 1.2212 - val_acc: 0.8690\n",
            "Epoch 800/1000\n",
            "1/1 [==============================] - 0s 462ms/step - loss: 1.7221 - acc: 0.5701 - val_loss: 1.2208 - val_acc: 0.8690\n",
            "Epoch 801/1000\n",
            "1/1 [==============================] - 0s 484ms/step - loss: 1.7244 - acc: 0.5699 - val_loss: 1.2204 - val_acc: 0.8690\n",
            "Epoch 802/1000\n",
            "1/1 [==============================] - 0s 482ms/step - loss: 1.7219 - acc: 0.5701 - val_loss: 1.2200 - val_acc: 0.8690\n",
            "Epoch 803/1000\n",
            "1/1 [==============================] - 0s 475ms/step - loss: 1.7231 - acc: 0.5699 - val_loss: 1.2195 - val_acc: 0.8690\n",
            "Epoch 804/1000\n",
            "1/1 [==============================] - 0s 468ms/step - loss: 1.7214 - acc: 0.5701 - val_loss: 1.2191 - val_acc: 0.8690\n",
            "Epoch 805/1000\n",
            "1/1 [==============================] - 1s 505ms/step - loss: 1.7244 - acc: 0.5701 - val_loss: 1.2187 - val_acc: 0.8690\n",
            "Epoch 806/1000\n",
            "1/1 [==============================] - 0s 494ms/step - loss: 1.7207 - acc: 0.5701 - val_loss: 1.2183 - val_acc: 0.8690\n",
            "Epoch 807/1000\n",
            "1/1 [==============================] - 0s 500ms/step - loss: 1.7205 - acc: 0.5701 - val_loss: 1.2178 - val_acc: 0.8690\n",
            "Epoch 808/1000\n",
            "1/1 [==============================] - 1s 502ms/step - loss: 1.7208 - acc: 0.5701 - val_loss: 1.2174 - val_acc: 0.8690\n",
            "Epoch 809/1000\n",
            "1/1 [==============================] - 0s 475ms/step - loss: 1.7207 - acc: 0.5699 - val_loss: 1.2170 - val_acc: 0.8690\n",
            "Epoch 810/1000\n",
            "1/1 [==============================] - 0s 487ms/step - loss: 1.7215 - acc: 0.5702 - val_loss: 1.2166 - val_acc: 0.8690\n",
            "Epoch 811/1000\n",
            "1/1 [==============================] - 0s 463ms/step - loss: 1.7209 - acc: 0.5701 - val_loss: 1.2161 - val_acc: 0.8690\n",
            "Epoch 812/1000\n",
            "1/1 [==============================] - 0s 485ms/step - loss: 1.7213 - acc: 0.5697 - val_loss: 1.2157 - val_acc: 0.8690\n",
            "Epoch 813/1000\n",
            "1/1 [==============================] - 0s 494ms/step - loss: 1.7244 - acc: 0.5701 - val_loss: 1.2153 - val_acc: 0.8690\n",
            "Epoch 814/1000\n",
            "1/1 [==============================] - 0s 477ms/step - loss: 1.7198 - acc: 0.5701 - val_loss: 1.2149 - val_acc: 0.8690\n",
            "Epoch 815/1000\n",
            "1/1 [==============================] - 0s 483ms/step - loss: 1.7232 - acc: 0.5695 - val_loss: 1.2145 - val_acc: 0.8690\n",
            "Epoch 816/1000\n",
            "1/1 [==============================] - 0s 493ms/step - loss: 1.7195 - acc: 0.5699 - val_loss: 1.2140 - val_acc: 0.8690\n",
            "Epoch 817/1000\n",
            "1/1 [==============================] - 0s 479ms/step - loss: 1.7183 - acc: 0.5701 - val_loss: 1.2136 - val_acc: 0.8690\n",
            "Epoch 818/1000\n",
            "1/1 [==============================] - 0s 483ms/step - loss: 1.7215 - acc: 0.5701 - val_loss: 1.2132 - val_acc: 0.8690\n",
            "Epoch 819/1000\n",
            "1/1 [==============================] - 0s 471ms/step - loss: 1.7189 - acc: 0.5701 - val_loss: 1.2128 - val_acc: 0.8690\n",
            "Epoch 820/1000\n",
            "1/1 [==============================] - 0s 464ms/step - loss: 1.7177 - acc: 0.5701 - val_loss: 1.2124 - val_acc: 0.8690\n",
            "Epoch 821/1000\n",
            "1/1 [==============================] - 0s 474ms/step - loss: 1.7173 - acc: 0.5701 - val_loss: 1.2120 - val_acc: 0.8690\n",
            "Epoch 822/1000\n",
            "1/1 [==============================] - 1s 502ms/step - loss: 1.7223 - acc: 0.5693 - val_loss: 1.2115 - val_acc: 0.8690\n",
            "Epoch 823/1000\n",
            "1/1 [==============================] - 0s 473ms/step - loss: 1.7166 - acc: 0.5702 - val_loss: 1.2111 - val_acc: 0.8690\n",
            "Epoch 824/1000\n",
            "1/1 [==============================] - 0s 481ms/step - loss: 1.7241 - acc: 0.5699 - val_loss: 1.2107 - val_acc: 0.8690\n",
            "Epoch 825/1000\n",
            "1/1 [==============================] - 0s 484ms/step - loss: 1.7174 - acc: 0.5699 - val_loss: 1.2103 - val_acc: 0.8690\n",
            "Epoch 826/1000\n",
            "1/1 [==============================] - 0s 482ms/step - loss: 1.7163 - acc: 0.5701 - val_loss: 1.2099 - val_acc: 0.8690\n",
            "Epoch 827/1000\n",
            "1/1 [==============================] - 1s 508ms/step - loss: 1.7167 - acc: 0.5701 - val_loss: 1.2095 - val_acc: 0.8690\n",
            "Epoch 828/1000\n",
            "1/1 [==============================] - 0s 493ms/step - loss: 1.7282 - acc: 0.5693 - val_loss: 1.2091 - val_acc: 0.8690\n",
            "Epoch 829/1000\n",
            "1/1 [==============================] - 0s 499ms/step - loss: 1.7182 - acc: 0.5697 - val_loss: 1.2086 - val_acc: 0.8690\n",
            "Epoch 830/1000\n",
            "1/1 [==============================] - 0s 482ms/step - loss: 1.7162 - acc: 0.5699 - val_loss: 1.2082 - val_acc: 0.8690\n",
            "Epoch 831/1000\n",
            "1/1 [==============================] - 0s 480ms/step - loss: 1.7152 - acc: 0.5701 - val_loss: 1.2078 - val_acc: 0.8690\n",
            "Epoch 832/1000\n",
            "1/1 [==============================] - 1s 502ms/step - loss: 1.7180 - acc: 0.5701 - val_loss: 1.2074 - val_acc: 0.8690\n",
            "Epoch 833/1000\n",
            "1/1 [==============================] - 0s 481ms/step - loss: 1.7159 - acc: 0.5699 - val_loss: 1.2070 - val_acc: 0.8690\n",
            "Epoch 834/1000\n",
            "1/1 [==============================] - 0s 471ms/step - loss: 1.7175 - acc: 0.5699 - val_loss: 1.2066 - val_acc: 0.8690\n",
            "Epoch 835/1000\n",
            "1/1 [==============================] - 1s 513ms/step - loss: 1.7147 - acc: 0.5701 - val_loss: 1.2062 - val_acc: 0.8690\n",
            "Epoch 836/1000\n",
            "1/1 [==============================] - 0s 491ms/step - loss: 1.7198 - acc: 0.5699 - val_loss: 1.2058 - val_acc: 0.8690\n",
            "Epoch 837/1000\n",
            "1/1 [==============================] - 0s 498ms/step - loss: 1.7138 - acc: 0.5701 - val_loss: 1.2054 - val_acc: 0.8690\n",
            "Epoch 838/1000\n",
            "1/1 [==============================] - 1s 518ms/step - loss: 1.7137 - acc: 0.5699 - val_loss: 1.2049 - val_acc: 0.8690\n",
            "Epoch 839/1000\n",
            "1/1 [==============================] - 0s 485ms/step - loss: 1.7139 - acc: 0.5699 - val_loss: 1.2045 - val_acc: 0.8690\n",
            "Epoch 840/1000\n",
            "1/1 [==============================] - 0s 473ms/step - loss: 1.7141 - acc: 0.5697 - val_loss: 1.2041 - val_acc: 0.8690\n",
            "Epoch 841/1000\n",
            "1/1 [==============================] - 0s 484ms/step - loss: 1.7128 - acc: 0.5701 - val_loss: 1.2037 - val_acc: 0.8690\n",
            "Epoch 842/1000\n",
            "1/1 [==============================] - 0s 474ms/step - loss: 1.7140 - acc: 0.5701 - val_loss: 1.2033 - val_acc: 0.8690\n",
            "Epoch 843/1000\n",
            "1/1 [==============================] - 1s 503ms/step - loss: 1.7126 - acc: 0.5701 - val_loss: 1.2029 - val_acc: 0.8690\n",
            "Epoch 844/1000\n",
            "1/1 [==============================] - 0s 494ms/step - loss: 1.7117 - acc: 0.5702 - val_loss: 1.2025 - val_acc: 0.8690\n",
            "Epoch 845/1000\n",
            "1/1 [==============================] - 1s 519ms/step - loss: 1.7132 - acc: 0.5695 - val_loss: 1.2021 - val_acc: 0.8690\n",
            "Epoch 846/1000\n",
            "1/1 [==============================] - 0s 472ms/step - loss: 1.7120 - acc: 0.5701 - val_loss: 1.2017 - val_acc: 0.8690\n",
            "Epoch 847/1000\n",
            "1/1 [==============================] - 0s 494ms/step - loss: 1.7133 - acc: 0.5699 - val_loss: 1.2013 - val_acc: 0.8690\n",
            "Epoch 848/1000\n",
            "1/1 [==============================] - 0s 464ms/step - loss: 1.7142 - acc: 0.5701 - val_loss: 1.2009 - val_acc: 0.8690\n",
            "Epoch 849/1000\n",
            "1/1 [==============================] - 0s 486ms/step - loss: 1.7111 - acc: 0.5701 - val_loss: 1.2005 - val_acc: 0.8690\n",
            "Epoch 850/1000\n",
            "1/1 [==============================] - 0s 492ms/step - loss: 1.7194 - acc: 0.5701 - val_loss: 1.2001 - val_acc: 0.8690\n",
            "Epoch 851/1000\n",
            "1/1 [==============================] - 0s 478ms/step - loss: 1.7108 - acc: 0.5701 - val_loss: 1.1997 - val_acc: 0.8690\n",
            "Epoch 852/1000\n",
            "1/1 [==============================] - 0s 464ms/step - loss: 1.7111 - acc: 0.5701 - val_loss: 1.1993 - val_acc: 0.8690\n",
            "Epoch 853/1000\n",
            "1/1 [==============================] - 0s 499ms/step - loss: 1.7140 - acc: 0.5701 - val_loss: 1.1989 - val_acc: 0.8690\n",
            "Epoch 854/1000\n",
            "1/1 [==============================] - 0s 485ms/step - loss: 1.7122 - acc: 0.5699 - val_loss: 1.1985 - val_acc: 0.8690\n",
            "Epoch 855/1000\n",
            "1/1 [==============================] - 1s 501ms/step - loss: 1.7112 - acc: 0.5701 - val_loss: 1.1981 - val_acc: 0.8690\n",
            "Epoch 856/1000\n",
            "1/1 [==============================] - 0s 477ms/step - loss: 1.7135 - acc: 0.5699 - val_loss: 1.1977 - val_acc: 0.8690\n",
            "Epoch 857/1000\n",
            "1/1 [==============================] - 0s 476ms/step - loss: 1.7145 - acc: 0.5701 - val_loss: 1.1973 - val_acc: 0.8690\n",
            "Epoch 858/1000\n",
            "1/1 [==============================] - 0s 499ms/step - loss: 1.7103 - acc: 0.5699 - val_loss: 1.1969 - val_acc: 0.8690\n",
            "Epoch 859/1000\n",
            "1/1 [==============================] - 0s 479ms/step - loss: 1.7092 - acc: 0.5701 - val_loss: 1.1965 - val_acc: 0.8690\n",
            "Epoch 860/1000\n",
            "1/1 [==============================] - 0s 457ms/step - loss: 1.7087 - acc: 0.5699 - val_loss: 1.1961 - val_acc: 0.8690\n",
            "Epoch 861/1000\n",
            "1/1 [==============================] - 0s 467ms/step - loss: 1.7085 - acc: 0.5701 - val_loss: 1.1957 - val_acc: 0.8690\n",
            "Epoch 862/1000\n",
            "1/1 [==============================] - 0s 488ms/step - loss: 1.7101 - acc: 0.5699 - val_loss: 1.1953 - val_acc: 0.8690\n",
            "Epoch 863/1000\n",
            "1/1 [==============================] - 0s 488ms/step - loss: 1.7097 - acc: 0.5699 - val_loss: 1.1949 - val_acc: 0.8690\n",
            "Epoch 864/1000\n",
            "1/1 [==============================] - 0s 490ms/step - loss: 1.7108 - acc: 0.5701 - val_loss: 1.1945 - val_acc: 0.8690\n",
            "Epoch 865/1000\n",
            "1/1 [==============================] - 1s 802ms/step - loss: 1.7088 - acc: 0.5701 - val_loss: 1.1941 - val_acc: 0.8690\n",
            "Epoch 866/1000\n",
            "1/1 [==============================] - 0s 495ms/step - loss: 1.7079 - acc: 0.5699 - val_loss: 1.1937 - val_acc: 0.8690\n",
            "Epoch 867/1000\n",
            "1/1 [==============================] - 0s 489ms/step - loss: 1.7079 - acc: 0.5701 - val_loss: 1.1933 - val_acc: 0.8690\n",
            "Epoch 868/1000\n",
            "1/1 [==============================] - 0s 459ms/step - loss: 1.7088 - acc: 0.5699 - val_loss: 1.1929 - val_acc: 0.8690\n",
            "Epoch 869/1000\n",
            "1/1 [==============================] - 0s 496ms/step - loss: 1.7074 - acc: 0.5701 - val_loss: 1.1925 - val_acc: 0.8690\n",
            "Epoch 870/1000\n",
            "1/1 [==============================] - 0s 480ms/step - loss: 1.7069 - acc: 0.5701 - val_loss: 1.1921 - val_acc: 0.8690\n",
            "Epoch 871/1000\n",
            "1/1 [==============================] - 0s 489ms/step - loss: 1.7070 - acc: 0.5699 - val_loss: 1.1918 - val_acc: 0.8690\n",
            "Epoch 872/1000\n",
            "1/1 [==============================] - 0s 477ms/step - loss: 1.7077 - acc: 0.5701 - val_loss: 1.1914 - val_acc: 0.8690\n",
            "Epoch 873/1000\n",
            "1/1 [==============================] - 0s 481ms/step - loss: 1.7062 - acc: 0.5701 - val_loss: 1.1910 - val_acc: 0.8690\n",
            "Epoch 874/1000\n",
            "1/1 [==============================] - 0s 475ms/step - loss: 1.7065 - acc: 0.5699 - val_loss: 1.1906 - val_acc: 0.8690\n",
            "Epoch 875/1000\n",
            "1/1 [==============================] - 0s 480ms/step - loss: 1.7059 - acc: 0.5701 - val_loss: 1.1902 - val_acc: 0.8690\n",
            "Epoch 876/1000\n",
            "1/1 [==============================] - 0s 493ms/step - loss: 1.7058 - acc: 0.5699 - val_loss: 1.1898 - val_acc: 0.8690\n",
            "Epoch 877/1000\n",
            "1/1 [==============================] - 0s 479ms/step - loss: 1.7056 - acc: 0.5701 - val_loss: 1.1894 - val_acc: 0.8690\n",
            "Epoch 878/1000\n",
            "1/1 [==============================] - 0s 480ms/step - loss: 1.7097 - acc: 0.5699 - val_loss: 1.1890 - val_acc: 0.8690\n",
            "Epoch 879/1000\n",
            "1/1 [==============================] - 0s 486ms/step - loss: 1.7064 - acc: 0.5699 - val_loss: 1.1886 - val_acc: 0.8690\n",
            "Epoch 880/1000\n",
            "1/1 [==============================] - 1s 509ms/step - loss: 1.7096 - acc: 0.5701 - val_loss: 1.1882 - val_acc: 0.8690\n",
            "Epoch 881/1000\n",
            "1/1 [==============================] - 0s 482ms/step - loss: 1.7119 - acc: 0.5699 - val_loss: 1.1879 - val_acc: 0.8690\n",
            "Epoch 882/1000\n",
            "1/1 [==============================] - 0s 476ms/step - loss: 1.7045 - acc: 0.5701 - val_loss: 1.1875 - val_acc: 0.8690\n",
            "Epoch 883/1000\n",
            "1/1 [==============================] - 0s 471ms/step - loss: 1.7152 - acc: 0.5699 - val_loss: 1.1871 - val_acc: 0.8690\n",
            "Epoch 884/1000\n",
            "1/1 [==============================] - 0s 480ms/step - loss: 1.7049 - acc: 0.5699 - val_loss: 1.1867 - val_acc: 0.8690\n",
            "Epoch 885/1000\n",
            "1/1 [==============================] - 0s 487ms/step - loss: 1.7058 - acc: 0.5699 - val_loss: 1.1863 - val_acc: 0.8690\n",
            "Epoch 886/1000\n",
            "1/1 [==============================] - 0s 481ms/step - loss: 1.7036 - acc: 0.5701 - val_loss: 1.1859 - val_acc: 0.8690\n",
            "Epoch 887/1000\n",
            "1/1 [==============================] - 0s 488ms/step - loss: 1.7043 - acc: 0.5701 - val_loss: 1.1855 - val_acc: 0.8690\n",
            "Epoch 888/1000\n",
            "1/1 [==============================] - 0s 497ms/step - loss: 1.7045 - acc: 0.5701 - val_loss: 1.1852 - val_acc: 0.8690\n",
            "Epoch 889/1000\n",
            "1/1 [==============================] - 0s 486ms/step - loss: 1.7031 - acc: 0.5701 - val_loss: 1.1848 - val_acc: 0.8690\n",
            "Epoch 890/1000\n",
            "1/1 [==============================] - 0s 477ms/step - loss: 1.7029 - acc: 0.5701 - val_loss: 1.1844 - val_acc: 0.8690\n",
            "Epoch 891/1000\n",
            "1/1 [==============================] - 0s 494ms/step - loss: 1.7026 - acc: 0.5701 - val_loss: 1.1840 - val_acc: 0.8690\n",
            "Epoch 892/1000\n",
            "1/1 [==============================] - 0s 479ms/step - loss: 1.7058 - acc: 0.5699 - val_loss: 1.1836 - val_acc: 0.8690\n",
            "Epoch 893/1000\n",
            "1/1 [==============================] - 0s 484ms/step - loss: 1.7049 - acc: 0.5699 - val_loss: 1.1832 - val_acc: 0.8690\n",
            "Epoch 894/1000\n",
            "1/1 [==============================] - 0s 484ms/step - loss: 1.7049 - acc: 0.5697 - val_loss: 1.1829 - val_acc: 0.8690\n",
            "Epoch 895/1000\n",
            "1/1 [==============================] - 0s 474ms/step - loss: 1.7018 - acc: 0.5701 - val_loss: 1.1825 - val_acc: 0.8690\n",
            "Epoch 896/1000\n",
            "1/1 [==============================] - 0s 489ms/step - loss: 1.7014 - acc: 0.5701 - val_loss: 1.1821 - val_acc: 0.8690\n",
            "Epoch 897/1000\n",
            "1/1 [==============================] - 0s 489ms/step - loss: 1.7023 - acc: 0.5701 - val_loss: 1.1817 - val_acc: 0.8690\n",
            "Epoch 898/1000\n",
            "1/1 [==============================] - 0s 491ms/step - loss: 1.7013 - acc: 0.5701 - val_loss: 1.1813 - val_acc: 0.8690\n",
            "Epoch 899/1000\n",
            "1/1 [==============================] - 0s 464ms/step - loss: 1.7011 - acc: 0.5701 - val_loss: 1.1810 - val_acc: 0.8690\n",
            "Epoch 900/1000\n",
            "1/1 [==============================] - 0s 462ms/step - loss: 1.7033 - acc: 0.5699 - val_loss: 1.1806 - val_acc: 0.8690\n",
            "Epoch 901/1000\n",
            "1/1 [==============================] - 1s 501ms/step - loss: 1.7007 - acc: 0.5701 - val_loss: 1.1802 - val_acc: 0.8690\n",
            "Epoch 902/1000\n",
            "1/1 [==============================] - 0s 479ms/step - loss: 1.7004 - acc: 0.5701 - val_loss: 1.1798 - val_acc: 0.8690\n",
            "Epoch 903/1000\n",
            "1/1 [==============================] - 0s 493ms/step - loss: 1.7010 - acc: 0.5701 - val_loss: 1.1794 - val_acc: 0.8690\n",
            "Epoch 904/1000\n",
            "1/1 [==============================] - 0s 466ms/step - loss: 1.7011 - acc: 0.5701 - val_loss: 1.1791 - val_acc: 0.8690\n",
            "Epoch 905/1000\n",
            "1/1 [==============================] - 0s 474ms/step - loss: 1.7052 - acc: 0.5699 - val_loss: 1.1787 - val_acc: 0.8690\n",
            "Epoch 906/1000\n",
            "1/1 [==============================] - 0s 486ms/step - loss: 1.7003 - acc: 0.5701 - val_loss: 1.1783 - val_acc: 0.8690\n",
            "Epoch 907/1000\n",
            "1/1 [==============================] - 0s 475ms/step - loss: 1.6995 - acc: 0.5701 - val_loss: 1.1779 - val_acc: 0.8690\n",
            "Epoch 908/1000\n",
            "1/1 [==============================] - 0s 486ms/step - loss: 1.7023 - acc: 0.5699 - val_loss: 1.1776 - val_acc: 0.8690\n",
            "Epoch 909/1000\n",
            "1/1 [==============================] - 1s 510ms/step - loss: 1.7001 - acc: 0.5699 - val_loss: 1.1772 - val_acc: 0.8690\n",
            "Epoch 910/1000\n",
            "1/1 [==============================] - 0s 472ms/step - loss: 1.7015 - acc: 0.5699 - val_loss: 1.1768 - val_acc: 0.8690\n",
            "Epoch 911/1000\n",
            "1/1 [==============================] - 1s 504ms/step - loss: 1.6987 - acc: 0.5701 - val_loss: 1.1764 - val_acc: 0.8690\n",
            "Epoch 912/1000\n",
            "1/1 [==============================] - 1s 501ms/step - loss: 1.6987 - acc: 0.5699 - val_loss: 1.1761 - val_acc: 0.8690\n",
            "Epoch 913/1000\n",
            "1/1 [==============================] - 0s 497ms/step - loss: 1.6984 - acc: 0.5701 - val_loss: 1.1757 - val_acc: 0.8690\n",
            "Epoch 914/1000\n",
            "1/1 [==============================] - 0s 475ms/step - loss: 1.6983 - acc: 0.5701 - val_loss: 1.1753 - val_acc: 0.8690\n",
            "Epoch 915/1000\n",
            "1/1 [==============================] - 0s 500ms/step - loss: 1.6977 - acc: 0.5701 - val_loss: 1.1750 - val_acc: 0.8690\n",
            "Epoch 916/1000\n",
            "1/1 [==============================] - 0s 487ms/step - loss: 1.6979 - acc: 0.5701 - val_loss: 1.1746 - val_acc: 0.8690\n",
            "Epoch 917/1000\n",
            "1/1 [==============================] - 0s 475ms/step - loss: 1.6992 - acc: 0.5693 - val_loss: 1.1742 - val_acc: 0.8690\n",
            "Epoch 918/1000\n",
            "1/1 [==============================] - 0s 484ms/step - loss: 1.6989 - acc: 0.5701 - val_loss: 1.1738 - val_acc: 0.8690\n",
            "Epoch 919/1000\n",
            "1/1 [==============================] - 1s 500ms/step - loss: 1.6988 - acc: 0.5699 - val_loss: 1.1735 - val_acc: 0.8690\n",
            "Epoch 920/1000\n",
            "1/1 [==============================] - 1s 505ms/step - loss: 1.6993 - acc: 0.5699 - val_loss: 1.1731 - val_acc: 0.8690\n",
            "Epoch 921/1000\n",
            "1/1 [==============================] - 0s 493ms/step - loss: 1.6990 - acc: 0.5701 - val_loss: 1.1727 - val_acc: 0.8690\n",
            "Epoch 922/1000\n",
            "1/1 [==============================] - 0s 486ms/step - loss: 1.7019 - acc: 0.5701 - val_loss: 1.1724 - val_acc: 0.8690\n",
            "Epoch 923/1000\n",
            "1/1 [==============================] - 0s 500ms/step - loss: 1.6975 - acc: 0.5699 - val_loss: 1.1720 - val_acc: 0.8690\n",
            "Epoch 924/1000\n",
            "1/1 [==============================] - 1s 500ms/step - loss: 1.6989 - acc: 0.5699 - val_loss: 1.1716 - val_acc: 0.8690\n",
            "Epoch 925/1000\n",
            "1/1 [==============================] - 0s 467ms/step - loss: 1.7013 - acc: 0.5701 - val_loss: 1.1713 - val_acc: 0.8690\n",
            "Epoch 926/1000\n",
            "1/1 [==============================] - 0s 487ms/step - loss: 1.6958 - acc: 0.5701 - val_loss: 1.1709 - val_acc: 0.8690\n",
            "Epoch 927/1000\n",
            "1/1 [==============================] - 0s 488ms/step - loss: 1.6958 - acc: 0.5701 - val_loss: 1.1705 - val_acc: 0.8690\n",
            "Epoch 928/1000\n",
            "1/1 [==============================] - 0s 487ms/step - loss: 1.6960 - acc: 0.5699 - val_loss: 1.1702 - val_acc: 0.8690\n",
            "Epoch 929/1000\n",
            "1/1 [==============================] - 0s 470ms/step - loss: 1.6954 - acc: 0.5701 - val_loss: 1.1698 - val_acc: 0.8690\n",
            "Epoch 930/1000\n",
            "1/1 [==============================] - 0s 495ms/step - loss: 1.6953 - acc: 0.5701 - val_loss: 1.1694 - val_acc: 0.8690\n",
            "Epoch 931/1000\n",
            "1/1 [==============================] - 0s 477ms/step - loss: 1.6976 - acc: 0.5699 - val_loss: 1.1691 - val_acc: 0.8690\n",
            "Epoch 932/1000\n",
            "1/1 [==============================] - 0s 491ms/step - loss: 1.6949 - acc: 0.5699 - val_loss: 1.1687 - val_acc: 0.8690\n",
            "Epoch 933/1000\n",
            "1/1 [==============================] - 0s 480ms/step - loss: 1.6942 - acc: 0.5702 - val_loss: 1.1683 - val_acc: 0.8690\n",
            "Epoch 934/1000\n",
            "1/1 [==============================] - 1s 515ms/step - loss: 1.6944 - acc: 0.5701 - val_loss: 1.1680 - val_acc: 0.8690\n",
            "Epoch 935/1000\n",
            "1/1 [==============================] - 0s 492ms/step - loss: 1.6974 - acc: 0.5701 - val_loss: 1.1676 - val_acc: 0.8690\n",
            "Epoch 936/1000\n",
            "1/1 [==============================] - 0s 496ms/step - loss: 1.6958 - acc: 0.5701 - val_loss: 1.1673 - val_acc: 0.8690\n",
            "Epoch 937/1000\n",
            "1/1 [==============================] - 0s 477ms/step - loss: 1.6947 - acc: 0.5701 - val_loss: 1.1669 - val_acc: 0.8690\n",
            "Epoch 938/1000\n",
            "1/1 [==============================] - 1s 503ms/step - loss: 1.6936 - acc: 0.5701 - val_loss: 1.1665 - val_acc: 0.8690\n",
            "Epoch 939/1000\n",
            "1/1 [==============================] - 0s 479ms/step - loss: 1.6934 - acc: 0.5701 - val_loss: 1.1662 - val_acc: 0.8690\n",
            "Epoch 940/1000\n",
            "1/1 [==============================] - 0s 496ms/step - loss: 1.6933 - acc: 0.5701 - val_loss: 1.1658 - val_acc: 0.8690\n",
            "Epoch 941/1000\n",
            "1/1 [==============================] - 0s 481ms/step - loss: 1.6931 - acc: 0.5699 - val_loss: 1.1654 - val_acc: 0.8690\n",
            "Epoch 942/1000\n",
            "1/1 [==============================] - 1s 517ms/step - loss: 1.6936 - acc: 0.5701 - val_loss: 1.1651 - val_acc: 0.8690\n",
            "Epoch 943/1000\n",
            "1/1 [==============================] - 0s 458ms/step - loss: 1.7109 - acc: 0.5699 - val_loss: 1.1647 - val_acc: 0.8690\n",
            "Epoch 944/1000\n",
            "1/1 [==============================] - 0s 484ms/step - loss: 1.6933 - acc: 0.5701 - val_loss: 1.1644 - val_acc: 0.8690\n",
            "Epoch 945/1000\n",
            "1/1 [==============================] - 0s 486ms/step - loss: 1.6923 - acc: 0.5701 - val_loss: 1.1640 - val_acc: 0.8690\n",
            "Epoch 946/1000\n",
            "1/1 [==============================] - 0s 484ms/step - loss: 1.6921 - acc: 0.5701 - val_loss: 1.1637 - val_acc: 0.8690\n",
            "Epoch 947/1000\n",
            "1/1 [==============================] - 0s 487ms/step - loss: 1.6921 - acc: 0.5699 - val_loss: 1.1633 - val_acc: 0.8690\n",
            "Epoch 948/1000\n",
            "1/1 [==============================] - 1s 512ms/step - loss: 1.6917 - acc: 0.5701 - val_loss: 1.1629 - val_acc: 0.8690\n",
            "Epoch 949/1000\n",
            "1/1 [==============================] - 0s 496ms/step - loss: 1.6975 - acc: 0.5699 - val_loss: 1.1626 - val_acc: 0.8690\n",
            "Epoch 950/1000\n",
            "1/1 [==============================] - 0s 494ms/step - loss: 1.6915 - acc: 0.5701 - val_loss: 1.1622 - val_acc: 0.8690\n",
            "Epoch 951/1000\n",
            "1/1 [==============================] - 0s 493ms/step - loss: 1.6912 - acc: 0.5701 - val_loss: 1.1619 - val_acc: 0.8690\n",
            "Epoch 952/1000\n",
            "1/1 [==============================] - 1s 516ms/step - loss: 1.6911 - acc: 0.5699 - val_loss: 1.1615 - val_acc: 0.8690\n",
            "Epoch 953/1000\n",
            "1/1 [==============================] - 1s 502ms/step - loss: 1.6915 - acc: 0.5701 - val_loss: 1.1612 - val_acc: 0.8690\n",
            "Epoch 954/1000\n",
            "1/1 [==============================] - 0s 492ms/step - loss: 1.6907 - acc: 0.5701 - val_loss: 1.1608 - val_acc: 0.8690\n",
            "Epoch 955/1000\n",
            "1/1 [==============================] - 1s 503ms/step - loss: 1.6906 - acc: 0.5701 - val_loss: 1.1605 - val_acc: 0.8690\n",
            "Epoch 956/1000\n",
            "1/1 [==============================] - 1s 504ms/step - loss: 1.6910 - acc: 0.5701 - val_loss: 1.1601 - val_acc: 0.8690\n",
            "Epoch 957/1000\n",
            "1/1 [==============================] - 1s 529ms/step - loss: 1.6902 - acc: 0.5701 - val_loss: 1.1597 - val_acc: 0.8690\n",
            "Epoch 958/1000\n",
            "1/1 [==============================] - 0s 499ms/step - loss: 1.6896 - acc: 0.5702 - val_loss: 1.1594 - val_acc: 0.8690\n",
            "Epoch 959/1000\n",
            "1/1 [==============================] - 0s 486ms/step - loss: 1.6927 - acc: 0.5699 - val_loss: 1.1590 - val_acc: 0.8690\n",
            "Epoch 960/1000\n",
            "1/1 [==============================] - 1s 501ms/step - loss: 1.6898 - acc: 0.5699 - val_loss: 1.1587 - val_acc: 0.8690\n",
            "Epoch 961/1000\n",
            "1/1 [==============================] - 0s 482ms/step - loss: 1.6921 - acc: 0.5701 - val_loss: 1.1583 - val_acc: 0.8690\n",
            "Epoch 962/1000\n",
            "1/1 [==============================] - 0s 488ms/step - loss: 1.6903 - acc: 0.5699 - val_loss: 1.1580 - val_acc: 0.8690\n",
            "Epoch 963/1000\n",
            "1/1 [==============================] - 0s 473ms/step - loss: 1.6892 - acc: 0.5701 - val_loss: 1.1576 - val_acc: 0.8690\n",
            "Epoch 964/1000\n",
            "1/1 [==============================] - 0s 497ms/step - loss: 1.6890 - acc: 0.5699 - val_loss: 1.1573 - val_acc: 0.8690\n",
            "Epoch 965/1000\n",
            "1/1 [==============================] - 0s 482ms/step - loss: 1.6924 - acc: 0.5699 - val_loss: 1.1569 - val_acc: 0.8690\n",
            "Epoch 966/1000\n",
            "1/1 [==============================] - 1s 506ms/step - loss: 1.6886 - acc: 0.5701 - val_loss: 1.1566 - val_acc: 0.8690\n",
            "Epoch 967/1000\n",
            "1/1 [==============================] - 0s 484ms/step - loss: 1.6885 - acc: 0.5701 - val_loss: 1.1562 - val_acc: 0.8690\n",
            "Epoch 968/1000\n",
            "1/1 [==============================] - 1s 525ms/step - loss: 1.6883 - acc: 0.5701 - val_loss: 1.1559 - val_acc: 0.8690\n",
            "Epoch 969/1000\n",
            "1/1 [==============================] - 0s 483ms/step - loss: 1.6884 - acc: 0.5701 - val_loss: 1.1555 - val_acc: 0.8690\n",
            "Epoch 970/1000\n",
            "1/1 [==============================] - 0s 487ms/step - loss: 1.6884 - acc: 0.5701 - val_loss: 1.1552 - val_acc: 0.8690\n",
            "Epoch 971/1000\n",
            "1/1 [==============================] - 0s 478ms/step - loss: 1.6902 - acc: 0.5701 - val_loss: 1.1549 - val_acc: 0.8690\n",
            "Epoch 972/1000\n",
            "1/1 [==============================] - 0s 485ms/step - loss: 1.6932 - acc: 0.5702 - val_loss: 1.1545 - val_acc: 0.8690\n",
            "Epoch 973/1000\n",
            "1/1 [==============================] - 0s 467ms/step - loss: 1.6879 - acc: 0.5701 - val_loss: 1.1542 - val_acc: 0.8690\n",
            "Epoch 974/1000\n",
            "1/1 [==============================] - 0s 483ms/step - loss: 1.6878 - acc: 0.5699 - val_loss: 1.1538 - val_acc: 0.8690\n",
            "Epoch 975/1000\n",
            "1/1 [==============================] - 1s 505ms/step - loss: 1.6873 - acc: 0.5701 - val_loss: 1.1535 - val_acc: 0.8690\n",
            "Epoch 976/1000\n",
            "1/1 [==============================] - 0s 476ms/step - loss: 1.6874 - acc: 0.5701 - val_loss: 1.1531 - val_acc: 0.8690\n",
            "Epoch 977/1000\n",
            "1/1 [==============================] - 1s 501ms/step - loss: 1.6867 - acc: 0.5701 - val_loss: 1.1528 - val_acc: 0.8690\n",
            "Epoch 978/1000\n",
            "1/1 [==============================] - 1s 504ms/step - loss: 1.6904 - acc: 0.5699 - val_loss: 1.1524 - val_acc: 0.8690\n",
            "Epoch 979/1000\n",
            "1/1 [==============================] - 0s 483ms/step - loss: 1.6875 - acc: 0.5697 - val_loss: 1.1521 - val_acc: 0.8690\n",
            "Epoch 980/1000\n",
            "1/1 [==============================] - 0s 494ms/step - loss: 1.6860 - acc: 0.5701 - val_loss: 1.1517 - val_acc: 0.8690\n",
            "Epoch 981/1000\n",
            "1/1 [==============================] - 1s 524ms/step - loss: 1.6883 - acc: 0.5699 - val_loss: 1.1514 - val_acc: 0.8690\n",
            "Epoch 982/1000\n",
            "1/1 [==============================] - 1s 521ms/step - loss: 1.6857 - acc: 0.5701 - val_loss: 1.1511 - val_acc: 0.8690\n",
            "Epoch 983/1000\n",
            "1/1 [==============================] - 0s 480ms/step - loss: 1.6856 - acc: 0.5701 - val_loss: 1.1507 - val_acc: 0.8690\n",
            "Epoch 984/1000\n",
            "1/1 [==============================] - 1s 516ms/step - loss: 1.6873 - acc: 0.5701 - val_loss: 1.1504 - val_acc: 0.8690\n",
            "Epoch 985/1000\n",
            "1/1 [==============================] - 0s 492ms/step - loss: 1.6853 - acc: 0.5701 - val_loss: 1.1500 - val_acc: 0.8690\n",
            "Epoch 986/1000\n",
            "1/1 [==============================] - 0s 478ms/step - loss: 1.6865 - acc: 0.5699 - val_loss: 1.1497 - val_acc: 0.8690\n",
            "Epoch 987/1000\n",
            "1/1 [==============================] - 0s 496ms/step - loss: 1.6873 - acc: 0.5697 - val_loss: 1.1494 - val_acc: 0.8690\n",
            "Epoch 988/1000\n",
            "1/1 [==============================] - 0s 494ms/step - loss: 1.6850 - acc: 0.5701 - val_loss: 1.1490 - val_acc: 0.8690\n",
            "Epoch 989/1000\n",
            "1/1 [==============================] - 1s 514ms/step - loss: 1.6887 - acc: 0.5697 - val_loss: 1.1487 - val_acc: 0.8690\n",
            "Epoch 990/1000\n",
            "1/1 [==============================] - 0s 496ms/step - loss: 1.6845 - acc: 0.5701 - val_loss: 1.1483 - val_acc: 0.8690\n",
            "Epoch 991/1000\n",
            "1/1 [==============================] - 1s 504ms/step - loss: 1.6843 - acc: 0.5701 - val_loss: 1.1480 - val_acc: 0.8690\n",
            "Epoch 992/1000\n",
            "1/1 [==============================] - 0s 480ms/step - loss: 1.6841 - acc: 0.5701 - val_loss: 1.1477 - val_acc: 0.8690\n",
            "Epoch 993/1000\n",
            "1/1 [==============================] - 1s 509ms/step - loss: 1.6850 - acc: 0.5701 - val_loss: 1.1473 - val_acc: 0.8690\n",
            "Epoch 994/1000\n",
            "1/1 [==============================] - 0s 495ms/step - loss: 1.6867 - acc: 0.5699 - val_loss: 1.1470 - val_acc: 0.8690\n",
            "Epoch 995/1000\n",
            "1/1 [==============================] - 0s 479ms/step - loss: 1.6835 - acc: 0.5701 - val_loss: 1.1467 - val_acc: 0.8690\n",
            "Epoch 996/1000\n",
            "1/1 [==============================] - 0s 487ms/step - loss: 1.6832 - acc: 0.5701 - val_loss: 1.1463 - val_acc: 0.8690\n",
            "Epoch 997/1000\n",
            "1/1 [==============================] - 1s 536ms/step - loss: 1.6869 - acc: 0.5699 - val_loss: 1.1460 - val_acc: 0.8690\n",
            "Epoch 998/1000\n",
            "1/1 [==============================] - 0s 499ms/step - loss: 1.6837 - acc: 0.5701 - val_loss: 1.1457 - val_acc: 0.8690\n",
            "Epoch 999/1000\n",
            "1/1 [==============================] - 0s 441ms/step - loss: 1.6847 - acc: 0.5697 - val_loss: 1.1453 - val_acc: 0.8690\n",
            "Epoch 1000/1000\n",
            "1/1 [==============================] - 0s 442ms/step - loss: 1.6828 - acc: 0.5701 - val_loss: 1.1450 - val_acc: 0.8690\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWH_YzXzTsjn"
      },
      "source": [
        "# Plotting after training second batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "XTAPOohBTwq3",
        "outputId": "9d6bc843-704f-46ec-81be-35e57f3667e8"
      },
      "source": [
        "import keras\n",
        "from matplotlib import pyplot as plt\n",
        "# history = model1.fit(train_x, train_y,validation_split = 0.1, epochs=50, batch_size=4)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fe160b6d6d0>]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSw0NZEeT6FS"
      },
      "source": [
        "# Retraining Third Batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0clW7g_DdPTE",
        "outputId": "15e363a2-7c5a-499a-811d-fe1ab459dc3f"
      },
      "source": [
        "N = np.shape(A3)[0]\n",
        "history = model.fit([b3_features, A3],\n",
        "          e3,\n",
        "          epochs=1000,\n",
        "          batch_size=N,\n",
        "          validation_data=([val_fea, A4], val_enc),\n",
        "          callbacks=[callback]\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "1/1 [==============================] - 3s 3s/step - loss: 28.4645 - acc: 0.2864 - val_loss: 1.1446 - val_acc: 0.8690\n",
            "Epoch 2/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 12.5364 - acc: 0.3836 - val_loss: 1.1441 - val_acc: 0.8690\n",
            "Epoch 3/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 5.9001 - acc: 0.4711 - val_loss: 1.1437 - val_acc: 0.8690\n",
            "Epoch 4/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 3.5368 - acc: 0.5175 - val_loss: 1.1432 - val_acc: 0.8690\n",
            "Epoch 5/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 2.7236 - acc: 0.5480 - val_loss: 1.1428 - val_acc: 0.8690\n",
            "Epoch 6/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 2.3302 - acc: 0.5615 - val_loss: 1.1423 - val_acc: 0.8690\n",
            "Epoch 7/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 2.0438 - acc: 0.5670 - val_loss: 1.1419 - val_acc: 0.8690\n",
            "Epoch 8/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.8759 - acc: 0.5737 - val_loss: 1.1415 - val_acc: 0.8690\n",
            "Epoch 9/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.8922 - acc: 0.5747 - val_loss: 1.1411 - val_acc: 0.8690\n",
            "Epoch 10/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.8322 - acc: 0.5745 - val_loss: 1.1407 - val_acc: 0.8690\n",
            "Epoch 11/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.9055 - acc: 0.5733 - val_loss: 1.1404 - val_acc: 0.8690\n",
            "Epoch 12/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.8446 - acc: 0.5760 - val_loss: 1.1400 - val_acc: 0.8690\n",
            "Epoch 13/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.7591 - acc: 0.5780 - val_loss: 1.1396 - val_acc: 0.8690\n",
            "Epoch 14/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.7257 - acc: 0.5791 - val_loss: 1.1393 - val_acc: 0.8690\n",
            "Epoch 15/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.7315 - acc: 0.5792 - val_loss: 1.1389 - val_acc: 0.8690\n",
            "Epoch 16/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.7389 - acc: 0.5802 - val_loss: 1.1386 - val_acc: 0.8690\n",
            "Epoch 17/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.7425 - acc: 0.5798 - val_loss: 1.1382 - val_acc: 0.8690\n",
            "Epoch 18/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.7019 - acc: 0.5805 - val_loss: 1.1379 - val_acc: 0.8690\n",
            "Epoch 19/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6803 - acc: 0.5806 - val_loss: 1.1375 - val_acc: 0.8690\n",
            "Epoch 20/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6970 - acc: 0.5813 - val_loss: 1.1372 - val_acc: 0.8690\n",
            "Epoch 21/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6653 - acc: 0.5817 - val_loss: 1.1369 - val_acc: 0.8690\n",
            "Epoch 22/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6630 - acc: 0.5821 - val_loss: 1.1366 - val_acc: 0.8690\n",
            "Epoch 23/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6759 - acc: 0.5819 - val_loss: 1.1362 - val_acc: 0.8690\n",
            "Epoch 24/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6575 - acc: 0.5826 - val_loss: 1.1359 - val_acc: 0.8690\n",
            "Epoch 25/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6557 - acc: 0.5827 - val_loss: 1.1356 - val_acc: 0.8690\n",
            "Epoch 26/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6572 - acc: 0.5823 - val_loss: 1.1353 - val_acc: 0.8690\n",
            "Epoch 27/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6455 - acc: 0.5825 - val_loss: 1.1350 - val_acc: 0.8690\n",
            "Epoch 28/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6525 - acc: 0.5819 - val_loss: 1.1346 - val_acc: 0.8690\n",
            "Epoch 29/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6559 - acc: 0.5824 - val_loss: 1.1343 - val_acc: 0.8690\n",
            "Epoch 30/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6634 - acc: 0.5823 - val_loss: 1.1340 - val_acc: 0.8690\n",
            "Epoch 31/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6521 - acc: 0.5826 - val_loss: 1.1337 - val_acc: 0.8690\n",
            "Epoch 32/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6419 - acc: 0.5823 - val_loss: 1.1334 - val_acc: 0.8690\n",
            "Epoch 33/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6379 - acc: 0.5826 - val_loss: 1.1331 - val_acc: 0.8690\n",
            "Epoch 34/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6443 - acc: 0.5824 - val_loss: 1.1328 - val_acc: 0.8690\n",
            "Epoch 35/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6368 - acc: 0.5827 - val_loss: 1.1325 - val_acc: 0.8690\n",
            "Epoch 36/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6389 - acc: 0.5827 - val_loss: 1.1322 - val_acc: 0.8690\n",
            "Epoch 37/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6418 - acc: 0.5826 - val_loss: 1.1319 - val_acc: 0.8690\n",
            "Epoch 38/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6357 - acc: 0.5825 - val_loss: 1.1316 - val_acc: 0.8690\n",
            "Epoch 39/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6453 - acc: 0.5826 - val_loss: 1.1313 - val_acc: 0.8690\n",
            "Epoch 40/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6416 - acc: 0.5827 - val_loss: 1.1310 - val_acc: 0.8690\n",
            "Epoch 41/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6340 - acc: 0.5828 - val_loss: 1.1307 - val_acc: 0.8690\n",
            "Epoch 42/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6435 - acc: 0.5827 - val_loss: 1.1304 - val_acc: 0.8690\n",
            "Epoch 43/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6370 - acc: 0.5823 - val_loss: 1.1302 - val_acc: 0.8690\n",
            "Epoch 44/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6532 - acc: 0.5827 - val_loss: 1.1299 - val_acc: 0.8690\n",
            "Epoch 45/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6574 - acc: 0.5826 - val_loss: 1.1296 - val_acc: 0.8690\n",
            "Epoch 46/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6324 - acc: 0.5829 - val_loss: 1.1293 - val_acc: 0.8690\n",
            "Epoch 47/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6391 - acc: 0.5827 - val_loss: 1.1290 - val_acc: 0.8690\n",
            "Epoch 48/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6472 - acc: 0.5823 - val_loss: 1.1287 - val_acc: 0.8690\n",
            "Epoch 49/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6350 - acc: 0.5829 - val_loss: 1.1284 - val_acc: 0.8690\n",
            "Epoch 50/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6388 - acc: 0.5826 - val_loss: 1.1281 - val_acc: 0.8690\n",
            "Epoch 51/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6349 - acc: 0.5828 - val_loss: 1.1279 - val_acc: 0.8690\n",
            "Epoch 52/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6283 - acc: 0.5829 - val_loss: 1.1276 - val_acc: 0.8690\n",
            "Epoch 53/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6351 - acc: 0.5824 - val_loss: 1.1273 - val_acc: 0.8690\n",
            "Epoch 54/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6326 - acc: 0.5828 - val_loss: 1.1270 - val_acc: 0.8690\n",
            "Epoch 55/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6305 - acc: 0.5826 - val_loss: 1.1267 - val_acc: 0.8690\n",
            "Epoch 56/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6315 - acc: 0.5827 - val_loss: 1.1265 - val_acc: 0.8690\n",
            "Epoch 57/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6283 - acc: 0.5830 - val_loss: 1.1262 - val_acc: 0.8690\n",
            "Epoch 58/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6271 - acc: 0.5828 - val_loss: 1.1259 - val_acc: 0.8690\n",
            "Epoch 59/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6301 - acc: 0.5827 - val_loss: 1.1256 - val_acc: 0.8690\n",
            "Epoch 60/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6348 - acc: 0.5828 - val_loss: 1.1254 - val_acc: 0.8690\n",
            "Epoch 61/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6423 - acc: 0.5827 - val_loss: 1.1251 - val_acc: 0.8690\n",
            "Epoch 62/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6252 - acc: 0.5830 - val_loss: 1.1248 - val_acc: 0.8690\n",
            "Epoch 63/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6273 - acc: 0.5826 - val_loss: 1.1245 - val_acc: 0.8690\n",
            "Epoch 64/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6295 - acc: 0.5827 - val_loss: 1.1243 - val_acc: 0.8690\n",
            "Epoch 65/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6424 - acc: 0.5829 - val_loss: 1.1240 - val_acc: 0.8690\n",
            "Epoch 66/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6257 - acc: 0.5828 - val_loss: 1.1237 - val_acc: 0.8690\n",
            "Epoch 67/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6253 - acc: 0.5828 - val_loss: 1.1235 - val_acc: 0.8690\n",
            "Epoch 68/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6246 - acc: 0.5828 - val_loss: 1.1232 - val_acc: 0.8690\n",
            "Epoch 69/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6252 - acc: 0.5827 - val_loss: 1.1229 - val_acc: 0.8690\n",
            "Epoch 70/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6284 - acc: 0.5825 - val_loss: 1.1226 - val_acc: 0.8690\n",
            "Epoch 71/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6267 - acc: 0.5827 - val_loss: 1.1224 - val_acc: 0.8690\n",
            "Epoch 72/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6246 - acc: 0.5830 - val_loss: 1.1221 - val_acc: 0.8690\n",
            "Epoch 73/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6202 - acc: 0.5829 - val_loss: 1.1219 - val_acc: 0.8690\n",
            "Epoch 74/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6224 - acc: 0.5827 - val_loss: 1.1216 - val_acc: 0.8690\n",
            "Epoch 75/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6231 - acc: 0.5830 - val_loss: 1.1213 - val_acc: 0.8690\n",
            "Epoch 76/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6233 - acc: 0.5829 - val_loss: 1.1211 - val_acc: 0.8690\n",
            "Epoch 77/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6350 - acc: 0.5830 - val_loss: 1.1208 - val_acc: 0.8690\n",
            "Epoch 78/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6191 - acc: 0.5828 - val_loss: 1.1205 - val_acc: 0.8690\n",
            "Epoch 79/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6402 - acc: 0.5825 - val_loss: 1.1203 - val_acc: 0.8690\n",
            "Epoch 80/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6209 - acc: 0.5828 - val_loss: 1.1200 - val_acc: 0.8690\n",
            "Epoch 81/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6303 - acc: 0.5827 - val_loss: 1.1198 - val_acc: 0.8690\n",
            "Epoch 82/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6196 - acc: 0.5828 - val_loss: 1.1195 - val_acc: 0.8690\n",
            "Epoch 83/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6218 - acc: 0.5828 - val_loss: 1.1192 - val_acc: 0.8690\n",
            "Epoch 84/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6211 - acc: 0.5829 - val_loss: 1.1190 - val_acc: 0.8690\n",
            "Epoch 85/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6165 - acc: 0.5829 - val_loss: 1.1187 - val_acc: 0.8690\n",
            "Epoch 86/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6194 - acc: 0.5824 - val_loss: 1.1185 - val_acc: 0.8690\n",
            "Epoch 87/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6161 - acc: 0.5830 - val_loss: 1.1182 - val_acc: 0.8690\n",
            "Epoch 88/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6258 - acc: 0.5828 - val_loss: 1.1179 - val_acc: 0.8690\n",
            "Epoch 89/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6248 - acc: 0.5828 - val_loss: 1.1177 - val_acc: 0.8690\n",
            "Epoch 90/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6215 - acc: 0.5827 - val_loss: 1.1174 - val_acc: 0.8690\n",
            "Epoch 91/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6239 - acc: 0.5828 - val_loss: 1.1172 - val_acc: 0.8690\n",
            "Epoch 92/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6204 - acc: 0.5828 - val_loss: 1.1169 - val_acc: 0.8690\n",
            "Epoch 93/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6176 - acc: 0.5829 - val_loss: 1.1167 - val_acc: 0.8690\n",
            "Epoch 94/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6193 - acc: 0.5825 - val_loss: 1.1164 - val_acc: 0.8690\n",
            "Epoch 95/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6224 - acc: 0.5826 - val_loss: 1.1162 - val_acc: 0.8690\n",
            "Epoch 96/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6210 - acc: 0.5828 - val_loss: 1.1159 - val_acc: 0.8690\n",
            "Epoch 97/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6128 - acc: 0.5830 - val_loss: 1.1157 - val_acc: 0.8690\n",
            "Epoch 98/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6144 - acc: 0.5828 - val_loss: 1.1154 - val_acc: 0.8690\n",
            "Epoch 99/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6152 - acc: 0.5827 - val_loss: 1.1152 - val_acc: 0.8690\n",
            "Epoch 100/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6166 - acc: 0.5830 - val_loss: 1.1149 - val_acc: 0.8690\n",
            "Epoch 101/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6128 - acc: 0.5829 - val_loss: 1.1147 - val_acc: 0.8690\n",
            "Epoch 102/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6135 - acc: 0.5828 - val_loss: 1.1144 - val_acc: 0.8690\n",
            "Epoch 103/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6117 - acc: 0.5829 - val_loss: 1.1142 - val_acc: 0.8690\n",
            "Epoch 104/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6199 - acc: 0.5829 - val_loss: 1.1139 - val_acc: 0.8690\n",
            "Epoch 105/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6128 - acc: 0.5829 - val_loss: 1.1137 - val_acc: 0.8690\n",
            "Epoch 106/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6166 - acc: 0.5829 - val_loss: 1.1134 - val_acc: 0.8690\n",
            "Epoch 107/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6110 - acc: 0.5829 - val_loss: 1.1132 - val_acc: 0.8690\n",
            "Epoch 108/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6116 - acc: 0.5828 - val_loss: 1.1129 - val_acc: 0.8690\n",
            "Epoch 109/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6153 - acc: 0.5828 - val_loss: 1.1127 - val_acc: 0.8690\n",
            "Epoch 110/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6095 - acc: 0.5830 - val_loss: 1.1125 - val_acc: 0.8690\n",
            "Epoch 111/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6112 - acc: 0.5827 - val_loss: 1.1122 - val_acc: 0.8690\n",
            "Epoch 112/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6087 - acc: 0.5831 - val_loss: 1.1120 - val_acc: 0.8690\n",
            "Epoch 113/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6171 - acc: 0.5827 - val_loss: 1.1117 - val_acc: 0.8690\n",
            "Epoch 114/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6197 - acc: 0.5829 - val_loss: 1.1115 - val_acc: 0.8690\n",
            "Epoch 115/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6092 - acc: 0.5825 - val_loss: 1.1112 - val_acc: 0.8690\n",
            "Epoch 116/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6178 - acc: 0.5828 - val_loss: 1.1110 - val_acc: 0.8690\n",
            "Epoch 117/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6104 - acc: 0.5828 - val_loss: 1.1108 - val_acc: 0.8690\n",
            "Epoch 118/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6106 - acc: 0.5829 - val_loss: 1.1105 - val_acc: 0.8690\n",
            "Epoch 119/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6078 - acc: 0.5831 - val_loss: 1.1103 - val_acc: 0.8690\n",
            "Epoch 120/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6106 - acc: 0.5827 - val_loss: 1.1100 - val_acc: 0.8690\n",
            "Epoch 121/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6121 - acc: 0.5828 - val_loss: 1.1098 - val_acc: 0.8690\n",
            "Epoch 122/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6094 - acc: 0.5829 - val_loss: 1.1096 - val_acc: 0.8690\n",
            "Epoch 123/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6069 - acc: 0.5828 - val_loss: 1.1093 - val_acc: 0.8690\n",
            "Epoch 124/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6078 - acc: 0.5827 - val_loss: 1.1091 - val_acc: 0.8690\n",
            "Epoch 125/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6071 - acc: 0.5829 - val_loss: 1.1088 - val_acc: 0.8690\n",
            "Epoch 126/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6062 - acc: 0.5828 - val_loss: 1.1086 - val_acc: 0.8690\n",
            "Epoch 127/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6112 - acc: 0.5829 - val_loss: 1.1084 - val_acc: 0.8690\n",
            "Epoch 128/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6043 - acc: 0.5830 - val_loss: 1.1081 - val_acc: 0.8690\n",
            "Epoch 129/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6147 - acc: 0.5828 - val_loss: 1.1079 - val_acc: 0.8690\n",
            "Epoch 130/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6156 - acc: 0.5829 - val_loss: 1.1077 - val_acc: 0.8690\n",
            "Epoch 131/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6042 - acc: 0.5828 - val_loss: 1.1074 - val_acc: 0.8690\n",
            "Epoch 132/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6029 - acc: 0.5830 - val_loss: 1.1072 - val_acc: 0.8690\n",
            "Epoch 133/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6058 - acc: 0.5828 - val_loss: 1.1070 - val_acc: 0.8690\n",
            "Epoch 134/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6083 - acc: 0.5826 - val_loss: 1.1067 - val_acc: 0.8690\n",
            "Epoch 135/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6059 - acc: 0.5829 - val_loss: 1.1065 - val_acc: 0.8690\n",
            "Epoch 136/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6055 - acc: 0.5828 - val_loss: 1.1063 - val_acc: 0.8690\n",
            "Epoch 137/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6024 - acc: 0.5829 - val_loss: 1.1060 - val_acc: 0.8690\n",
            "Epoch 138/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6028 - acc: 0.5829 - val_loss: 1.1058 - val_acc: 0.8690\n",
            "Epoch 139/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6025 - acc: 0.5830 - val_loss: 1.1056 - val_acc: 0.8690\n",
            "Epoch 140/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6008 - acc: 0.5830 - val_loss: 1.1053 - val_acc: 0.8690\n",
            "Epoch 141/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6018 - acc: 0.5831 - val_loss: 1.1051 - val_acc: 0.8690\n",
            "Epoch 142/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6007 - acc: 0.5829 - val_loss: 1.1049 - val_acc: 0.8690\n",
            "Epoch 143/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6003 - acc: 0.5828 - val_loss: 1.1047 - val_acc: 0.8690\n",
            "Epoch 144/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6010 - acc: 0.5830 - val_loss: 1.1044 - val_acc: 0.8690\n",
            "Epoch 145/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6004 - acc: 0.5828 - val_loss: 1.1042 - val_acc: 0.8690\n",
            "Epoch 146/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6018 - acc: 0.5826 - val_loss: 1.1040 - val_acc: 0.8690\n",
            "Epoch 147/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6019 - acc: 0.5828 - val_loss: 1.1037 - val_acc: 0.8690\n",
            "Epoch 148/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6031 - acc: 0.5829 - val_loss: 1.1035 - val_acc: 0.8690\n",
            "Epoch 149/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6003 - acc: 0.5829 - val_loss: 1.1033 - val_acc: 0.8690\n",
            "Epoch 150/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6059 - acc: 0.5828 - val_loss: 1.1031 - val_acc: 0.8690\n",
            "Epoch 151/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6041 - acc: 0.5829 - val_loss: 1.1028 - val_acc: 0.8690\n",
            "Epoch 152/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5986 - acc: 0.5830 - val_loss: 1.1026 - val_acc: 0.8690\n",
            "Epoch 153/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5995 - acc: 0.5829 - val_loss: 1.1024 - val_acc: 0.8690\n",
            "Epoch 154/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5997 - acc: 0.5829 - val_loss: 1.1022 - val_acc: 0.8690\n",
            "Epoch 155/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5990 - acc: 0.5830 - val_loss: 1.1019 - val_acc: 0.8690\n",
            "Epoch 156/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6116 - acc: 0.5829 - val_loss: 1.1017 - val_acc: 0.8690\n",
            "Epoch 157/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6012 - acc: 0.5826 - val_loss: 1.1015 - val_acc: 0.8690\n",
            "Epoch 158/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6016 - acc: 0.5830 - val_loss: 1.1013 - val_acc: 0.8690\n",
            "Epoch 159/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5967 - acc: 0.5830 - val_loss: 1.1010 - val_acc: 0.8690\n",
            "Epoch 160/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5971 - acc: 0.5829 - val_loss: 1.1008 - val_acc: 0.8690\n",
            "Epoch 161/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5965 - acc: 0.5829 - val_loss: 1.1006 - val_acc: 0.8690\n",
            "Epoch 162/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6018 - acc: 0.5830 - val_loss: 1.1004 - val_acc: 0.8690\n",
            "Epoch 163/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5978 - acc: 0.5829 - val_loss: 1.1001 - val_acc: 0.8690\n",
            "Epoch 164/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5975 - acc: 0.5828 - val_loss: 1.0999 - val_acc: 0.8690\n",
            "Epoch 165/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5963 - acc: 0.5829 - val_loss: 1.0997 - val_acc: 0.8690\n",
            "Epoch 166/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5949 - acc: 0.5830 - val_loss: 1.0995 - val_acc: 0.8690\n",
            "Epoch 167/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5989 - acc: 0.5828 - val_loss: 1.0993 - val_acc: 0.8690\n",
            "Epoch 168/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5975 - acc: 0.5829 - val_loss: 1.0990 - val_acc: 0.8690\n",
            "Epoch 169/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5939 - acc: 0.5829 - val_loss: 1.0988 - val_acc: 0.8690\n",
            "Epoch 170/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5970 - acc: 0.5829 - val_loss: 1.0986 - val_acc: 0.8690\n",
            "Epoch 171/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5935 - acc: 0.5831 - val_loss: 1.0984 - val_acc: 0.8690\n",
            "Epoch 172/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5950 - acc: 0.5828 - val_loss: 1.0982 - val_acc: 0.8690\n",
            "Epoch 173/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5944 - acc: 0.5828 - val_loss: 1.0980 - val_acc: 0.8690\n",
            "Epoch 174/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5946 - acc: 0.5830 - val_loss: 1.0977 - val_acc: 0.8690\n",
            "Epoch 175/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5923 - acc: 0.5830 - val_loss: 1.0975 - val_acc: 0.8690\n",
            "Epoch 176/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5922 - acc: 0.5830 - val_loss: 1.0973 - val_acc: 0.8690\n",
            "Epoch 177/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5972 - acc: 0.5827 - val_loss: 1.0971 - val_acc: 0.8690\n",
            "Epoch 178/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5931 - acc: 0.5828 - val_loss: 1.0969 - val_acc: 0.8690\n",
            "Epoch 179/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5981 - acc: 0.5826 - val_loss: 1.0966 - val_acc: 0.8690\n",
            "Epoch 180/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6031 - acc: 0.5829 - val_loss: 1.0964 - val_acc: 0.8690\n",
            "Epoch 181/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5921 - acc: 0.5829 - val_loss: 1.0962 - val_acc: 0.8690\n",
            "Epoch 182/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5915 - acc: 0.5830 - val_loss: 1.0960 - val_acc: 0.8690\n",
            "Epoch 183/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5944 - acc: 0.5828 - val_loss: 1.0958 - val_acc: 0.8690\n",
            "Epoch 184/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5906 - acc: 0.5829 - val_loss: 1.0956 - val_acc: 0.8690\n",
            "Epoch 185/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5912 - acc: 0.5829 - val_loss: 1.0954 - val_acc: 0.8690\n",
            "Epoch 186/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5914 - acc: 0.5829 - val_loss: 1.0951 - val_acc: 0.8690\n",
            "Epoch 187/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5928 - acc: 0.5831 - val_loss: 1.0949 - val_acc: 0.8690\n",
            "Epoch 188/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5899 - acc: 0.5831 - val_loss: 1.0947 - val_acc: 0.8690\n",
            "Epoch 189/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5897 - acc: 0.5829 - val_loss: 1.0945 - val_acc: 0.8690\n",
            "Epoch 190/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5945 - acc: 0.5828 - val_loss: 1.0943 - val_acc: 0.8690\n",
            "Epoch 191/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5921 - acc: 0.5827 - val_loss: 1.0941 - val_acc: 0.8690\n",
            "Epoch 192/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5897 - acc: 0.5828 - val_loss: 1.0939 - val_acc: 0.8690\n",
            "Epoch 193/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5925 - acc: 0.5828 - val_loss: 1.0937 - val_acc: 0.8690\n",
            "Epoch 194/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5925 - acc: 0.5827 - val_loss: 1.0934 - val_acc: 0.8690\n",
            "Epoch 195/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5951 - acc: 0.5826 - val_loss: 1.0932 - val_acc: 0.8690\n",
            "Epoch 196/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5916 - acc: 0.5830 - val_loss: 1.0930 - val_acc: 0.8690\n",
            "Epoch 197/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5917 - acc: 0.5829 - val_loss: 1.0928 - val_acc: 0.8690\n",
            "Epoch 198/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5892 - acc: 0.5830 - val_loss: 1.0926 - val_acc: 0.8690\n",
            "Epoch 199/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6158 - acc: 0.5830 - val_loss: 1.0924 - val_acc: 0.8690\n",
            "Epoch 200/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.6015 - acc: 0.5829 - val_loss: 1.0922 - val_acc: 0.8690\n",
            "Epoch 201/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5868 - acc: 0.5830 - val_loss: 1.0920 - val_acc: 0.8690\n",
            "Epoch 202/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5886 - acc: 0.5827 - val_loss: 1.0918 - val_acc: 0.8690\n",
            "Epoch 203/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5907 - acc: 0.5828 - val_loss: 1.0916 - val_acc: 0.8690\n",
            "Epoch 204/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5918 - acc: 0.5829 - val_loss: 1.0914 - val_acc: 0.8690\n",
            "Epoch 205/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5893 - acc: 0.5828 - val_loss: 1.0911 - val_acc: 0.8690\n",
            "Epoch 206/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5879 - acc: 0.5829 - val_loss: 1.0909 - val_acc: 0.8690\n",
            "Epoch 207/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5863 - acc: 0.5828 - val_loss: 1.0907 - val_acc: 0.8690\n",
            "Epoch 208/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5941 - acc: 0.5830 - val_loss: 1.0905 - val_acc: 0.8690\n",
            "Epoch 209/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5854 - acc: 0.5829 - val_loss: 1.0903 - val_acc: 0.8690\n",
            "Epoch 210/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5869 - acc: 0.5829 - val_loss: 1.0901 - val_acc: 0.8690\n",
            "Epoch 211/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5903 - acc: 0.5830 - val_loss: 1.0899 - val_acc: 0.8690\n",
            "Epoch 212/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5891 - acc: 0.5827 - val_loss: 1.0897 - val_acc: 0.8690\n",
            "Epoch 213/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5868 - acc: 0.5829 - val_loss: 1.0895 - val_acc: 0.8690\n",
            "Epoch 214/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5842 - acc: 0.5829 - val_loss: 1.0893 - val_acc: 0.8690\n",
            "Epoch 215/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5882 - acc: 0.5830 - val_loss: 1.0891 - val_acc: 0.8690\n",
            "Epoch 216/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5846 - acc: 0.5830 - val_loss: 1.0889 - val_acc: 0.8690\n",
            "Epoch 217/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5853 - acc: 0.5829 - val_loss: 1.0887 - val_acc: 0.8690\n",
            "Epoch 218/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5833 - acc: 0.5830 - val_loss: 1.0885 - val_acc: 0.8690\n",
            "Epoch 219/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5845 - acc: 0.5825 - val_loss: 1.0883 - val_acc: 0.8690\n",
            "Epoch 220/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5854 - acc: 0.5828 - val_loss: 1.0881 - val_acc: 0.8690\n",
            "Epoch 221/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5828 - acc: 0.5829 - val_loss: 1.0879 - val_acc: 0.8690\n",
            "Epoch 222/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5895 - acc: 0.5830 - val_loss: 1.0877 - val_acc: 0.8690\n",
            "Epoch 223/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5826 - acc: 0.5828 - val_loss: 1.0874 - val_acc: 0.8690\n",
            "Epoch 224/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5820 - acc: 0.5830 - val_loss: 1.0872 - val_acc: 0.8690\n",
            "Epoch 225/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5848 - acc: 0.5828 - val_loss: 1.0870 - val_acc: 0.8690\n",
            "Epoch 226/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5826 - acc: 0.5828 - val_loss: 1.0868 - val_acc: 0.8690\n",
            "Epoch 227/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5835 - acc: 0.5829 - val_loss: 1.0866 - val_acc: 0.8690\n",
            "Epoch 228/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5823 - acc: 0.5830 - val_loss: 1.0864 - val_acc: 0.8690\n",
            "Epoch 229/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5846 - acc: 0.5828 - val_loss: 1.0862 - val_acc: 0.8690\n",
            "Epoch 230/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5820 - acc: 0.5828 - val_loss: 1.0860 - val_acc: 0.8690\n",
            "Epoch 231/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5812 - acc: 0.5829 - val_loss: 1.0858 - val_acc: 0.8690\n",
            "Epoch 232/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5822 - acc: 0.5829 - val_loss: 1.0856 - val_acc: 0.8690\n",
            "Epoch 233/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5852 - acc: 0.5828 - val_loss: 1.0854 - val_acc: 0.8690\n",
            "Epoch 234/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5844 - acc: 0.5829 - val_loss: 1.0852 - val_acc: 0.8690\n",
            "Epoch 235/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5828 - acc: 0.5830 - val_loss: 1.0850 - val_acc: 0.8690\n",
            "Epoch 236/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5800 - acc: 0.5829 - val_loss: 1.0848 - val_acc: 0.8690\n",
            "Epoch 237/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5823 - acc: 0.5829 - val_loss: 1.0846 - val_acc: 0.8690\n",
            "Epoch 238/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5815 - acc: 0.5829 - val_loss: 1.0844 - val_acc: 0.8690\n",
            "Epoch 239/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5874 - acc: 0.5830 - val_loss: 1.0842 - val_acc: 0.8690\n",
            "Epoch 240/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5843 - acc: 0.5829 - val_loss: 1.0840 - val_acc: 0.8690\n",
            "Epoch 241/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5786 - acc: 0.5830 - val_loss: 1.0838 - val_acc: 0.8690\n",
            "Epoch 242/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5807 - acc: 0.5829 - val_loss: 1.0836 - val_acc: 0.8690\n",
            "Epoch 243/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5809 - acc: 0.5826 - val_loss: 1.0834 - val_acc: 0.8690\n",
            "Epoch 244/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5791 - acc: 0.5830 - val_loss: 1.0832 - val_acc: 0.8690\n",
            "Epoch 245/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5816 - acc: 0.5828 - val_loss: 1.0831 - val_acc: 0.8690\n",
            "Epoch 246/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5800 - acc: 0.5829 - val_loss: 1.0829 - val_acc: 0.8690\n",
            "Epoch 247/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5811 - acc: 0.5829 - val_loss: 1.0827 - val_acc: 0.8690\n",
            "Epoch 248/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5810 - acc: 0.5830 - val_loss: 1.0825 - val_acc: 0.8690\n",
            "Epoch 249/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5817 - acc: 0.5828 - val_loss: 1.0823 - val_acc: 0.8690\n",
            "Epoch 250/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5800 - acc: 0.5827 - val_loss: 1.0821 - val_acc: 0.8690\n",
            "Epoch 251/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5819 - acc: 0.5830 - val_loss: 1.0819 - val_acc: 0.8690\n",
            "Epoch 252/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5797 - acc: 0.5830 - val_loss: 1.0817 - val_acc: 0.8690\n",
            "Epoch 253/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5792 - acc: 0.5829 - val_loss: 1.0815 - val_acc: 0.8690\n",
            "Epoch 254/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5814 - acc: 0.5829 - val_loss: 1.0813 - val_acc: 0.8690\n",
            "Epoch 255/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5763 - acc: 0.5830 - val_loss: 1.0811 - val_acc: 0.8690\n",
            "Epoch 256/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5770 - acc: 0.5828 - val_loss: 1.0809 - val_acc: 0.8690\n",
            "Epoch 257/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5761 - acc: 0.5830 - val_loss: 1.0807 - val_acc: 0.8690\n",
            "Epoch 258/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5797 - acc: 0.5828 - val_loss: 1.0805 - val_acc: 0.8690\n",
            "Epoch 259/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5767 - acc: 0.5828 - val_loss: 1.0803 - val_acc: 0.8690\n",
            "Epoch 260/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5755 - acc: 0.5830 - val_loss: 1.0801 - val_acc: 0.8690\n",
            "Epoch 261/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5792 - acc: 0.5830 - val_loss: 1.0799 - val_acc: 0.8690\n",
            "Epoch 262/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5806 - acc: 0.5828 - val_loss: 1.0797 - val_acc: 0.8690\n",
            "Epoch 263/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5812 - acc: 0.5829 - val_loss: 1.0795 - val_acc: 0.8690\n",
            "Epoch 264/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5752 - acc: 0.5830 - val_loss: 1.0793 - val_acc: 0.8690\n",
            "Epoch 265/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5745 - acc: 0.5830 - val_loss: 1.0792 - val_acc: 0.8690\n",
            "Epoch 266/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5785 - acc: 0.5828 - val_loss: 1.0790 - val_acc: 0.8690\n",
            "Epoch 267/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5803 - acc: 0.5830 - val_loss: 1.0788 - val_acc: 0.8690\n",
            "Epoch 268/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5762 - acc: 0.5826 - val_loss: 1.0786 - val_acc: 0.8690\n",
            "Epoch 269/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5794 - acc: 0.5828 - val_loss: 1.0784 - val_acc: 0.8690\n",
            "Epoch 270/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5737 - acc: 0.5830 - val_loss: 1.0782 - val_acc: 0.8690\n",
            "Epoch 271/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5743 - acc: 0.5830 - val_loss: 1.0780 - val_acc: 0.8690\n",
            "Epoch 272/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5753 - acc: 0.5829 - val_loss: 1.0778 - val_acc: 0.8690\n",
            "Epoch 273/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5740 - acc: 0.5829 - val_loss: 1.0776 - val_acc: 0.8690\n",
            "Epoch 274/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5751 - acc: 0.5829 - val_loss: 1.0774 - val_acc: 0.8690\n",
            "Epoch 275/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5739 - acc: 0.5829 - val_loss: 1.0772 - val_acc: 0.8690\n",
            "Epoch 276/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5754 - acc: 0.5829 - val_loss: 1.0771 - val_acc: 0.8690\n",
            "Epoch 277/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5729 - acc: 0.5830 - val_loss: 1.0769 - val_acc: 0.8690\n",
            "Epoch 278/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5798 - acc: 0.5829 - val_loss: 1.0767 - val_acc: 0.8690\n",
            "Epoch 279/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5773 - acc: 0.5829 - val_loss: 1.0765 - val_acc: 0.8690\n",
            "Epoch 280/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5719 - acc: 0.5830 - val_loss: 1.0763 - val_acc: 0.8690\n",
            "Epoch 281/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5729 - acc: 0.5829 - val_loss: 1.0761 - val_acc: 0.8690\n",
            "Epoch 282/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5731 - acc: 0.5827 - val_loss: 1.0759 - val_acc: 0.8690\n",
            "Epoch 283/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5745 - acc: 0.5827 - val_loss: 1.0757 - val_acc: 0.8690\n",
            "Epoch 284/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5728 - acc: 0.5829 - val_loss: 1.0755 - val_acc: 0.8690\n",
            "Epoch 285/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5722 - acc: 0.5829 - val_loss: 1.0754 - val_acc: 0.8690\n",
            "Epoch 286/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5763 - acc: 0.5828 - val_loss: 1.0752 - val_acc: 0.8690\n",
            "Epoch 287/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5726 - acc: 0.5827 - val_loss: 1.0750 - val_acc: 0.8690\n",
            "Epoch 288/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5708 - acc: 0.5829 - val_loss: 1.0748 - val_acc: 0.8690\n",
            "Epoch 289/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5777 - acc: 0.5828 - val_loss: 1.0746 - val_acc: 0.8690\n",
            "Epoch 290/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5726 - acc: 0.5829 - val_loss: 1.0744 - val_acc: 0.8690\n",
            "Epoch 291/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5702 - acc: 0.5831 - val_loss: 1.0742 - val_acc: 0.8690\n",
            "Epoch 292/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5700 - acc: 0.5830 - val_loss: 1.0740 - val_acc: 0.8690\n",
            "Epoch 293/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.5713 - acc: 0.5829 - val_loss: 1.0739 - val_acc: 0.8690\n",
            "Epoch 294/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5700 - acc: 0.5829 - val_loss: 1.0737 - val_acc: 0.8690\n",
            "Epoch 295/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5700 - acc: 0.5829 - val_loss: 1.0735 - val_acc: 0.8690\n",
            "Epoch 296/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5696 - acc: 0.5829 - val_loss: 1.0733 - val_acc: 0.8690\n",
            "Epoch 297/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5714 - acc: 0.5829 - val_loss: 1.0731 - val_acc: 0.8690\n",
            "Epoch 298/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5694 - acc: 0.5830 - val_loss: 1.0729 - val_acc: 0.8690\n",
            "Epoch 299/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5732 - acc: 0.5829 - val_loss: 1.0727 - val_acc: 0.8690\n",
            "Epoch 300/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5691 - acc: 0.5829 - val_loss: 1.0726 - val_acc: 0.8690\n",
            "Epoch 301/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5729 - acc: 0.5829 - val_loss: 1.0724 - val_acc: 0.8690\n",
            "Epoch 302/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5708 - acc: 0.5829 - val_loss: 1.0722 - val_acc: 0.8690\n",
            "Epoch 303/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5682 - acc: 0.5830 - val_loss: 1.0720 - val_acc: 0.8690\n",
            "Epoch 304/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5696 - acc: 0.5828 - val_loss: 1.0718 - val_acc: 0.8690\n",
            "Epoch 305/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5679 - acc: 0.5830 - val_loss: 1.0716 - val_acc: 0.8690\n",
            "Epoch 306/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5682 - acc: 0.5827 - val_loss: 1.0715 - val_acc: 0.8690\n",
            "Epoch 307/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5703 - acc: 0.5826 - val_loss: 1.0713 - val_acc: 0.8690\n",
            "Epoch 308/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5698 - acc: 0.5830 - val_loss: 1.0711 - val_acc: 0.8690\n",
            "Epoch 309/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5674 - acc: 0.5830 - val_loss: 1.0709 - val_acc: 0.8690\n",
            "Epoch 310/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5670 - acc: 0.5830 - val_loss: 1.0707 - val_acc: 0.8690\n",
            "Epoch 311/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5684 - acc: 0.5828 - val_loss: 1.0705 - val_acc: 0.8690\n",
            "Epoch 312/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5670 - acc: 0.5830 - val_loss: 1.0704 - val_acc: 0.8690\n",
            "Epoch 313/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.5679 - acc: 0.5828 - val_loss: 1.0702 - val_acc: 0.8690\n",
            "Epoch 314/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.5676 - acc: 0.5830 - val_loss: 1.0700 - val_acc: 0.8690\n",
            "Epoch 315/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.5725 - acc: 0.5829 - val_loss: 1.0698 - val_acc: 0.8690\n",
            "Epoch 316/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.5662 - acc: 0.5830 - val_loss: 1.0696 - val_acc: 0.8690\n",
            "Epoch 317/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5679 - acc: 0.5829 - val_loss: 1.0694 - val_acc: 0.8690\n",
            "Epoch 318/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5676 - acc: 0.5827 - val_loss: 1.0693 - val_acc: 0.8690\n",
            "Epoch 319/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5657 - acc: 0.5830 - val_loss: 1.0691 - val_acc: 0.8690\n",
            "Epoch 320/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5688 - acc: 0.5828 - val_loss: 1.0689 - val_acc: 0.8690\n",
            "Epoch 321/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5663 - acc: 0.5829 - val_loss: 1.0687 - val_acc: 0.8690\n",
            "Epoch 322/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5651 - acc: 0.5830 - val_loss: 1.0685 - val_acc: 0.8690\n",
            "Epoch 323/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5688 - acc: 0.5829 - val_loss: 1.0684 - val_acc: 0.8690\n",
            "Epoch 324/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5652 - acc: 0.5830 - val_loss: 1.0682 - val_acc: 0.8690\n",
            "Epoch 325/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5652 - acc: 0.5830 - val_loss: 1.0680 - val_acc: 0.8690\n",
            "Epoch 326/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5674 - acc: 0.5827 - val_loss: 1.0678 - val_acc: 0.8690\n",
            "Epoch 327/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5650 - acc: 0.5828 - val_loss: 1.0676 - val_acc: 0.8690\n",
            "Epoch 328/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5717 - acc: 0.5830 - val_loss: 1.0675 - val_acc: 0.8690\n",
            "Epoch 329/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5651 - acc: 0.5829 - val_loss: 1.0673 - val_acc: 0.8690\n",
            "Epoch 330/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5641 - acc: 0.5830 - val_loss: 1.0671 - val_acc: 0.8690\n",
            "Epoch 331/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5670 - acc: 0.5829 - val_loss: 1.0669 - val_acc: 0.8690\n",
            "Epoch 332/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5655 - acc: 0.5830 - val_loss: 1.0667 - val_acc: 0.8690\n",
            "Epoch 333/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5659 - acc: 0.5828 - val_loss: 1.0666 - val_acc: 0.8690\n",
            "Epoch 334/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5635 - acc: 0.5829 - val_loss: 1.0664 - val_acc: 0.8690\n",
            "Epoch 335/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5636 - acc: 0.5828 - val_loss: 1.0662 - val_acc: 0.8690\n",
            "Epoch 336/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5635 - acc: 0.5830 - val_loss: 1.0660 - val_acc: 0.8690\n",
            "Epoch 337/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5709 - acc: 0.5827 - val_loss: 1.0658 - val_acc: 0.8690\n",
            "Epoch 338/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5651 - acc: 0.5829 - val_loss: 1.0657 - val_acc: 0.8690\n",
            "Epoch 339/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5628 - acc: 0.5830 - val_loss: 1.0655 - val_acc: 0.8690\n",
            "Epoch 340/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5628 - acc: 0.5830 - val_loss: 1.0653 - val_acc: 0.8690\n",
            "Epoch 341/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5664 - acc: 0.5829 - val_loss: 1.0651 - val_acc: 0.8690\n",
            "Epoch 342/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5622 - acc: 0.5830 - val_loss: 1.0650 - val_acc: 0.8690\n",
            "Epoch 343/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5647 - acc: 0.5829 - val_loss: 1.0648 - val_acc: 0.8690\n",
            "Epoch 344/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5622 - acc: 0.5829 - val_loss: 1.0646 - val_acc: 0.8690\n",
            "Epoch 345/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5655 - acc: 0.5829 - val_loss: 1.0644 - val_acc: 0.8690\n",
            "Epoch 346/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5685 - acc: 0.5828 - val_loss: 1.0643 - val_acc: 0.8690\n",
            "Epoch 347/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5628 - acc: 0.5830 - val_loss: 1.0641 - val_acc: 0.8690\n",
            "Epoch 348/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5643 - acc: 0.5827 - val_loss: 1.0639 - val_acc: 0.8690\n",
            "Epoch 349/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5612 - acc: 0.5830 - val_loss: 1.0637 - val_acc: 0.8690\n",
            "Epoch 350/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5671 - acc: 0.5828 - val_loss: 1.0636 - val_acc: 0.8690\n",
            "Epoch 351/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5618 - acc: 0.5829 - val_loss: 1.0634 - val_acc: 0.8690\n",
            "Epoch 352/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5631 - acc: 0.5828 - val_loss: 1.0632 - val_acc: 0.8690\n",
            "Epoch 353/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5610 - acc: 0.5829 - val_loss: 1.0630 - val_acc: 0.8690\n",
            "Epoch 354/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5610 - acc: 0.5830 - val_loss: 1.0629 - val_acc: 0.8690\n",
            "Epoch 355/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5604 - acc: 0.5830 - val_loss: 1.0627 - val_acc: 0.8690\n",
            "Epoch 356/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5643 - acc: 0.5829 - val_loss: 1.0625 - val_acc: 0.8690\n",
            "Epoch 357/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5604 - acc: 0.5830 - val_loss: 1.0623 - val_acc: 0.8690\n",
            "Epoch 358/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5649 - acc: 0.5828 - val_loss: 1.0622 - val_acc: 0.8690\n",
            "Epoch 359/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5602 - acc: 0.5829 - val_loss: 1.0620 - val_acc: 0.8690\n",
            "Epoch 360/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5741 - acc: 0.5829 - val_loss: 1.0618 - val_acc: 0.8690\n",
            "Epoch 361/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5598 - acc: 0.5830 - val_loss: 1.0616 - val_acc: 0.8690\n",
            "Epoch 362/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5606 - acc: 0.5829 - val_loss: 1.0615 - val_acc: 0.8690\n",
            "Epoch 363/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5595 - acc: 0.5830 - val_loss: 1.0613 - val_acc: 0.8690\n",
            "Epoch 364/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5603 - acc: 0.5830 - val_loss: 1.0611 - val_acc: 0.8690\n",
            "Epoch 365/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5592 - acc: 0.5830 - val_loss: 1.0609 - val_acc: 0.8690\n",
            "Epoch 366/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5605 - acc: 0.5829 - val_loss: 1.0608 - val_acc: 0.8690\n",
            "Epoch 367/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5615 - acc: 0.5829 - val_loss: 1.0606 - val_acc: 0.8690\n",
            "Epoch 368/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5601 - acc: 0.5829 - val_loss: 1.0604 - val_acc: 0.8690\n",
            "Epoch 369/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5588 - acc: 0.5829 - val_loss: 1.0603 - val_acc: 0.8690\n",
            "Epoch 370/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5598 - acc: 0.5825 - val_loss: 1.0601 - val_acc: 0.8690\n",
            "Epoch 371/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5628 - acc: 0.5830 - val_loss: 1.0599 - val_acc: 0.8690\n",
            "Epoch 372/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5656 - acc: 0.5827 - val_loss: 1.0597 - val_acc: 0.8690\n",
            "Epoch 373/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5599 - acc: 0.5829 - val_loss: 1.0596 - val_acc: 0.8690\n",
            "Epoch 374/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5584 - acc: 0.5830 - val_loss: 1.0594 - val_acc: 0.8690\n",
            "Epoch 375/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5598 - acc: 0.5829 - val_loss: 1.0592 - val_acc: 0.8690\n",
            "Epoch 376/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5581 - acc: 0.5829 - val_loss: 1.0590 - val_acc: 0.8690\n",
            "Epoch 377/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5606 - acc: 0.5829 - val_loss: 1.0589 - val_acc: 0.8690\n",
            "Epoch 378/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5574 - acc: 0.5830 - val_loss: 1.0587 - val_acc: 0.8690\n",
            "Epoch 379/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5571 - acc: 0.5830 - val_loss: 1.0585 - val_acc: 0.8690\n",
            "Epoch 380/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5569 - acc: 0.5830 - val_loss: 1.0584 - val_acc: 0.8690\n",
            "Epoch 381/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5567 - acc: 0.5830 - val_loss: 1.0582 - val_acc: 0.8690\n",
            "Epoch 382/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5578 - acc: 0.5829 - val_loss: 1.0580 - val_acc: 0.8690\n",
            "Epoch 383/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5569 - acc: 0.5829 - val_loss: 1.0579 - val_acc: 0.8690\n",
            "Epoch 384/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5580 - acc: 0.5830 - val_loss: 1.0577 - val_acc: 0.8690\n",
            "Epoch 385/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5560 - acc: 0.5831 - val_loss: 1.0575 - val_acc: 0.8690\n",
            "Epoch 386/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5562 - acc: 0.5829 - val_loss: 1.0573 - val_acc: 0.8690\n",
            "Epoch 387/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5566 - acc: 0.5829 - val_loss: 1.0572 - val_acc: 0.8690\n",
            "Epoch 388/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5623 - acc: 0.5830 - val_loss: 1.0570 - val_acc: 0.8690\n",
            "Epoch 389/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5564 - acc: 0.5828 - val_loss: 1.0568 - val_acc: 0.8690\n",
            "Epoch 390/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5557 - acc: 0.5830 - val_loss: 1.0567 - val_acc: 0.8690\n",
            "Epoch 391/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5577 - acc: 0.5829 - val_loss: 1.0565 - val_acc: 0.8690\n",
            "Epoch 392/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5558 - acc: 0.5829 - val_loss: 1.0563 - val_acc: 0.8690\n",
            "Epoch 393/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5569 - acc: 0.5829 - val_loss: 1.0562 - val_acc: 0.8690\n",
            "Epoch 394/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5606 - acc: 0.5828 - val_loss: 1.0560 - val_acc: 0.8690\n",
            "Epoch 395/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5558 - acc: 0.5829 - val_loss: 1.0558 - val_acc: 0.8690\n",
            "Epoch 396/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5549 - acc: 0.5830 - val_loss: 1.0557 - val_acc: 0.8690\n",
            "Epoch 397/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5559 - acc: 0.5829 - val_loss: 1.0555 - val_acc: 0.8690\n",
            "Epoch 398/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5567 - acc: 0.5829 - val_loss: 1.0553 - val_acc: 0.8690\n",
            "Epoch 399/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5552 - acc: 0.5830 - val_loss: 1.0552 - val_acc: 0.8690\n",
            "Epoch 400/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5555 - acc: 0.5827 - val_loss: 1.0550 - val_acc: 0.8690\n",
            "Epoch 401/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5551 - acc: 0.5830 - val_loss: 1.0548 - val_acc: 0.8690\n",
            "Epoch 402/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5559 - acc: 0.5827 - val_loss: 1.0547 - val_acc: 0.8690\n",
            "Epoch 403/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5551 - acc: 0.5829 - val_loss: 1.0545 - val_acc: 0.8690\n",
            "Epoch 404/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5539 - acc: 0.5830 - val_loss: 1.0543 - val_acc: 0.8690\n",
            "Epoch 405/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5557 - acc: 0.5830 - val_loss: 1.0542 - val_acc: 0.8690\n",
            "Epoch 406/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5537 - acc: 0.5830 - val_loss: 1.0540 - val_acc: 0.8690\n",
            "Epoch 407/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5536 - acc: 0.5830 - val_loss: 1.0538 - val_acc: 0.8690\n",
            "Epoch 408/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5554 - acc: 0.5829 - val_loss: 1.0537 - val_acc: 0.8690\n",
            "Epoch 409/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5543 - acc: 0.5828 - val_loss: 1.0535 - val_acc: 0.8690\n",
            "Epoch 410/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5542 - acc: 0.5829 - val_loss: 1.0533 - val_acc: 0.8690\n",
            "Epoch 411/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5595 - acc: 0.5827 - val_loss: 1.0532 - val_acc: 0.8690\n",
            "Epoch 412/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5529 - acc: 0.5830 - val_loss: 1.0530 - val_acc: 0.8690\n",
            "Epoch 413/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5527 - acc: 0.5830 - val_loss: 1.0528 - val_acc: 0.8690\n",
            "Epoch 414/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5526 - acc: 0.5830 - val_loss: 1.0527 - val_acc: 0.8690\n",
            "Epoch 415/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5524 - acc: 0.5830 - val_loss: 1.0525 - val_acc: 0.8690\n",
            "Epoch 416/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5558 - acc: 0.5830 - val_loss: 1.0523 - val_acc: 0.8690\n",
            "Epoch 417/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5567 - acc: 0.5830 - val_loss: 1.0522 - val_acc: 0.8690\n",
            "Epoch 418/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5524 - acc: 0.5829 - val_loss: 1.0520 - val_acc: 0.8690\n",
            "Epoch 419/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5524 - acc: 0.5829 - val_loss: 1.0518 - val_acc: 0.8690\n",
            "Epoch 420/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5527 - acc: 0.5829 - val_loss: 1.0517 - val_acc: 0.8690\n",
            "Epoch 421/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5525 - acc: 0.5831 - val_loss: 1.0515 - val_acc: 0.8690\n",
            "Epoch 422/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5571 - acc: 0.5828 - val_loss: 1.0513 - val_acc: 0.8690\n",
            "Epoch 423/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5537 - acc: 0.5828 - val_loss: 1.0512 - val_acc: 0.8690\n",
            "Epoch 424/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5517 - acc: 0.5829 - val_loss: 1.0510 - val_acc: 0.8690\n",
            "Epoch 425/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5540 - acc: 0.5829 - val_loss: 1.0509 - val_acc: 0.8690\n",
            "Epoch 426/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5517 - acc: 0.5830 - val_loss: 1.0507 - val_acc: 0.8690\n",
            "Epoch 427/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5523 - acc: 0.5830 - val_loss: 1.0505 - val_acc: 0.8690\n",
            "Epoch 428/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5544 - acc: 0.5828 - val_loss: 1.0504 - val_acc: 0.8690\n",
            "Epoch 429/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5551 - acc: 0.5829 - val_loss: 1.0502 - val_acc: 0.8690\n",
            "Epoch 430/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5518 - acc: 0.5829 - val_loss: 1.0500 - val_acc: 0.8690\n",
            "Epoch 431/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5528 - acc: 0.5829 - val_loss: 1.0499 - val_acc: 0.8690\n",
            "Epoch 432/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5513 - acc: 0.5828 - val_loss: 1.0497 - val_acc: 0.8690\n",
            "Epoch 433/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5509 - acc: 0.5829 - val_loss: 1.0496 - val_acc: 0.8690\n",
            "Epoch 434/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5526 - acc: 0.5828 - val_loss: 1.0494 - val_acc: 0.8690\n",
            "Epoch 435/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5506 - acc: 0.5829 - val_loss: 1.0492 - val_acc: 0.8690\n",
            "Epoch 436/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5522 - acc: 0.5828 - val_loss: 1.0491 - val_acc: 0.8690\n",
            "Epoch 437/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5497 - acc: 0.5830 - val_loss: 1.0489 - val_acc: 0.8690\n",
            "Epoch 438/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5510 - acc: 0.5830 - val_loss: 1.0487 - val_acc: 0.8690\n",
            "Epoch 439/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5495 - acc: 0.5830 - val_loss: 1.0486 - val_acc: 0.8690\n",
            "Epoch 440/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5495 - acc: 0.5830 - val_loss: 1.0484 - val_acc: 0.8690\n",
            "Epoch 441/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5514 - acc: 0.5828 - val_loss: 1.0483 - val_acc: 0.8690\n",
            "Epoch 442/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5489 - acc: 0.5830 - val_loss: 1.0481 - val_acc: 0.8690\n",
            "Epoch 443/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5500 - acc: 0.5829 - val_loss: 1.0479 - val_acc: 0.8690\n",
            "Epoch 444/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5516 - acc: 0.5828 - val_loss: 1.0478 - val_acc: 0.8690\n",
            "Epoch 445/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5513 - acc: 0.5829 - val_loss: 1.0476 - val_acc: 0.8690\n",
            "Epoch 446/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5489 - acc: 0.5830 - val_loss: 1.0475 - val_acc: 0.8690\n",
            "Epoch 447/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5486 - acc: 0.5830 - val_loss: 1.0473 - val_acc: 0.8690\n",
            "Epoch 448/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5495 - acc: 0.5830 - val_loss: 1.0471 - val_acc: 0.8690\n",
            "Epoch 449/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5482 - acc: 0.5831 - val_loss: 1.0470 - val_acc: 0.8690\n",
            "Epoch 450/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5491 - acc: 0.5829 - val_loss: 1.0468 - val_acc: 0.8690\n",
            "Epoch 451/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5505 - acc: 0.5829 - val_loss: 1.0467 - val_acc: 0.8690\n",
            "Epoch 452/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5490 - acc: 0.5830 - val_loss: 1.0465 - val_acc: 0.8690\n",
            "Epoch 453/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5538 - acc: 0.5827 - val_loss: 1.0463 - val_acc: 0.8690\n",
            "Epoch 454/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5486 - acc: 0.5831 - val_loss: 1.0462 - val_acc: 0.8690\n",
            "Epoch 455/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5509 - acc: 0.5829 - val_loss: 1.0460 - val_acc: 0.8690\n",
            "Epoch 456/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5541 - acc: 0.5828 - val_loss: 1.0459 - val_acc: 0.8690\n",
            "Epoch 457/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5475 - acc: 0.5830 - val_loss: 1.0457 - val_acc: 0.8690\n",
            "Epoch 458/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5488 - acc: 0.5829 - val_loss: 1.0455 - val_acc: 0.8690\n",
            "Epoch 459/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5498 - acc: 0.5829 - val_loss: 1.0454 - val_acc: 0.8690\n",
            "Epoch 460/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5491 - acc: 0.5829 - val_loss: 1.0452 - val_acc: 0.8690\n",
            "Epoch 461/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5501 - acc: 0.5830 - val_loss: 1.0451 - val_acc: 0.8690\n",
            "Epoch 462/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5597 - acc: 0.5830 - val_loss: 1.0449 - val_acc: 0.8690\n",
            "Epoch 463/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5479 - acc: 0.5830 - val_loss: 1.0448 - val_acc: 0.8690\n",
            "Epoch 464/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5494 - acc: 0.5828 - val_loss: 1.0446 - val_acc: 0.8690\n",
            "Epoch 465/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5521 - acc: 0.5828 - val_loss: 1.0444 - val_acc: 0.8690\n",
            "Epoch 466/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5475 - acc: 0.5829 - val_loss: 1.0443 - val_acc: 0.8690\n",
            "Epoch 467/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5482 - acc: 0.5827 - val_loss: 1.0441 - val_acc: 0.8690\n",
            "Epoch 468/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5472 - acc: 0.5828 - val_loss: 1.0440 - val_acc: 0.8690\n",
            "Epoch 469/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5472 - acc: 0.5829 - val_loss: 1.0438 - val_acc: 0.8690\n",
            "Epoch 470/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5468 - acc: 0.5830 - val_loss: 1.0437 - val_acc: 0.8690\n",
            "Epoch 471/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5478 - acc: 0.5828 - val_loss: 1.0435 - val_acc: 0.8690\n",
            "Epoch 472/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5482 - acc: 0.5828 - val_loss: 1.0433 - val_acc: 0.8690\n",
            "Epoch 473/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5485 - acc: 0.5829 - val_loss: 1.0432 - val_acc: 0.8690\n",
            "Epoch 474/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5457 - acc: 0.5830 - val_loss: 1.0430 - val_acc: 0.8690\n",
            "Epoch 475/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5454 - acc: 0.5830 - val_loss: 1.0429 - val_acc: 0.8690\n",
            "Epoch 476/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5495 - acc: 0.5827 - val_loss: 1.0427 - val_acc: 0.8690\n",
            "Epoch 477/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5455 - acc: 0.5830 - val_loss: 1.0426 - val_acc: 0.8690\n",
            "Epoch 478/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5467 - acc: 0.5828 - val_loss: 1.0424 - val_acc: 0.8690\n",
            "Epoch 479/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5451 - acc: 0.5830 - val_loss: 1.0422 - val_acc: 0.8690\n",
            "Epoch 480/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5457 - acc: 0.5828 - val_loss: 1.0421 - val_acc: 0.8690\n",
            "Epoch 481/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5452 - acc: 0.5830 - val_loss: 1.0419 - val_acc: 0.8690\n",
            "Epoch 482/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5475 - acc: 0.5830 - val_loss: 1.0418 - val_acc: 0.8690\n",
            "Epoch 483/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5455 - acc: 0.5831 - val_loss: 1.0416 - val_acc: 0.8690\n",
            "Epoch 484/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5499 - acc: 0.5828 - val_loss: 1.0415 - val_acc: 0.8690\n",
            "Epoch 485/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5447 - acc: 0.5829 - val_loss: 1.0413 - val_acc: 0.8690\n",
            "Epoch 486/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5482 - acc: 0.5829 - val_loss: 1.0412 - val_acc: 0.8690\n",
            "Epoch 487/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5467 - acc: 0.5829 - val_loss: 1.0410 - val_acc: 0.8690\n",
            "Epoch 488/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5459 - acc: 0.5829 - val_loss: 1.0409 - val_acc: 0.8690\n",
            "Epoch 489/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5457 - acc: 0.5830 - val_loss: 1.0407 - val_acc: 0.8690\n",
            "Epoch 490/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5450 - acc: 0.5830 - val_loss: 1.0405 - val_acc: 0.8690\n",
            "Epoch 491/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5469 - acc: 0.5829 - val_loss: 1.0404 - val_acc: 0.8690\n",
            "Epoch 492/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5506 - acc: 0.5828 - val_loss: 1.0402 - val_acc: 0.8690\n",
            "Epoch 493/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5436 - acc: 0.5830 - val_loss: 1.0401 - val_acc: 0.8690\n",
            "Epoch 494/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5448 - acc: 0.5827 - val_loss: 1.0399 - val_acc: 0.8690\n",
            "Epoch 495/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5438 - acc: 0.5829 - val_loss: 1.0398 - val_acc: 0.8690\n",
            "Epoch 496/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5437 - acc: 0.5828 - val_loss: 1.0396 - val_acc: 0.8690\n",
            "Epoch 497/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5430 - acc: 0.5831 - val_loss: 1.0395 - val_acc: 0.8690\n",
            "Epoch 498/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5432 - acc: 0.5829 - val_loss: 1.0393 - val_acc: 0.8690\n",
            "Epoch 499/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5430 - acc: 0.5830 - val_loss: 1.0392 - val_acc: 0.8690\n",
            "Epoch 500/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5429 - acc: 0.5830 - val_loss: 1.0390 - val_acc: 0.8690\n",
            "Epoch 501/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5430 - acc: 0.5830 - val_loss: 1.0389 - val_acc: 0.8690\n",
            "Epoch 502/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5436 - acc: 0.5829 - val_loss: 1.0387 - val_acc: 0.8690\n",
            "Epoch 503/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5426 - acc: 0.5830 - val_loss: 1.0386 - val_acc: 0.8690\n",
            "Epoch 504/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5438 - acc: 0.5828 - val_loss: 1.0384 - val_acc: 0.8690\n",
            "Epoch 505/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5461 - acc: 0.5829 - val_loss: 1.0383 - val_acc: 0.8690\n",
            "Epoch 506/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5461 - acc: 0.5828 - val_loss: 1.0381 - val_acc: 0.8690\n",
            "Epoch 507/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5423 - acc: 0.5830 - val_loss: 1.0379 - val_acc: 0.8690\n",
            "Epoch 508/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5424 - acc: 0.5829 - val_loss: 1.0378 - val_acc: 0.8690\n",
            "Epoch 509/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5452 - acc: 0.5827 - val_loss: 1.0376 - val_acc: 0.8690\n",
            "Epoch 510/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5419 - acc: 0.5829 - val_loss: 1.0375 - val_acc: 0.8690\n",
            "Epoch 511/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5436 - acc: 0.5829 - val_loss: 1.0373 - val_acc: 0.8690\n",
            "Epoch 512/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5437 - acc: 0.5829 - val_loss: 1.0372 - val_acc: 0.8690\n",
            "Epoch 513/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5475 - acc: 0.5828 - val_loss: 1.0370 - val_acc: 0.8690\n",
            "Epoch 514/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5414 - acc: 0.5830 - val_loss: 1.0369 - val_acc: 0.8690\n",
            "Epoch 515/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5426 - acc: 0.5829 - val_loss: 1.0367 - val_acc: 0.8690\n",
            "Epoch 516/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5411 - acc: 0.5830 - val_loss: 1.0366 - val_acc: 0.8690\n",
            "Epoch 517/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5428 - acc: 0.5830 - val_loss: 1.0364 - val_acc: 0.8690\n",
            "Epoch 518/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5438 - acc: 0.5830 - val_loss: 1.0363 - val_acc: 0.8690\n",
            "Epoch 519/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5437 - acc: 0.5826 - val_loss: 1.0361 - val_acc: 0.8690\n",
            "Epoch 520/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5458 - acc: 0.5829 - val_loss: 1.0360 - val_acc: 0.8690\n",
            "Epoch 521/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5408 - acc: 0.5830 - val_loss: 1.0358 - val_acc: 0.8690\n",
            "Epoch 522/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5412 - acc: 0.5830 - val_loss: 1.0357 - val_acc: 0.8690\n",
            "Epoch 523/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5437 - acc: 0.5830 - val_loss: 1.0355 - val_acc: 0.8690\n",
            "Epoch 524/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5462 - acc: 0.5829 - val_loss: 1.0354 - val_acc: 0.8690\n",
            "Epoch 525/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5443 - acc: 0.5828 - val_loss: 1.0352 - val_acc: 0.8690\n",
            "Epoch 526/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5405 - acc: 0.5830 - val_loss: 1.0351 - val_acc: 0.8690\n",
            "Epoch 527/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5430 - acc: 0.5829 - val_loss: 1.0349 - val_acc: 0.8690\n",
            "Epoch 528/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5425 - acc: 0.5828 - val_loss: 1.0348 - val_acc: 0.8690\n",
            "Epoch 529/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5409 - acc: 0.5830 - val_loss: 1.0346 - val_acc: 0.8690\n",
            "Epoch 530/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5441 - acc: 0.5828 - val_loss: 1.0345 - val_acc: 0.8690\n",
            "Epoch 531/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5409 - acc: 0.5827 - val_loss: 1.0343 - val_acc: 0.8690\n",
            "Epoch 532/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5399 - acc: 0.5829 - val_loss: 1.0342 - val_acc: 0.8690\n",
            "Epoch 533/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5504 - acc: 0.5828 - val_loss: 1.0340 - val_acc: 0.8690\n",
            "Epoch 534/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5394 - acc: 0.5830 - val_loss: 1.0339 - val_acc: 0.8690\n",
            "Epoch 535/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5392 - acc: 0.5830 - val_loss: 1.0338 - val_acc: 0.8690\n",
            "Epoch 536/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5443 - acc: 0.5826 - val_loss: 1.0336 - val_acc: 0.8690\n",
            "Epoch 537/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5393 - acc: 0.5829 - val_loss: 1.0335 - val_acc: 0.8690\n",
            "Epoch 538/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5399 - acc: 0.5830 - val_loss: 1.0333 - val_acc: 0.8690\n",
            "Epoch 539/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5395 - acc: 0.5829 - val_loss: 1.0332 - val_acc: 0.8690\n",
            "Epoch 540/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5388 - acc: 0.5830 - val_loss: 1.0330 - val_acc: 0.8690\n",
            "Epoch 541/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5390 - acc: 0.5829 - val_loss: 1.0329 - val_acc: 0.8690\n",
            "Epoch 542/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5401 - acc: 0.5829 - val_loss: 1.0327 - val_acc: 0.8690\n",
            "Epoch 543/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5384 - acc: 0.5830 - val_loss: 1.0326 - val_acc: 0.8690\n",
            "Epoch 544/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5399 - acc: 0.5829 - val_loss: 1.0324 - val_acc: 0.8690\n",
            "Epoch 545/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5408 - acc: 0.5829 - val_loss: 1.0323 - val_acc: 0.8690\n",
            "Epoch 546/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5381 - acc: 0.5829 - val_loss: 1.0321 - val_acc: 0.8690\n",
            "Epoch 547/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5390 - acc: 0.5829 - val_loss: 1.0320 - val_acc: 0.8690\n",
            "Epoch 548/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5384 - acc: 0.5830 - val_loss: 1.0318 - val_acc: 0.8690\n",
            "Epoch 549/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5397 - acc: 0.5831 - val_loss: 1.0317 - val_acc: 0.8690\n",
            "Epoch 550/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5402 - acc: 0.5829 - val_loss: 1.0315 - val_acc: 0.8690\n",
            "Epoch 551/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5376 - acc: 0.5831 - val_loss: 1.0314 - val_acc: 0.8690\n",
            "Epoch 552/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5377 - acc: 0.5830 - val_loss: 1.0313 - val_acc: 0.8690\n",
            "Epoch 553/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5376 - acc: 0.5830 - val_loss: 1.0311 - val_acc: 0.8690\n",
            "Epoch 554/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5383 - acc: 0.5830 - val_loss: 1.0310 - val_acc: 0.8690\n",
            "Epoch 555/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5412 - acc: 0.5830 - val_loss: 1.0308 - val_acc: 0.8690\n",
            "Epoch 556/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5374 - acc: 0.5830 - val_loss: 1.0307 - val_acc: 0.8690\n",
            "Epoch 557/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5372 - acc: 0.5830 - val_loss: 1.0305 - val_acc: 0.8690\n",
            "Epoch 558/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5371 - acc: 0.5829 - val_loss: 1.0304 - val_acc: 0.8690\n",
            "Epoch 559/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5378 - acc: 0.5829 - val_loss: 1.0302 - val_acc: 0.8690\n",
            "Epoch 560/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5373 - acc: 0.5830 - val_loss: 1.0301 - val_acc: 0.8690\n",
            "Epoch 561/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5368 - acc: 0.5830 - val_loss: 1.0300 - val_acc: 0.8690\n",
            "Epoch 562/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5371 - acc: 0.5830 - val_loss: 1.0298 - val_acc: 0.8690\n",
            "Epoch 563/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5366 - acc: 0.5830 - val_loss: 1.0297 - val_acc: 0.8690\n",
            "Epoch 564/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5364 - acc: 0.5830 - val_loss: 1.0295 - val_acc: 0.8690\n",
            "Epoch 565/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5364 - acc: 0.5830 - val_loss: 1.0294 - val_acc: 0.8690\n",
            "Epoch 566/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5363 - acc: 0.5830 - val_loss: 1.0292 - val_acc: 0.8690\n",
            "Epoch 567/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5375 - acc: 0.5828 - val_loss: 1.0291 - val_acc: 0.8690\n",
            "Epoch 568/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5362 - acc: 0.5830 - val_loss: 1.0289 - val_acc: 0.8690\n",
            "Epoch 569/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5361 - acc: 0.5830 - val_loss: 1.0288 - val_acc: 0.8690\n",
            "Epoch 570/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5377 - acc: 0.5830 - val_loss: 1.0287 - val_acc: 0.8690\n",
            "Epoch 571/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5359 - acc: 0.5830 - val_loss: 1.0285 - val_acc: 0.8690\n",
            "Epoch 572/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5357 - acc: 0.5830 - val_loss: 1.0284 - val_acc: 0.8690\n",
            "Epoch 573/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5353 - acc: 0.5831 - val_loss: 1.0282 - val_acc: 0.8690\n",
            "Epoch 574/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5356 - acc: 0.5830 - val_loss: 1.0281 - val_acc: 0.8690\n",
            "Epoch 575/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5354 - acc: 0.5830 - val_loss: 1.0279 - val_acc: 0.8690\n",
            "Epoch 576/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5361 - acc: 0.5830 - val_loss: 1.0278 - val_acc: 0.8690\n",
            "Epoch 577/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5349 - acc: 0.5833 - val_loss: 1.0277 - val_acc: 0.8690\n",
            "Epoch 578/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5412 - acc: 0.5829 - val_loss: 1.0275 - val_acc: 0.8690\n",
            "Epoch 579/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5391 - acc: 0.5829 - val_loss: 1.0274 - val_acc: 0.8690\n",
            "Epoch 580/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5353 - acc: 0.5830 - val_loss: 1.0272 - val_acc: 0.8690\n",
            "Epoch 581/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5350 - acc: 0.5830 - val_loss: 1.0271 - val_acc: 0.8690\n",
            "Epoch 582/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5349 - acc: 0.5830 - val_loss: 1.0269 - val_acc: 0.8690\n",
            "Epoch 583/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5364 - acc: 0.5829 - val_loss: 1.0268 - val_acc: 0.8690\n",
            "Epoch 584/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5346 - acc: 0.5830 - val_loss: 1.0267 - val_acc: 0.8690\n",
            "Epoch 585/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5355 - acc: 0.5831 - val_loss: 1.0265 - val_acc: 0.8690\n",
            "Epoch 586/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5349 - acc: 0.5829 - val_loss: 1.0264 - val_acc: 0.8690\n",
            "Epoch 587/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5402 - acc: 0.5829 - val_loss: 1.0262 - val_acc: 0.8690\n",
            "Epoch 588/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5342 - acc: 0.5830 - val_loss: 1.0261 - val_acc: 0.8690\n",
            "Epoch 589/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5351 - acc: 0.5829 - val_loss: 1.0260 - val_acc: 0.8690\n",
            "Epoch 590/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5341 - acc: 0.5830 - val_loss: 1.0258 - val_acc: 0.8690\n",
            "Epoch 591/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5340 - acc: 0.5830 - val_loss: 1.0257 - val_acc: 0.8690\n",
            "Epoch 592/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5349 - acc: 0.5829 - val_loss: 1.0255 - val_acc: 0.8690\n",
            "Epoch 593/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5362 - acc: 0.5829 - val_loss: 1.0254 - val_acc: 0.8690\n",
            "Epoch 594/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5349 - acc: 0.5830 - val_loss: 1.0253 - val_acc: 0.8690\n",
            "Epoch 595/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5337 - acc: 0.5830 - val_loss: 1.0251 - val_acc: 0.8690\n",
            "Epoch 596/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5391 - acc: 0.5829 - val_loss: 1.0250 - val_acc: 0.8690\n",
            "Epoch 597/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5336 - acc: 0.5830 - val_loss: 1.0248 - val_acc: 0.8690\n",
            "Epoch 598/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5336 - acc: 0.5830 - val_loss: 1.0247 - val_acc: 0.8690\n",
            "Epoch 599/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5346 - acc: 0.5829 - val_loss: 1.0246 - val_acc: 0.8690\n",
            "Epoch 600/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5331 - acc: 0.5831 - val_loss: 1.0244 - val_acc: 0.8690\n",
            "Epoch 601/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5332 - acc: 0.5830 - val_loss: 1.0243 - val_acc: 0.8690\n",
            "Epoch 602/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5348 - acc: 0.5829 - val_loss: 1.0241 - val_acc: 0.8690\n",
            "Epoch 603/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5332 - acc: 0.5829 - val_loss: 1.0240 - val_acc: 0.8690\n",
            "Epoch 604/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5330 - acc: 0.5829 - val_loss: 1.0239 - val_acc: 0.8690\n",
            "Epoch 605/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5343 - acc: 0.5829 - val_loss: 1.0237 - val_acc: 0.8690\n",
            "Epoch 606/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5332 - acc: 0.5828 - val_loss: 1.0236 - val_acc: 0.8690\n",
            "Epoch 607/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5328 - acc: 0.5829 - val_loss: 1.0234 - val_acc: 0.8690\n",
            "Epoch 608/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5340 - acc: 0.5828 - val_loss: 1.0233 - val_acc: 0.8690\n",
            "Epoch 609/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5345 - acc: 0.5827 - val_loss: 1.0232 - val_acc: 0.8690\n",
            "Epoch 610/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5343 - acc: 0.5829 - val_loss: 1.0230 - val_acc: 0.8690\n",
            "Epoch 611/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5325 - acc: 0.5830 - val_loss: 1.0229 - val_acc: 0.8690\n",
            "Epoch 612/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5334 - acc: 0.5830 - val_loss: 1.0228 - val_acc: 0.8690\n",
            "Epoch 613/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5342 - acc: 0.5827 - val_loss: 1.0226 - val_acc: 0.8690\n",
            "Epoch 614/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5322 - acc: 0.5829 - val_loss: 1.0225 - val_acc: 0.8690\n",
            "Epoch 615/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5359 - acc: 0.5829 - val_loss: 1.0223 - val_acc: 0.8690\n",
            "Epoch 616/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5319 - acc: 0.5830 - val_loss: 1.0222 - val_acc: 0.8690\n",
            "Epoch 617/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5319 - acc: 0.5829 - val_loss: 1.0221 - val_acc: 0.8690\n",
            "Epoch 618/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5317 - acc: 0.5830 - val_loss: 1.0219 - val_acc: 0.8690\n",
            "Epoch 619/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5316 - acc: 0.5830 - val_loss: 1.0218 - val_acc: 0.8690\n",
            "Epoch 620/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5315 - acc: 0.5829 - val_loss: 1.0217 - val_acc: 0.8690\n",
            "Epoch 621/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5334 - acc: 0.5831 - val_loss: 1.0215 - val_acc: 0.8690\n",
            "Epoch 622/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5312 - acc: 0.5830 - val_loss: 1.0214 - val_acc: 0.8690\n",
            "Epoch 623/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5312 - acc: 0.5830 - val_loss: 1.0212 - val_acc: 0.8690\n",
            "Epoch 624/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5312 - acc: 0.5830 - val_loss: 1.0211 - val_acc: 0.8690\n",
            "Epoch 625/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5327 - acc: 0.5829 - val_loss: 1.0210 - val_acc: 0.8690\n",
            "Epoch 626/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5310 - acc: 0.5830 - val_loss: 1.0208 - val_acc: 0.8690\n",
            "Epoch 627/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5309 - acc: 0.5830 - val_loss: 1.0207 - val_acc: 0.8690\n",
            "Epoch 628/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5309 - acc: 0.5830 - val_loss: 1.0206 - val_acc: 0.8690\n",
            "Epoch 629/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5314 - acc: 0.5830 - val_loss: 1.0204 - val_acc: 0.8690\n",
            "Epoch 630/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5312 - acc: 0.5829 - val_loss: 1.0203 - val_acc: 0.8690\n",
            "Epoch 631/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5306 - acc: 0.5830 - val_loss: 1.0202 - val_acc: 0.8690\n",
            "Epoch 632/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5323 - acc: 0.5830 - val_loss: 1.0200 - val_acc: 0.8690\n",
            "Epoch 633/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5348 - acc: 0.5828 - val_loss: 1.0199 - val_acc: 0.8690\n",
            "Epoch 634/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5303 - acc: 0.5830 - val_loss: 1.0198 - val_acc: 0.8690\n",
            "Epoch 635/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5364 - acc: 0.5828 - val_loss: 1.0196 - val_acc: 0.8690\n",
            "Epoch 636/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5314 - acc: 0.5829 - val_loss: 1.0195 - val_acc: 0.8690\n",
            "Epoch 637/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5314 - acc: 0.5828 - val_loss: 1.0193 - val_acc: 0.8690\n",
            "Epoch 638/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5325 - acc: 0.5828 - val_loss: 1.0192 - val_acc: 0.8690\n",
            "Epoch 639/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5300 - acc: 0.5830 - val_loss: 1.0191 - val_acc: 0.8690\n",
            "Epoch 640/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5326 - acc: 0.5830 - val_loss: 1.0189 - val_acc: 0.8690\n",
            "Epoch 641/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5306 - acc: 0.5829 - val_loss: 1.0188 - val_acc: 0.8690\n",
            "Epoch 642/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5344 - acc: 0.5831 - val_loss: 1.0187 - val_acc: 0.8690\n",
            "Epoch 643/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5345 - acc: 0.5826 - val_loss: 1.0185 - val_acc: 0.8690\n",
            "Epoch 644/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5339 - acc: 0.5829 - val_loss: 1.0184 - val_acc: 0.8690\n",
            "Epoch 645/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5294 - acc: 0.5830 - val_loss: 1.0183 - val_acc: 0.8690\n",
            "Epoch 646/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5305 - acc: 0.5827 - val_loss: 1.0181 - val_acc: 0.8690\n",
            "Epoch 647/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5293 - acc: 0.5830 - val_loss: 1.0180 - val_acc: 0.8690\n",
            "Epoch 648/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5301 - acc: 0.5830 - val_loss: 1.0179 - val_acc: 0.8690\n",
            "Epoch 649/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5328 - acc: 0.5827 - val_loss: 1.0177 - val_acc: 0.8690\n",
            "Epoch 650/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5308 - acc: 0.5828 - val_loss: 1.0176 - val_acc: 0.8690\n",
            "Epoch 651/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5287 - acc: 0.5830 - val_loss: 1.0175 - val_acc: 0.8690\n",
            "Epoch 652/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5312 - acc: 0.5829 - val_loss: 1.0173 - val_acc: 0.8690\n",
            "Epoch 653/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5338 - acc: 0.5828 - val_loss: 1.0172 - val_acc: 0.8690\n",
            "Epoch 654/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5289 - acc: 0.5830 - val_loss: 1.0171 - val_acc: 0.8690\n",
            "Epoch 655/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5285 - acc: 0.5830 - val_loss: 1.0169 - val_acc: 0.8690\n",
            "Epoch 656/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5287 - acc: 0.5830 - val_loss: 1.0168 - val_acc: 0.8690\n",
            "Epoch 657/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5327 - acc: 0.5829 - val_loss: 1.0167 - val_acc: 0.8690\n",
            "Epoch 658/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5310 - acc: 0.5830 - val_loss: 1.0166 - val_acc: 0.8690\n",
            "Epoch 659/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5308 - acc: 0.5828 - val_loss: 1.0164 - val_acc: 0.8690\n",
            "Epoch 660/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5302 - acc: 0.5829 - val_loss: 1.0163 - val_acc: 0.8690\n",
            "Epoch 661/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5333 - acc: 0.5829 - val_loss: 1.0162 - val_acc: 0.8690\n",
            "Epoch 662/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5291 - acc: 0.5829 - val_loss: 1.0160 - val_acc: 0.8690\n",
            "Epoch 663/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5300 - acc: 0.5829 - val_loss: 1.0159 - val_acc: 0.8690\n",
            "Epoch 664/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5279 - acc: 0.5830 - val_loss: 1.0158 - val_acc: 0.8690\n",
            "Epoch 665/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5292 - acc: 0.5830 - val_loss: 1.0156 - val_acc: 0.8690\n",
            "Epoch 666/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5326 - acc: 0.5827 - val_loss: 1.0155 - val_acc: 0.8690\n",
            "Epoch 667/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5283 - acc: 0.5828 - val_loss: 1.0154 - val_acc: 0.8690\n",
            "Epoch 668/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5279 - acc: 0.5829 - val_loss: 1.0152 - val_acc: 0.8690\n",
            "Epoch 669/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5313 - acc: 0.5830 - val_loss: 1.0151 - val_acc: 0.8690\n",
            "Epoch 670/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5281 - acc: 0.5829 - val_loss: 1.0150 - val_acc: 0.8690\n",
            "Epoch 671/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5275 - acc: 0.5830 - val_loss: 1.0148 - val_acc: 0.8690\n",
            "Epoch 672/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5276 - acc: 0.5830 - val_loss: 1.0147 - val_acc: 0.8690\n",
            "Epoch 673/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5276 - acc: 0.5830 - val_loss: 1.0146 - val_acc: 0.8690\n",
            "Epoch 674/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5279 - acc: 0.5829 - val_loss: 1.0145 - val_acc: 0.8690\n",
            "Epoch 675/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5322 - acc: 0.5829 - val_loss: 1.0143 - val_acc: 0.8690\n",
            "Epoch 676/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5275 - acc: 0.5830 - val_loss: 1.0142 - val_acc: 0.8690\n",
            "Epoch 677/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5281 - acc: 0.5830 - val_loss: 1.0141 - val_acc: 0.8690\n",
            "Epoch 678/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5269 - acc: 0.5829 - val_loss: 1.0139 - val_acc: 0.8690\n",
            "Epoch 679/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5326 - acc: 0.5829 - val_loss: 1.0138 - val_acc: 0.8690\n",
            "Epoch 680/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5277 - acc: 0.5829 - val_loss: 1.0137 - val_acc: 0.8690\n",
            "Epoch 681/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5270 - acc: 0.5830 - val_loss: 1.0136 - val_acc: 0.8690\n",
            "Epoch 682/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5266 - acc: 0.5830 - val_loss: 1.0134 - val_acc: 0.8690\n",
            "Epoch 683/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5264 - acc: 0.5830 - val_loss: 1.0133 - val_acc: 0.8690\n",
            "Epoch 684/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5265 - acc: 0.5829 - val_loss: 1.0132 - val_acc: 0.8690\n",
            "Epoch 685/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5263 - acc: 0.5830 - val_loss: 1.0130 - val_acc: 0.8690\n",
            "Epoch 686/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5262 - acc: 0.5829 - val_loss: 1.0129 - val_acc: 0.8690\n",
            "Epoch 687/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5262 - acc: 0.5830 - val_loss: 1.0128 - val_acc: 0.8690\n",
            "Epoch 688/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5260 - acc: 0.5830 - val_loss: 1.0127 - val_acc: 0.8690\n",
            "Epoch 689/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5259 - acc: 0.5830 - val_loss: 1.0125 - val_acc: 0.8690\n",
            "Epoch 690/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5271 - acc: 0.5829 - val_loss: 1.0124 - val_acc: 0.8690\n",
            "Epoch 691/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5295 - acc: 0.5828 - val_loss: 1.0123 - val_acc: 0.8690\n",
            "Epoch 692/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5284 - acc: 0.5829 - val_loss: 1.0121 - val_acc: 0.8690\n",
            "Epoch 693/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5257 - acc: 0.5830 - val_loss: 1.0120 - val_acc: 0.8690\n",
            "Epoch 694/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5256 - acc: 0.5830 - val_loss: 1.0119 - val_acc: 0.8690\n",
            "Epoch 695/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5264 - acc: 0.5829 - val_loss: 1.0118 - val_acc: 0.8690\n",
            "Epoch 696/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5255 - acc: 0.5830 - val_loss: 1.0116 - val_acc: 0.8690\n",
            "Epoch 697/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5253 - acc: 0.5830 - val_loss: 1.0115 - val_acc: 0.8690\n",
            "Epoch 698/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5261 - acc: 0.5831 - val_loss: 1.0114 - val_acc: 0.8690\n",
            "Epoch 699/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5259 - acc: 0.5830 - val_loss: 1.0112 - val_acc: 0.8690\n",
            "Epoch 700/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5251 - acc: 0.5830 - val_loss: 1.0111 - val_acc: 0.8690\n",
            "Epoch 701/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5249 - acc: 0.5830 - val_loss: 1.0110 - val_acc: 0.8690\n",
            "Epoch 702/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5253 - acc: 0.5830 - val_loss: 1.0109 - val_acc: 0.8690\n",
            "Epoch 703/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5248 - acc: 0.5830 - val_loss: 1.0107 - val_acc: 0.8690\n",
            "Epoch 704/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5248 - acc: 0.5831 - val_loss: 1.0106 - val_acc: 0.8690\n",
            "Epoch 705/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5250 - acc: 0.5830 - val_loss: 1.0105 - val_acc: 0.8690\n",
            "Epoch 706/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5250 - acc: 0.5830 - val_loss: 1.0104 - val_acc: 0.8690\n",
            "Epoch 707/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5259 - acc: 0.5829 - val_loss: 1.0102 - val_acc: 0.8690\n",
            "Epoch 708/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5252 - acc: 0.5830 - val_loss: 1.0101 - val_acc: 0.8690\n",
            "Epoch 709/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5245 - acc: 0.5830 - val_loss: 1.0100 - val_acc: 0.8690\n",
            "Epoch 710/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5264 - acc: 0.5829 - val_loss: 1.0099 - val_acc: 0.8690\n",
            "Epoch 711/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5261 - acc: 0.5830 - val_loss: 1.0097 - val_acc: 0.8690\n",
            "Epoch 712/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5259 - acc: 0.5827 - val_loss: 1.0096 - val_acc: 0.8690\n",
            "Epoch 713/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5276 - acc: 0.5829 - val_loss: 1.0095 - val_acc: 0.8690\n",
            "Epoch 714/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5306 - acc: 0.5829 - val_loss: 1.0094 - val_acc: 0.8690\n",
            "Epoch 715/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5239 - acc: 0.5830 - val_loss: 1.0092 - val_acc: 0.8690\n",
            "Epoch 716/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5254 - acc: 0.5829 - val_loss: 1.0091 - val_acc: 0.8690\n",
            "Epoch 717/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5239 - acc: 0.5830 - val_loss: 1.0090 - val_acc: 0.8690\n",
            "Epoch 718/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5237 - acc: 0.5830 - val_loss: 1.0089 - val_acc: 0.8690\n",
            "Epoch 719/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5238 - acc: 0.5830 - val_loss: 1.0087 - val_acc: 0.8690\n",
            "Epoch 720/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5268 - acc: 0.5829 - val_loss: 1.0086 - val_acc: 0.8690\n",
            "Epoch 721/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5240 - acc: 0.5830 - val_loss: 1.0085 - val_acc: 0.8690\n",
            "Epoch 722/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5234 - acc: 0.5830 - val_loss: 1.0084 - val_acc: 0.8690\n",
            "Epoch 723/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5234 - acc: 0.5830 - val_loss: 1.0082 - val_acc: 0.8690\n",
            "Epoch 724/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5239 - acc: 0.5829 - val_loss: 1.0081 - val_acc: 0.8690\n",
            "Epoch 725/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5244 - acc: 0.5828 - val_loss: 1.0080 - val_acc: 0.8690\n",
            "Epoch 726/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5232 - acc: 0.5830 - val_loss: 1.0079 - val_acc: 0.8690\n",
            "Epoch 727/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5230 - acc: 0.5830 - val_loss: 1.0078 - val_acc: 0.8690\n",
            "Epoch 728/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5250 - acc: 0.5830 - val_loss: 1.0076 - val_acc: 0.8690\n",
            "Epoch 729/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5236 - acc: 0.5828 - val_loss: 1.0075 - val_acc: 0.8690\n",
            "Epoch 730/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5233 - acc: 0.5830 - val_loss: 1.0074 - val_acc: 0.8690\n",
            "Epoch 731/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5246 - acc: 0.5830 - val_loss: 1.0073 - val_acc: 0.8690\n",
            "Epoch 732/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5237 - acc: 0.5830 - val_loss: 1.0071 - val_acc: 0.8690\n",
            "Epoch 733/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5243 - acc: 0.5829 - val_loss: 1.0070 - val_acc: 0.8690\n",
            "Epoch 734/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5235 - acc: 0.5830 - val_loss: 1.0069 - val_acc: 0.8690\n",
            "Epoch 735/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5226 - acc: 0.5829 - val_loss: 1.0068 - val_acc: 0.8690\n",
            "Epoch 736/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5224 - acc: 0.5831 - val_loss: 1.0066 - val_acc: 0.8690\n",
            "Epoch 737/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5224 - acc: 0.5830 - val_loss: 1.0065 - val_acc: 0.8690\n",
            "Epoch 738/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5254 - acc: 0.5830 - val_loss: 1.0064 - val_acc: 0.8690\n",
            "Epoch 739/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5244 - acc: 0.5829 - val_loss: 1.0063 - val_acc: 0.8690\n",
            "Epoch 740/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5223 - acc: 0.5830 - val_loss: 1.0062 - val_acc: 0.8690\n",
            "Epoch 741/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5221 - acc: 0.5830 - val_loss: 1.0060 - val_acc: 0.8690\n",
            "Epoch 742/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5236 - acc: 0.5829 - val_loss: 1.0059 - val_acc: 0.8690\n",
            "Epoch 743/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5239 - acc: 0.5828 - val_loss: 1.0058 - val_acc: 0.8690\n",
            "Epoch 744/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5220 - acc: 0.5829 - val_loss: 1.0057 - val_acc: 0.8690\n",
            "Epoch 745/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5217 - acc: 0.5830 - val_loss: 1.0056 - val_acc: 0.8690\n",
            "Epoch 746/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5219 - acc: 0.5830 - val_loss: 1.0054 - val_acc: 0.8690\n",
            "Epoch 747/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5234 - acc: 0.5830 - val_loss: 1.0053 - val_acc: 0.8690\n",
            "Epoch 748/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5217 - acc: 0.5830 - val_loss: 1.0052 - val_acc: 0.8690\n",
            "Epoch 749/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5214 - acc: 0.5831 - val_loss: 1.0051 - val_acc: 0.8690\n",
            "Epoch 750/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5229 - acc: 0.5829 - val_loss: 1.0049 - val_acc: 0.8690\n",
            "Epoch 751/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5223 - acc: 0.5830 - val_loss: 1.0048 - val_acc: 0.8690\n",
            "Epoch 752/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5214 - acc: 0.5830 - val_loss: 1.0047 - val_acc: 0.8690\n",
            "Epoch 753/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5214 - acc: 0.5830 - val_loss: 1.0046 - val_acc: 0.8690\n",
            "Epoch 754/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5242 - acc: 0.5828 - val_loss: 1.0045 - val_acc: 0.8690\n",
            "Epoch 755/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5212 - acc: 0.5830 - val_loss: 1.0043 - val_acc: 0.8690\n",
            "Epoch 756/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5216 - acc: 0.5829 - val_loss: 1.0042 - val_acc: 0.8690\n",
            "Epoch 757/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5221 - acc: 0.5829 - val_loss: 1.0041 - val_acc: 0.8690\n",
            "Epoch 758/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5209 - acc: 0.5830 - val_loss: 1.0040 - val_acc: 0.8690\n",
            "Epoch 759/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5209 - acc: 0.5830 - val_loss: 1.0039 - val_acc: 0.8690\n",
            "Epoch 760/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5217 - acc: 0.5829 - val_loss: 1.0038 - val_acc: 0.8690\n",
            "Epoch 761/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5257 - acc: 0.5830 - val_loss: 1.0036 - val_acc: 0.8690\n",
            "Epoch 762/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5207 - acc: 0.5830 - val_loss: 1.0035 - val_acc: 0.8690\n",
            "Epoch 763/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5206 - acc: 0.5830 - val_loss: 1.0034 - val_acc: 0.8690\n",
            "Epoch 764/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5204 - acc: 0.5830 - val_loss: 1.0033 - val_acc: 0.8690\n",
            "Epoch 765/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.5223 - acc: 0.5829 - val_loss: 1.0032 - val_acc: 0.8690\n",
            "Epoch 766/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.5208 - acc: 0.5829 - val_loss: 1.0030 - val_acc: 0.8690\n",
            "Epoch 767/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.5211 - acc: 0.5830 - val_loss: 1.0029 - val_acc: 0.8690\n",
            "Epoch 768/1000\n",
            "1/1 [==============================] - 3s 3s/step - loss: 1.5211 - acc: 0.5828 - val_loss: 1.0028 - val_acc: 0.8690\n",
            "Epoch 769/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.5220 - acc: 0.5829 - val_loss: 1.0027 - val_acc: 0.8690\n",
            "Epoch 770/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.5219 - acc: 0.5829 - val_loss: 1.0026 - val_acc: 0.8690\n",
            "Epoch 771/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5219 - acc: 0.5830 - val_loss: 1.0024 - val_acc: 0.8690\n",
            "Epoch 772/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5223 - acc: 0.5829 - val_loss: 1.0023 - val_acc: 0.8690\n",
            "Epoch 773/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5198 - acc: 0.5830 - val_loss: 1.0022 - val_acc: 0.8690\n",
            "Epoch 774/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5247 - acc: 0.5829 - val_loss: 1.0021 - val_acc: 0.8690\n",
            "Epoch 775/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5208 - acc: 0.5829 - val_loss: 1.0020 - val_acc: 0.8690\n",
            "Epoch 776/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5197 - acc: 0.5830 - val_loss: 1.0019 - val_acc: 0.8690\n",
            "Epoch 777/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5211 - acc: 0.5827 - val_loss: 1.0017 - val_acc: 0.8690\n",
            "Epoch 778/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5197 - acc: 0.5830 - val_loss: 1.0016 - val_acc: 0.8690\n",
            "Epoch 779/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5193 - acc: 0.5830 - val_loss: 1.0015 - val_acc: 0.8690\n",
            "Epoch 780/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5241 - acc: 0.5828 - val_loss: 1.0014 - val_acc: 0.8690\n",
            "Epoch 781/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5224 - acc: 0.5830 - val_loss: 1.0013 - val_acc: 0.8690\n",
            "Epoch 782/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5193 - acc: 0.5830 - val_loss: 1.0012 - val_acc: 0.8690\n",
            "Epoch 783/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5193 - acc: 0.5830 - val_loss: 1.0010 - val_acc: 0.8690\n",
            "Epoch 784/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5218 - acc: 0.5828 - val_loss: 1.0009 - val_acc: 0.8690\n",
            "Epoch 785/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5202 - acc: 0.5828 - val_loss: 1.0008 - val_acc: 0.8690\n",
            "Epoch 786/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5198 - acc: 0.5830 - val_loss: 1.0007 - val_acc: 0.8690\n",
            "Epoch 787/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5193 - acc: 0.5830 - val_loss: 1.0006 - val_acc: 0.8690\n",
            "Epoch 788/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5211 - acc: 0.5829 - val_loss: 1.0005 - val_acc: 0.8690\n",
            "Epoch 789/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5219 - acc: 0.5829 - val_loss: 1.0003 - val_acc: 0.8690\n",
            "Epoch 790/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5205 - acc: 0.5830 - val_loss: 1.0002 - val_acc: 0.8690\n",
            "Epoch 791/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5208 - acc: 0.5829 - val_loss: 1.0001 - val_acc: 0.8690\n",
            "Epoch 792/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5226 - acc: 0.5829 - val_loss: 1.0000 - val_acc: 0.8690\n",
            "Epoch 793/1000\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.5184 - acc: 0.5830 - val_loss: 0.9999 - val_acc: 0.8690\n",
            "Epoch 794/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5224 - acc: 0.5830 - val_loss: 0.9998 - val_acc: 0.8690\n",
            "Epoch 795/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5187 - acc: 0.5829 - val_loss: 0.9996 - val_acc: 0.8690\n",
            "Epoch 796/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5186 - acc: 0.5829 - val_loss: 0.9995 - val_acc: 0.8690\n",
            "Epoch 797/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5207 - acc: 0.5829 - val_loss: 0.9994 - val_acc: 0.8690\n",
            "Epoch 798/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5190 - acc: 0.5828 - val_loss: 0.9993 - val_acc: 0.8690\n",
            "Epoch 799/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5183 - acc: 0.5830 - val_loss: 0.9992 - val_acc: 0.8690\n",
            "Epoch 800/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5182 - acc: 0.5830 - val_loss: 0.9991 - val_acc: 0.8690\n",
            "Epoch 801/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5198 - acc: 0.5829 - val_loss: 0.9990 - val_acc: 0.8690\n",
            "Epoch 802/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5182 - acc: 0.5830 - val_loss: 0.9988 - val_acc: 0.8690\n",
            "Epoch 803/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5184 - acc: 0.5829 - val_loss: 0.9987 - val_acc: 0.8690\n",
            "Epoch 804/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5184 - acc: 0.5829 - val_loss: 0.9986 - val_acc: 0.8690\n",
            "Epoch 805/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5191 - acc: 0.5829 - val_loss: 0.9985 - val_acc: 0.8690\n",
            "Epoch 806/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5181 - acc: 0.5830 - val_loss: 0.9984 - val_acc: 0.8690\n",
            "Epoch 807/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5177 - acc: 0.5830 - val_loss: 0.9983 - val_acc: 0.8690\n",
            "Epoch 808/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5182 - acc: 0.5829 - val_loss: 0.9982 - val_acc: 0.8690\n",
            "Epoch 809/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5183 - acc: 0.5829 - val_loss: 0.9981 - val_acc: 0.8690\n",
            "Epoch 810/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5177 - acc: 0.5830 - val_loss: 0.9979 - val_acc: 0.8690\n",
            "Epoch 811/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5174 - acc: 0.5830 - val_loss: 0.9978 - val_acc: 0.8690\n",
            "Epoch 812/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5175 - acc: 0.5830 - val_loss: 0.9977 - val_acc: 0.8690\n",
            "Epoch 813/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5173 - acc: 0.5830 - val_loss: 0.9976 - val_acc: 0.8690\n",
            "Epoch 814/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5173 - acc: 0.5830 - val_loss: 0.9975 - val_acc: 0.8690\n",
            "Epoch 815/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5179 - acc: 0.5829 - val_loss: 0.9974 - val_acc: 0.8690\n",
            "Epoch 816/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5181 - acc: 0.5828 - val_loss: 0.9973 - val_acc: 0.8690\n",
            "Epoch 817/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5171 - acc: 0.5830 - val_loss: 0.9971 - val_acc: 0.8690\n",
            "Epoch 818/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5231 - acc: 0.5829 - val_loss: 0.9970 - val_acc: 0.8690\n",
            "Epoch 819/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5170 - acc: 0.5830 - val_loss: 0.9969 - val_acc: 0.8690\n",
            "Epoch 820/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5171 - acc: 0.5830 - val_loss: 0.9968 - val_acc: 0.8690\n",
            "Epoch 821/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5181 - acc: 0.5830 - val_loss: 0.9967 - val_acc: 0.8690\n",
            "Epoch 822/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5169 - acc: 0.5830 - val_loss: 0.9966 - val_acc: 0.8690\n",
            "Epoch 823/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5167 - acc: 0.5830 - val_loss: 0.9965 - val_acc: 0.8690\n",
            "Epoch 824/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5190 - acc: 0.5830 - val_loss: 0.9964 - val_acc: 0.8690\n",
            "Epoch 825/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5182 - acc: 0.5829 - val_loss: 0.9963 - val_acc: 0.8690\n",
            "Epoch 826/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5164 - acc: 0.5830 - val_loss: 0.9961 - val_acc: 0.8690\n",
            "Epoch 827/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5165 - acc: 0.5829 - val_loss: 0.9960 - val_acc: 0.8690\n",
            "Epoch 828/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5169 - acc: 0.5829 - val_loss: 0.9959 - val_acc: 0.8690\n",
            "Epoch 829/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5213 - acc: 0.5830 - val_loss: 0.9958 - val_acc: 0.8690\n",
            "Epoch 830/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5196 - acc: 0.5828 - val_loss: 0.9957 - val_acc: 0.8690\n",
            "Epoch 831/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5159 - acc: 0.5830 - val_loss: 0.9956 - val_acc: 0.8690\n",
            "Epoch 832/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5162 - acc: 0.5830 - val_loss: 0.9955 - val_acc: 0.8690\n",
            "Epoch 833/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5160 - acc: 0.5830 - val_loss: 0.9954 - val_acc: 0.8690\n",
            "Epoch 834/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5165 - acc: 0.5830 - val_loss: 0.9953 - val_acc: 0.8690\n",
            "Epoch 835/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5163 - acc: 0.5830 - val_loss: 0.9951 - val_acc: 0.8690\n",
            "Epoch 836/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5228 - acc: 0.5829 - val_loss: 0.9950 - val_acc: 0.8690\n",
            "Epoch 837/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5158 - acc: 0.5830 - val_loss: 0.9949 - val_acc: 0.8690\n",
            "Epoch 838/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5169 - acc: 0.5830 - val_loss: 0.9948 - val_acc: 0.8690\n",
            "Epoch 839/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5157 - acc: 0.5830 - val_loss: 0.9947 - val_acc: 0.8690\n",
            "Epoch 840/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5168 - acc: 0.5828 - val_loss: 0.9946 - val_acc: 0.8690\n",
            "Epoch 841/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5175 - acc: 0.5830 - val_loss: 0.9945 - val_acc: 0.8690\n",
            "Epoch 842/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5160 - acc: 0.5830 - val_loss: 0.9944 - val_acc: 0.8690\n",
            "Epoch 843/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5154 - acc: 0.5830 - val_loss: 0.9943 - val_acc: 0.8690\n",
            "Epoch 844/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5154 - acc: 0.5830 - val_loss: 0.9942 - val_acc: 0.8690\n",
            "Epoch 845/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5153 - acc: 0.5830 - val_loss: 0.9941 - val_acc: 0.8690\n",
            "Epoch 846/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5162 - acc: 0.5829 - val_loss: 0.9939 - val_acc: 0.8690\n",
            "Epoch 847/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5152 - acc: 0.5831 - val_loss: 0.9938 - val_acc: 0.8690\n",
            "Epoch 848/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5152 - acc: 0.5830 - val_loss: 0.9937 - val_acc: 0.8690\n",
            "Epoch 849/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5152 - acc: 0.5830 - val_loss: 0.9936 - val_acc: 0.8690\n",
            "Epoch 850/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5150 - acc: 0.5830 - val_loss: 0.9935 - val_acc: 0.8690\n",
            "Epoch 851/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5149 - acc: 0.5830 - val_loss: 0.9934 - val_acc: 0.8690\n",
            "Epoch 852/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5154 - acc: 0.5829 - val_loss: 0.9933 - val_acc: 0.8690\n",
            "Epoch 853/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5151 - acc: 0.5828 - val_loss: 0.9932 - val_acc: 0.8690\n",
            "Epoch 854/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5148 - acc: 0.5830 - val_loss: 0.9931 - val_acc: 0.8690\n",
            "Epoch 855/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5165 - acc: 0.5830 - val_loss: 0.9930 - val_acc: 0.8690\n",
            "Epoch 856/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5149 - acc: 0.5830 - val_loss: 0.9929 - val_acc: 0.8690\n",
            "Epoch 857/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5146 - acc: 0.5830 - val_loss: 0.9928 - val_acc: 0.8690\n",
            "Epoch 858/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5148 - acc: 0.5829 - val_loss: 0.9926 - val_acc: 0.8690\n",
            "Epoch 859/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5145 - acc: 0.5830 - val_loss: 0.9925 - val_acc: 0.8690\n",
            "Epoch 860/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5144 - acc: 0.5830 - val_loss: 0.9924 - val_acc: 0.8690\n",
            "Epoch 861/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5147 - acc: 0.5830 - val_loss: 0.9923 - val_acc: 0.8690\n",
            "Epoch 862/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5143 - acc: 0.5830 - val_loss: 0.9922 - val_acc: 0.8690\n",
            "Epoch 863/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5142 - acc: 0.5830 - val_loss: 0.9921 - val_acc: 0.8690\n",
            "Epoch 864/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5144 - acc: 0.5829 - val_loss: 0.9920 - val_acc: 0.8690\n",
            "Epoch 865/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5153 - acc: 0.5829 - val_loss: 0.9919 - val_acc: 0.8690\n",
            "Epoch 866/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5141 - acc: 0.5830 - val_loss: 0.9918 - val_acc: 0.8690\n",
            "Epoch 867/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5140 - acc: 0.5830 - val_loss: 0.9917 - val_acc: 0.8690\n",
            "Epoch 868/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5139 - acc: 0.5830 - val_loss: 0.9916 - val_acc: 0.8690\n",
            "Epoch 869/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5156 - acc: 0.5830 - val_loss: 0.9915 - val_acc: 0.8690\n",
            "Epoch 870/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5147 - acc: 0.5830 - val_loss: 0.9914 - val_acc: 0.8690\n",
            "Epoch 871/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5198 - acc: 0.5828 - val_loss: 0.9913 - val_acc: 0.8690\n",
            "Epoch 872/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5137 - acc: 0.5830 - val_loss: 0.9912 - val_acc: 0.8690\n",
            "Epoch 873/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5135 - acc: 0.5831 - val_loss: 0.9910 - val_acc: 0.8690\n",
            "Epoch 874/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5137 - acc: 0.5830 - val_loss: 0.9909 - val_acc: 0.8690\n",
            "Epoch 875/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5134 - acc: 0.5830 - val_loss: 0.9908 - val_acc: 0.8690\n",
            "Epoch 876/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5137 - acc: 0.5830 - val_loss: 0.9907 - val_acc: 0.8690\n",
            "Epoch 877/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5134 - acc: 0.5830 - val_loss: 0.9906 - val_acc: 0.8690\n",
            "Epoch 878/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5133 - acc: 0.5830 - val_loss: 0.9905 - val_acc: 0.8690\n",
            "Epoch 879/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5133 - acc: 0.5830 - val_loss: 0.9904 - val_acc: 0.8690\n",
            "Epoch 880/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5143 - acc: 0.5830 - val_loss: 0.9903 - val_acc: 0.8690\n",
            "Epoch 881/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5132 - acc: 0.5830 - val_loss: 0.9902 - val_acc: 0.8690\n",
            "Epoch 882/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5143 - acc: 0.5829 - val_loss: 0.9901 - val_acc: 0.8690\n",
            "Epoch 883/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5132 - acc: 0.5830 - val_loss: 0.9900 - val_acc: 0.8690\n",
            "Epoch 884/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5131 - acc: 0.5830 - val_loss: 0.9899 - val_acc: 0.8690\n",
            "Epoch 885/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5129 - acc: 0.5830 - val_loss: 0.9898 - val_acc: 0.8690\n",
            "Epoch 886/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5135 - acc: 0.5830 - val_loss: 0.9897 - val_acc: 0.8690\n",
            "Epoch 887/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5127 - acc: 0.5830 - val_loss: 0.9896 - val_acc: 0.8690\n",
            "Epoch 888/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5133 - acc: 0.5830 - val_loss: 0.9895 - val_acc: 0.8690\n",
            "Epoch 889/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5126 - acc: 0.5830 - val_loss: 0.9894 - val_acc: 0.8690\n",
            "Epoch 890/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5142 - acc: 0.5830 - val_loss: 0.9893 - val_acc: 0.8690\n",
            "Epoch 891/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5124 - acc: 0.5830 - val_loss: 0.9892 - val_acc: 0.8690\n",
            "Epoch 892/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5201 - acc: 0.5829 - val_loss: 0.9891 - val_acc: 0.8690\n",
            "Epoch 893/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5126 - acc: 0.5829 - val_loss: 0.9890 - val_acc: 0.8690\n",
            "Epoch 894/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5121 - acc: 0.5831 - val_loss: 0.9889 - val_acc: 0.8690\n",
            "Epoch 895/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5145 - acc: 0.5828 - val_loss: 0.9887 - val_acc: 0.8690\n",
            "Epoch 896/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5123 - acc: 0.5830 - val_loss: 0.9886 - val_acc: 0.8690\n",
            "Epoch 897/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5125 - acc: 0.5830 - val_loss: 0.9885 - val_acc: 0.8690\n",
            "Epoch 898/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5122 - acc: 0.5830 - val_loss: 0.9884 - val_acc: 0.8690\n",
            "Epoch 899/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5123 - acc: 0.5830 - val_loss: 0.9883 - val_acc: 0.8690\n",
            "Epoch 900/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5123 - acc: 0.5829 - val_loss: 0.9882 - val_acc: 0.8690\n",
            "Epoch 901/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5121 - acc: 0.5830 - val_loss: 0.9881 - val_acc: 0.8690\n",
            "Epoch 902/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5159 - acc: 0.5830 - val_loss: 0.9880 - val_acc: 0.8690\n",
            "Epoch 903/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5131 - acc: 0.5829 - val_loss: 0.9879 - val_acc: 0.8690\n",
            "Epoch 904/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5124 - acc: 0.5830 - val_loss: 0.9878 - val_acc: 0.8690\n",
            "Epoch 905/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5195 - acc: 0.5828 - val_loss: 0.9877 - val_acc: 0.8690\n",
            "Epoch 906/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5115 - acc: 0.5830 - val_loss: 0.9876 - val_acc: 0.8690\n",
            "Epoch 907/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5126 - acc: 0.5830 - val_loss: 0.9875 - val_acc: 0.8690\n",
            "Epoch 908/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5145 - acc: 0.5828 - val_loss: 0.9874 - val_acc: 0.8690\n",
            "Epoch 909/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5131 - acc: 0.5830 - val_loss: 0.9873 - val_acc: 0.8690\n",
            "Epoch 910/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5138 - acc: 0.5829 - val_loss: 0.9872 - val_acc: 0.8690\n",
            "Epoch 911/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5158 - acc: 0.5829 - val_loss: 0.9871 - val_acc: 0.8690\n",
            "Epoch 912/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5118 - acc: 0.5830 - val_loss: 0.9870 - val_acc: 0.8690\n",
            "Epoch 913/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5113 - acc: 0.5830 - val_loss: 0.9869 - val_acc: 0.8690\n",
            "Epoch 914/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5118 - acc: 0.5829 - val_loss: 0.9868 - val_acc: 0.8690\n",
            "Epoch 915/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5112 - acc: 0.5830 - val_loss: 0.9867 - val_acc: 0.8690\n",
            "Epoch 916/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5113 - acc: 0.5830 - val_loss: 0.9866 - val_acc: 0.8690\n",
            "Epoch 917/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5111 - acc: 0.5830 - val_loss: 0.9865 - val_acc: 0.8690\n",
            "Epoch 918/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5121 - acc: 0.5830 - val_loss: 0.9864 - val_acc: 0.8690\n",
            "Epoch 919/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5126 - acc: 0.5830 - val_loss: 0.9863 - val_acc: 0.8690\n",
            "Epoch 920/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5113 - acc: 0.5830 - val_loss: 0.9862 - val_acc: 0.8690\n",
            "Epoch 921/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5109 - acc: 0.5830 - val_loss: 0.9861 - val_acc: 0.8690\n",
            "Epoch 922/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5110 - acc: 0.5830 - val_loss: 0.9860 - val_acc: 0.8690\n",
            "Epoch 923/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5108 - acc: 0.5830 - val_loss: 0.9859 - val_acc: 0.8690\n",
            "Epoch 924/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5111 - acc: 0.5830 - val_loss: 0.9858 - val_acc: 0.8690\n",
            "Epoch 925/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5107 - acc: 0.5830 - val_loss: 0.9857 - val_acc: 0.8690\n",
            "Epoch 926/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5117 - acc: 0.5830 - val_loss: 0.9856 - val_acc: 0.8690\n",
            "Epoch 927/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5106 - acc: 0.5830 - val_loss: 0.9855 - val_acc: 0.8690\n",
            "Epoch 928/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5117 - acc: 0.5829 - val_loss: 0.9854 - val_acc: 0.8690\n",
            "Epoch 929/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5108 - acc: 0.5830 - val_loss: 0.9853 - val_acc: 0.8690\n",
            "Epoch 930/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5104 - acc: 0.5830 - val_loss: 0.9852 - val_acc: 0.8690\n",
            "Epoch 931/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5104 - acc: 0.5830 - val_loss: 0.9851 - val_acc: 0.8690\n",
            "Epoch 932/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5104 - acc: 0.5829 - val_loss: 0.9850 - val_acc: 0.8690\n",
            "Epoch 933/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5103 - acc: 0.5830 - val_loss: 0.9849 - val_acc: 0.8690\n",
            "Epoch 934/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5103 - acc: 0.5829 - val_loss: 0.9848 - val_acc: 0.8690\n",
            "Epoch 935/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5101 - acc: 0.5830 - val_loss: 0.9847 - val_acc: 0.8690\n",
            "Epoch 936/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5141 - acc: 0.5829 - val_loss: 0.9846 - val_acc: 0.8690\n",
            "Epoch 937/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5114 - acc: 0.5827 - val_loss: 0.9845 - val_acc: 0.8690\n",
            "Epoch 938/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5107 - acc: 0.5829 - val_loss: 0.9844 - val_acc: 0.8690\n",
            "Epoch 939/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5100 - acc: 0.5830 - val_loss: 0.9843 - val_acc: 0.8690\n",
            "Epoch 940/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5110 - acc: 0.5830 - val_loss: 0.9842 - val_acc: 0.8690\n",
            "Epoch 941/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5099 - acc: 0.5830 - val_loss: 0.9841 - val_acc: 0.8690\n",
            "Epoch 942/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5101 - acc: 0.5830 - val_loss: 0.9840 - val_acc: 0.8690\n",
            "Epoch 943/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5096 - acc: 0.5831 - val_loss: 0.9839 - val_acc: 0.8690\n",
            "Epoch 944/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5096 - acc: 0.5830 - val_loss: 0.9838 - val_acc: 0.8690\n",
            "Epoch 945/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5102 - acc: 0.5829 - val_loss: 0.9837 - val_acc: 0.8690\n",
            "Epoch 946/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5103 - acc: 0.5830 - val_loss: 0.9836 - val_acc: 0.8690\n",
            "Epoch 947/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5093 - acc: 0.5830 - val_loss: 0.9835 - val_acc: 0.8690\n",
            "Epoch 948/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5095 - acc: 0.5830 - val_loss: 0.9834 - val_acc: 0.8690\n",
            "Epoch 949/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5102 - acc: 0.5830 - val_loss: 0.9833 - val_acc: 0.8690\n",
            "Epoch 950/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5098 - acc: 0.5829 - val_loss: 0.9832 - val_acc: 0.8690\n",
            "Epoch 951/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5116 - acc: 0.5829 - val_loss: 0.9831 - val_acc: 0.8690\n",
            "Epoch 952/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5092 - acc: 0.5830 - val_loss: 0.9831 - val_acc: 0.8690\n",
            "Epoch 953/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5109 - acc: 0.5830 - val_loss: 0.9830 - val_acc: 0.8690\n",
            "Epoch 954/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5097 - acc: 0.5828 - val_loss: 0.9829 - val_acc: 0.8690\n",
            "Epoch 955/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5119 - acc: 0.5828 - val_loss: 0.9828 - val_acc: 0.8690\n",
            "Epoch 956/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5090 - acc: 0.5830 - val_loss: 0.9827 - val_acc: 0.8690\n",
            "Epoch 957/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5101 - acc: 0.5829 - val_loss: 0.9826 - val_acc: 0.8690\n",
            "Epoch 958/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5089 - acc: 0.5830 - val_loss: 0.9825 - val_acc: 0.8690\n",
            "Epoch 959/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5088 - acc: 0.5830 - val_loss: 0.9824 - val_acc: 0.8690\n",
            "Epoch 960/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5139 - acc: 0.5828 - val_loss: 0.9823 - val_acc: 0.8690\n",
            "Epoch 961/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5103 - acc: 0.5828 - val_loss: 0.9822 - val_acc: 0.8690\n",
            "Epoch 962/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5090 - acc: 0.5830 - val_loss: 0.9821 - val_acc: 0.8690\n",
            "Epoch 963/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5108 - acc: 0.5828 - val_loss: 0.9820 - val_acc: 0.8690\n",
            "Epoch 964/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5098 - acc: 0.5830 - val_loss: 0.9819 - val_acc: 0.8690\n",
            "Epoch 965/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5085 - acc: 0.5830 - val_loss: 0.9818 - val_acc: 0.8690\n",
            "Epoch 966/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5122 - acc: 0.5829 - val_loss: 0.9817 - val_acc: 0.8690\n",
            "Epoch 967/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5083 - acc: 0.5830 - val_loss: 0.9816 - val_acc: 0.8690\n",
            "Epoch 968/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5091 - acc: 0.5829 - val_loss: 0.9815 - val_acc: 0.8690\n",
            "Epoch 969/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5083 - acc: 0.5830 - val_loss: 0.9814 - val_acc: 0.8690\n",
            "Epoch 970/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5084 - acc: 0.5830 - val_loss: 0.9813 - val_acc: 0.8690\n",
            "Epoch 971/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5083 - acc: 0.5830 - val_loss: 0.9812 - val_acc: 0.8690\n",
            "Epoch 972/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5082 - acc: 0.5829 - val_loss: 0.9811 - val_acc: 0.8690\n",
            "Epoch 973/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5085 - acc: 0.5830 - val_loss: 0.9810 - val_acc: 0.8690\n",
            "Epoch 974/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5081 - acc: 0.5830 - val_loss: 0.9809 - val_acc: 0.8690\n",
            "Epoch 975/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5084 - acc: 0.5830 - val_loss: 0.9809 - val_acc: 0.8690\n",
            "Epoch 976/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5080 - acc: 0.5830 - val_loss: 0.9808 - val_acc: 0.8690\n",
            "Epoch 977/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5099 - acc: 0.5829 - val_loss: 0.9807 - val_acc: 0.8690\n",
            "Epoch 978/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5087 - acc: 0.5830 - val_loss: 0.9806 - val_acc: 0.8690\n",
            "Epoch 979/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5084 - acc: 0.5830 - val_loss: 0.9805 - val_acc: 0.8690\n",
            "Epoch 980/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5078 - acc: 0.5830 - val_loss: 0.9804 - val_acc: 0.8690\n",
            "Epoch 981/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5088 - acc: 0.5830 - val_loss: 0.9803 - val_acc: 0.8690\n",
            "Epoch 982/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5089 - acc: 0.5829 - val_loss: 0.9802 - val_acc: 0.8690\n",
            "Epoch 983/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5077 - acc: 0.5830 - val_loss: 0.9801 - val_acc: 0.8690\n",
            "Epoch 984/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5095 - acc: 0.5830 - val_loss: 0.9800 - val_acc: 0.8690\n",
            "Epoch 985/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5077 - acc: 0.5829 - val_loss: 0.9799 - val_acc: 0.8690\n",
            "Epoch 986/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5081 - acc: 0.5829 - val_loss: 0.9798 - val_acc: 0.8690\n",
            "Epoch 987/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5075 - acc: 0.5830 - val_loss: 0.9797 - val_acc: 0.8690\n",
            "Epoch 988/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5074 - acc: 0.5830 - val_loss: 0.9796 - val_acc: 0.8690\n",
            "Epoch 989/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5073 - acc: 0.5830 - val_loss: 0.9795 - val_acc: 0.8690\n",
            "Epoch 990/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5075 - acc: 0.5829 - val_loss: 0.9795 - val_acc: 0.8690\n",
            "Epoch 991/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5076 - acc: 0.5829 - val_loss: 0.9794 - val_acc: 0.8690\n",
            "Epoch 992/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5079 - acc: 0.5829 - val_loss: 0.9793 - val_acc: 0.8690\n",
            "Epoch 993/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5086 - acc: 0.5830 - val_loss: 0.9792 - val_acc: 0.8690\n",
            "Epoch 994/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5072 - acc: 0.5830 - val_loss: 0.9791 - val_acc: 0.8690\n",
            "Epoch 995/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5080 - acc: 0.5830 - val_loss: 0.9790 - val_acc: 0.8690\n",
            "Epoch 996/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5071 - acc: 0.5829 - val_loss: 0.9789 - val_acc: 0.8690\n",
            "Epoch 997/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5102 - acc: 0.5829 - val_loss: 0.9788 - val_acc: 0.8690\n",
            "Epoch 998/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5070 - acc: 0.5830 - val_loss: 0.9787 - val_acc: 0.8690\n",
            "Epoch 999/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5070 - acc: 0.5829 - val_loss: 0.9786 - val_acc: 0.8690\n",
            "Epoch 1000/1000\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.5114 - acc: 0.5829 - val_loss: 0.9785 - val_acc: 0.8690\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwUbytqeUCd7"
      },
      "source": [
        "# Plotting Final Loss After Training 3rd Batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "L0_1WKQEUDQo",
        "outputId": "c073e59c-eb9d-4b9f-a6ef-497391f848e9"
      },
      "source": [
        "import keras\n",
        "from matplotlib import pyplot as plt\n",
        "# history = model1.fit(train_x, train_y,validation_split = 0.1, epochs=50, batch_size=4)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fe163503510>]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0HoTAmnfcNM"
      },
      "source": [
        "# Saving Final Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJb43yM5fgKj"
      },
      "source": [
        "model.save(\"/content/drive/MyDrive/Project GCN Dataset/Final_Model_IDS_8690.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iQ6R0NsiXa9"
      },
      "source": [
        "# Test Accuracy Check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2IM44SniWD7",
        "outputId": "74882480-8bca-4fc3-dc8e-8db1aa5ff778"
      },
      "source": [
        "tsfeatures = Node_Features(A5) \n",
        "print(\"Shape of test features: \", np.shape(tsfeatures))\n",
        "score_test = model.test_on_batch([tsfeatures, A5], test_enc)\n",
        "print('Test accuracy:', score_test[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of test features:  (298, 8)\n",
            "Test accuracy: 0.8489933013916016\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QU4i6oJewzyS"
      },
      "source": [
        "# Prediction Starts Here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ntC7778B2Ar",
        "outputId": "ff8ec22f-9c12-4a02-f0d5-dac7fd196813"
      },
      "source": [
        "!apt-get install poppler-utils \n",
        "!pip install pdf2image"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 37 not upgraded.\n",
            "Need to get 154 kB of archives.\n",
            "After this operation, 613 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 poppler-utils amd64 0.62.0-2ubuntu2.12 [154 kB]\n",
            "Fetched 154 kB in 1s (116 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 155222 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_0.62.0-2ubuntu2.12_amd64.deb ...\n",
            "Unpacking poppler-utils (0.62.0-2ubuntu2.12) ...\n",
            "Setting up poppler-utils (0.62.0-2ubuntu2.12) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Collecting pdf2image\n",
            "  Downloading pdf2image-1.16.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from pdf2image) (7.1.2)\n",
            "Installing collected packages: pdf2image\n",
            "Successfully installed pdf2image-1.16.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-90za9Fw23Q",
        "outputId": "be21761d-d93a-45e7-bf00-7ad1422ff74f"
      },
      "source": [
        "import os\n",
        "from pdf2image import convert_from_path\n",
        "from PIL import Image\n",
        "\n",
        "test_invoices = \"/content/drive/MyDrive/Project GCN Dataset/Test Invoices Folder/PDF Invoices/\"\n",
        "im_path = \"/content/drive/MyDrive/Project GCN Dataset/Test Invoices Folder/Image Files/\"\n",
        "files = os.listdir(test_invoices)\n",
        "print(\"Files found are: \", files)\n",
        "\n",
        "def pdf_toImage(f):\n",
        "    file_name = f\n",
        "    # print(\"File processed is: \", file_name)\n",
        "    if(file_name.endswith('.pdf')):\n",
        "        # print(\"PDF File Found..\")\n",
        "        # print(\"Converting\", file_name.split('/')[-1], \"to Image..\")\n",
        "        pages = convert_from_path(file_name)\n",
        "\n",
        "        for i in range(len(pages)):\n",
        "            pages[i].save(im_path+(file_name.split('/')[-1]).split('.')\n",
        "                          [0]+' page' + str(i) + '.png', 'PNG')\n",
        "            #converting to grey-scale\n",
        "            img = Image.open(\"/content/drive/MyDrive/Project GCN Dataset/Test Invoices Folder/Image Files/\"+file_name.split('/')[-1].split('.')[0]+' page' + str(i) + '.png').convert('LA')\n",
        "            img.save(\"/content/drive/MyDrive/Project GCN Dataset/Test Invoices Folder/Image Files/\"+file_name.split('/')[-1].split('.')[0]+' page' + str(i) + '.png')\n",
        "            img = img.resize((1700,1800),Image.ANTIALIAS)\n",
        "            img.save(\"/content/drive/MyDrive/Project GCN Dataset/Test Invoices Folder/Image Files/\"+file_name.split('/')[-1].split('.')[0]+' page' + str(i) + '.png')\n",
        "    else:\n",
        "        print(\"Image File Found..\")\n",
        "\n",
        "pdf_toImage(test_invoices+files[0])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files found are:  ['Faber 1007843134 (1).pdf', 'pdf_1_AWFA02-user3_5fd169861ba88.pdf', '1007871548 (1).pdf', 'PO 1007880806 (1).pdf']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojEZHP440-z_"
      },
      "source": [
        "# Installing Keras-OCR "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSh7qVYL1DH-",
        "outputId": "d910f5be-270b-4c23-f621-83f791fa5d69"
      },
      "source": [
        "!pip install keras-ocr"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-ocr\n",
            "  Downloading keras_ocr-0.8.8-py3-none-any.whl (41 kB)\n",
            "\u001b[?25l\r\u001b[K     |                        | 10 kB 17.3 MB/s eta 0:00:01\r\u001b[K     |                | 20 kB 16.5 MB/s eta 0:00:01\r\u001b[K     |        | 30 kB 13.3 MB/s eta 0:00:01\r\u001b[K     || 40 kB 10.0 MB/s eta 0:00:01\r\u001b[K     || 41 kB 289 kB/s \n",
            "\u001b[?25hCollecting essential_generators\n",
            "  Downloading essential_generators-1.0-py3-none-any.whl (9.5 MB)\n",
            "\u001b[K     || 9.5 MB 9.0 MB/s \n",
            "\u001b[?25hCollecting efficientnet==1.0.0\n",
            "  Downloading efficientnet-1.0.0-py3-none-any.whl (17 kB)\n",
            "Collecting fonttools\n",
            "  Downloading fonttools-4.28.1-py3-none-any.whl (873 kB)\n",
            "\u001b[K     || 873 kB 41.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from keras-ocr) (4.62.3)\n",
            "Requirement already satisfied: shapely in /usr/local/lib/python3.7/dist-packages (from keras-ocr) (1.8.0)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.7/dist-packages (from keras-ocr) (0.5.3)\n",
            "Requirement already satisfied: imgaug in /usr/local/lib/python3.7/dist-packages (from keras-ocr) (0.2.9)\n",
            "Collecting validators\n",
            "  Downloading validators-0.18.2-py3-none-any.whl (19 kB)\n",
            "Collecting pyclipper\n",
            "  Downloading pyclipper-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (123 kB)\n",
            "\u001b[K     || 123 kB 51.8 MB/s \n",
            "\u001b[?25hCollecting keras-applications<=1.0.8,>=1.0.7\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     || 50 kB 6.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (from efficientnet==1.0.0->keras-ocr) (0.16.2)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet==1.0.0->keras-ocr) (1.19.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet==1.0.0->keras-ocr) (3.1.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications<=1.0.8,>=1.0.7->efficientnet==1.0.0->keras-ocr) (1.5.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from imgaug->keras-ocr) (1.15.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from imgaug->keras-ocr) (3.2.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from imgaug->keras-ocr) (4.1.2.30)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from imgaug->keras-ocr) (1.4.1)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from imgaug->keras-ocr) (2.4.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from imgaug->keras-ocr) (7.1.2)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.0.0->keras-ocr) (1.2.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image->efficientnet==1.0.0->keras-ocr) (2.6.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug->keras-ocr) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug->keras-ocr) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug->keras-ocr) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug->keras-ocr) (2.4.7)\n",
            "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from validators->keras-ocr) (4.4.2)\n",
            "Installing collected packages: keras-applications, validators, pyclipper, fonttools, essential-generators, efficientnet, keras-ocr\n",
            "Successfully installed efficientnet-1.0.0 essential-generators-1.0 fonttools-4.28.1 keras-applications-1.0.8 keras-ocr-0.8.8 pyclipper-1.3.0 validators-0.18.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "id": "r3b1LbCC1YZf",
        "outputId": "f2f1d766-91ea-45e0-ebb3-aabe7ad02439"
      },
      "source": [
        "import keras_ocr\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "pipeline = keras_ocr.pipeline.Pipeline()\n",
        "im_path = \"/content/drive/MyDrive/Project GCN Dataset/Test Invoices Folder/Image Files/\"\n",
        "\n",
        "im_files_list = os.listdir(im_path)\n",
        "img = mpimg.imread('/content/drive/MyDrive/Project GCN Dataset/Test Invoices Folder/Image Files/'+im_files_list[0])\n",
        "imgplot = plt.imshow(img)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking for /root/.keras-ocr/craft_mlt_25k.h5\n",
            "Downloading /root/.keras-ocr/craft_mlt_25k.h5\n",
            "Looking for /root/.keras-ocr/crnn_kurapan.h5\n",
            "Downloading /root/.keras-ocr/crnn_kurapan.h5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVoecC0JJda4"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "def tuples_toCSV(csv_dir, file_name, b_prediction):\n",
        "  Object = []\n",
        "  xmin = []\n",
        "  xmax = []\n",
        "  ymin = []\n",
        "  ymax = []\n",
        "  dataframes = []\n",
        "\n",
        "  for i in range(len(b_prediction)):\n",
        "    df = pd.DataFrame()\n",
        "    SmallList_OBJECT = []\n",
        "    SmallList_XMIN = []\n",
        "    SmallList_XMAX = []\n",
        "    SmallList_YMIN = []\n",
        "    SmallList_YMAX = []\n",
        "    FILE = b_prediction[i]\n",
        "    for j in range(len(FILE)):\n",
        "      SmallList_OBJECT.append(b_prediction[i][j][0])\n",
        "      SmallList_XMIN.append(b_prediction[i][j][1][1][0])\n",
        "      SmallList_YMIN.append(b_prediction[i][j][1][1][1])\n",
        "      SmallList_XMAX.append(b_prediction[i][j][1][2][0])\n",
        "      SmallList_YMAX.append(b_prediction[i][j][1][2][1])\n",
        "    \n",
        "    Object.append(SmallList_OBJECT)\n",
        "    xmin.append(SmallList_XMIN)\n",
        "    xmax.append(SmallList_XMAX)\n",
        "    ymin.append(SmallList_YMIN)\n",
        "    ymax.append(SmallList_YMAX)\n",
        "\n",
        "    df['xmin'] = SmallList_XMIN\n",
        "    df['xmax'] = SmallList_XMAX\n",
        "    df['ymin'] = SmallList_YMIN\n",
        "    df['ymax'] = SmallList_YMAX\n",
        "    df['Object'] = SmallList_OBJECT\n",
        "    dataframes.append(df)\n",
        "\n",
        "  return dataframes\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9gj_NOY3cXv",
        "outputId": "c0ce70f7-9cb2-4362-eb49-2b83da26eb79"
      },
      "source": [
        "import numpy as np\n",
        "image_dir = im_path\n",
        "\n",
        "csv_dir = \"/content/drive/MyDrive/Project GCN Dataset/Test Invoices Folder/CSVs\"\n",
        "\n",
        "def batchData_Tuple(ImagePath):\n",
        "  f_names = []\n",
        "  custom_images = []\n",
        "  for filename in os.listdir(ImagePath):\n",
        "      # print(os.path.join(ImagePath, filename))\n",
        "      f_names.append(filename)\n",
        "      custom_images.append(os.path.join(ImagePath, filename))\n",
        "\n",
        "  images = [ keras_ocr.tools.read(path) for path in custom_images ]\n",
        "  t_images = len(images)\n",
        "  image_batches = []\n",
        "  batch_start = 0\n",
        "  batch_end = 3\n",
        "  while(batch_start < t_images):\n",
        "    batch = []\n",
        "    batch = images[batch_start:batch_end]\n",
        "    batch_start = batch_end\n",
        "    batch_end = batch_start + 3\n",
        "    image_batches.append(batch)\n",
        "  \n",
        "  batch = images[batch_start:]    \n",
        "  return f_names, image_batches\n",
        "\n",
        "file_names = batchData_Tuple(image_dir)[0]\n",
        "image_batches = batchData_Tuple(image_dir)[1]\n",
        "\n",
        "\n",
        "print(\"File names are: \", file_names)\n",
        "t_batches = len(image_batches[1])\n",
        "print(\"Total batches are: \", t_batches)\n",
        "\n",
        "def batch_toDataframe(batches_array):\n",
        "  dataframes = []\n",
        "  for i in range(len(batches_array)):\n",
        "    im_batch = batches_array[i]\n",
        "    prediction = pipeline.recognize(im_batch)\n",
        "    df_batch = tuples_toCSV(csv_dir, file_names[i], prediction)\n",
        "    dataframes.append(df_batch)\n",
        "  \n",
        "  return dataframes\n",
        "\n",
        "df_batches = batch_toDataframe(image_batches)\n",
        "print(len(df_batches))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File names are:  ['Faber 1007843134 (1) page0.png', 'Faber 1007843134 (1) page4.png', 'Faber 1007843134 (1) page1.png', 'Faber 1007843134 (1) page3.png', 'Faber 1007843134 (1) page2.png', 'Faber 1007843134 (1) page6.png', 'Faber 1007843134 (1) page5.png']\n",
            "Total batches are:  3\n",
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZWaMQt-d4Lm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "296d0e14-91d7-4bbe-dfd2-b82e9a4e591d"
      },
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def df_AdjMatrix(df_array):\n",
        "  matrices  = []\n",
        "  for i in range(len(df_array)):\n",
        "    for j in range(len(df_array[i])):\n",
        "      G = makeGraph(df_array[i][j])\n",
        "      M = nx.to_numpy_array(G, dtype=np.int32)\n",
        "      matrices.append(M)\n",
        "      \n",
        "  return matrices\n",
        "\n",
        "matrices = df_AdjMatrix(df_batches)\n",
        "print(\"Total matrices are: \", len(matrices))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total matrices are:  7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRUt1KtIrUVK",
        "outputId": "9bc815d1-58e7-44e3-f83b-ba2b7ce7215f"
      },
      "source": [
        "mat_nodeFeatures = []\n",
        "for i in range(len(matrices)):\n",
        "  F = Node_Features(matrices[i])\n",
        "  mat_nodeFeatures.append(F)\n",
        "\n",
        "for j in range(len(mat_nodeFeatures)):\n",
        "  print(mat_nodeFeatures[j])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.41387184 1.33379805 ... 1.92934057 0.         2.8579611 ]\n",
            " [0.         0.49174509 1.58476268 ... 2.29236123 0.         3.39570904]\n",
            " [0.         0.42210087 1.36031801 ... 1.96770172 0.         2.91478606]\n",
            " ...\n",
            " [0.         0.74787367 2.41019647 ... 3.48635225 0.         5.16438584]\n",
            " [0.         0.53585512 1.72691747 ... 2.49798831 0.         3.70030753]\n",
            " [0.         0.27446238 0.88451871 ... 1.27945743 0.         1.89527947]]\n",
            "[[0.         1.96544609 0.         ... 0.         0.         1.11779619]\n",
            " [0.         3.43112105 0.         ... 0.         0.         1.95136059]\n",
            " [0.         2.34267719 0.         ... 0.         0.         1.33233654]\n",
            " ...\n",
            " [0.         4.68054889 0.         ... 0.         0.         2.66194008]\n",
            " [0.         4.45949627 0.         ... 0.         0.         2.53622217]\n",
            " [0.         3.48878695 0.         ... 0.         0.         1.98415657]]\n",
            "[[0.         0.0135685  0.         ... 1.32232663 1.32538511 1.32525516]\n",
            " [0.         0.01604533 0.         ... 1.56370779 1.56732457 1.5671709 ]\n",
            " [0.         0.01327571 0.         ... 1.29379275 1.29678523 1.29665809]\n",
            " ...\n",
            " [0.         0.02732692 0.         ... 2.66316172 2.66932148 2.66905977]\n",
            " [0.         0.02619445 0.         ... 2.55279646 2.55870095 2.55845008]\n",
            " [0.         0.0203844  0.         ... 1.98657469 1.99116954 1.99097431]]\n",
            "[[0.         0.48008993 0.45355512 ... 0.         0.         0.35293543]\n",
            " [0.         0.64723236 0.6114595  ... 0.         0.         0.47580925]\n",
            " [0.         0.50940708 0.48125189 ... 0.         0.         0.37448777]\n",
            " ...\n",
            " [0.         0.89798936 0.84835703 ... 0.         0.         0.66015185]\n",
            " [0.         0.7704858  0.72790066 ... 0.         0.         0.56641833]\n",
            " [0.         0.5136572  0.48526711 ... 0.         0.         0.37761222]]\n",
            "[[0.         0.         0.         ... 0.         0.         0.55007519]\n",
            " [0.         0.         0.         ... 0.         0.         0.95158992]\n",
            " [0.         0.         0.         ... 0.         0.         0.57115471]\n",
            " ...\n",
            " [0.         0.         0.         ... 0.         0.         1.27146332]\n",
            " [0.         0.         0.         ... 0.         0.         1.12156449]\n",
            " [0.         0.         0.         ... 0.         0.         0.78295373]]\n",
            "[[0.76731404 0.         0.         ... 1.33806528 0.         0.        ]\n",
            " [0.61970331 0.         0.         ... 1.08065726 0.         0.        ]\n",
            " [0.53986825 0.         0.         ... 0.94143848 0.         0.        ]\n",
            " ...\n",
            " [1.13666806 0.         0.         ... 1.98215592 0.         0.        ]\n",
            " [1.09544175 0.         0.         ... 1.91026425 0.         0.        ]\n",
            " [0.84808394 0.         0.         ... 1.47891426 0.         0.        ]]\n",
            "[[3.18473676 0.         0.4776382  ... 0.70641142 0.24727719 0.61263904]\n",
            " [4.02282538 0.         0.60333247 ... 0.89230916 0.31235014 0.77385985]\n",
            " [3.1379972  0.         0.47062833 ... 0.69604405 0.24364813 0.60364788]\n",
            " ...\n",
            " [6.25343048 0.         0.93787259 ... 1.38708315 0.48554429 1.20295521]\n",
            " [5.98266338 0.         0.89726367 ... 1.32702388 0.46452072 1.15086849]\n",
            " [4.6610621  0.         0.69905349 ... 1.03387744 0.36190569 0.89663569]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yx7wbRoUFv7b"
      },
      "source": [
        "# Model evaluation\n",
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_R_Tm0f0WVH8",
        "outputId": "c2afeb79-08b8-4b64-dfd2-7f7e924cab35"
      },
      "source": [
        "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
        "print(np.shape(mat_nodeFeatures[0]))\n",
        "print(np.shape(matrices[0])) \n",
        "\n",
        "x = [mat_nodeFeatures[0], matrices[0]]\n",
        "\n",
        "l = model.predict_on_batch(x)\n",
        "print(l[0])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(267, 8)\n",
            "(267, 267)\n",
            "[0.00277735 0.00208281 0.0711107  0.00392426 0.24160786 0.07019469\n",
            " 0.01380891 0.0064751  0.01624123 0.0200671  0.0145571  0.5371529 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asGUZWRTgiNk"
      },
      "source": [
        "# Test Accuracy Check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWsYMvt3gn8k"
      },
      "source": [
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "def predict(self):\n",
        "        \"\"\"\n",
        "        Predicts the model output, and computes precision, recall, and F1 score.\n",
        "        INPUT\n",
        "            model: Model trained in Keras\n",
        "        OUTPUT\n",
        "            Precision, Recall, and F1 score\n",
        "        \"\"\"\n",
        "        predictions = model.predict(self.X_test)\n",
        "        predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "        # predictions[predictions >=1] = 1\n",
        "        # Remove when non binary classifier\n",
        "\n",
        "        y_test = np.argmax(y_test, axis=1)\n",
        "\n",
        "        precision = precision_score(self.y_test, predictions, average=\"micro\")\n",
        "        recall = recall_score(self.y_test, predictions, average=\"micro\")\n",
        "        f1 = f1_score(self.y_test, predictions, average=\"micro\")\n",
        "        cohen_kappa = cohen_kappa_score(self.y_test, predictions)\n",
        "        return precision, recall, f1, cohen_kappa, 'No Cuda Training'"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}