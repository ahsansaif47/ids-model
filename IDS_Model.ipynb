{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data-Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Label files are:  110\n",
      "Training Batch-1 files are:  30\n",
      "Training Batch-2 files are:  30\n",
      "Training Batch-3 files are:  30\n",
      "Validation files are:  5\n",
      "Testing files are:  5\n"
     ]
    }
   ],
   "source": [
    "# Dataset loading\n",
    "import os\n",
    "\n",
    "dataset = \"./New-Dataset/\"\n",
    "# dataset = \"./Dataset (Labelled Images)/\"\n",
    "lab_files_path = dataset+\"labels/\"\n",
    "# lab_files_path = dataset+\"label/\"\n",
    "\n",
    "# total invoice files\n",
    "files = os.listdir(lab_files_path)\n",
    "\n",
    "\n",
    "# training set\n",
    "Batch_1 = files[:30]\n",
    "\n",
    "Batch_2 = files[30:60]\n",
    "\n",
    "Batch_3 = files[60:90]\n",
    "\n",
    "# validation set\n",
    "valid_set = files[90:95]\n",
    "\n",
    "# test set\n",
    "test = files[95:100]\n",
    "\n",
    "print(\"Total Label files are: \", len(files))\n",
    "print(\"Training Batch-1 files are: \", len(Batch_1))\n",
    "print(\"Training Batch-2 files are: \", len(Batch_2))\n",
    "print(\"Training Batch-3 files are: \", len(Batch_3))\n",
    "print(\"Validation files are: \", len(valid_set))\n",
    "print(\"Testing files are: \", len(test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geometric Algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL.Image import Image\n",
    "from networkx.algorithms.shortest_paths.weighted import single_source_dijkstra\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.core.frame import DataFrame\n",
    "from PIL import Image\n",
    "import networkx as nx\n",
    "\n",
    "df = 0\n",
    "xMIN, xMAX = [], []\n",
    "yMIN, yMAX = [], []\n",
    "Text = []\n",
    "\n",
    "\n",
    "def findRight(df, df_ind, xMIN, xMAX, yMIN, yMAX):\n",
    "    S_list = []\n",
    "    xmax = xMAX[df_ind]\n",
    "    ymin = yMIN[df_ind]\n",
    "    ymax = yMAX[df_ind]\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        if(xMIN[i] > xmax):\n",
    "            if not (yMIN[i] > ymax or yMAX[i] < ymin):\n",
    "                if(yMIN[i] <= ymin and yMAX[i] <= ymax):\n",
    "                    S_list.append(i)\n",
    "                elif (yMIN[i] <= ymin and yMAX[i] >= ymax):\n",
    "                    S_list.append(i)\n",
    "                elif (yMIN[i] >= ymin and yMAX[i] <= ymax):\n",
    "                    S_list.append(i)\n",
    "                elif (yMIN[i] >= ymin and yMAX[i] >= ymax):\n",
    "                    S_list.append(i)\n",
    "                elif (yMIN[i] == ymin and yMAX[i] == ymax):\n",
    "                    S_list.append(i)\n",
    "\n",
    "    if S_list:\n",
    "        consec = S_list[0]\n",
    "        for j in S_list:\n",
    "            if(xMIN[consec] > xMIN[j]):\n",
    "                consec = j\n",
    "        return consec\n",
    "\n",
    "    return -1\n",
    "\n",
    "\n",
    "def findLeft(df, df_ind, xMIN, xMAX, yMIN, yMAX):\n",
    "    S_list = []\n",
    "    xmin = xMIN[df_ind]\n",
    "    ymin = yMIN[df_ind]\n",
    "    ymax = yMAX[df_ind]\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        if(xMAX[i] < xmin):\n",
    "            if not (yMIN[i] > ymax or yMAX[i] < ymin):\n",
    "                if(yMIN[i] <= ymin and yMAX[i] <= ymax):\n",
    "                    S_list.append(i)\n",
    "                elif (yMIN[i] <= ymin and yMAX[i] >= ymax):\n",
    "                    S_list.append(i)\n",
    "                elif (yMIN[i] >= ymin and yMAX[i] <= ymax):\n",
    "                    S_list.append(i)\n",
    "                elif (yMIN[i] >= ymin and yMAX[i] >= ymax):\n",
    "                    S_list.append(i)\n",
    "                elif (yMIN[i] == ymin and yMAX[i] == ymax):\n",
    "                    S_list.append(i)\n",
    "\n",
    "    if S_list:\n",
    "        consec = S_list[0]\n",
    "        for j in S_list:\n",
    "            if(xMAX[j] > xMAX[consec]):\n",
    "                consec = j\n",
    "        return consec\n",
    "    return -1\n",
    "\n",
    "\n",
    "def findUp(df, df_ind, xMIN, xMAX, yMIN, yMAX):\n",
    "    S_list = []\n",
    "    xmin = xMIN[df_ind]\n",
    "    xmax = xMAX[df_ind]\n",
    "    ymin = yMIN[df_ind]\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        if(yMAX[i] < ymin):\n",
    "            if not (xMAX[i] < xmin or xMIN[i] > xmax):\n",
    "                if(xMIN[i] <= xmin and xMAX[i] <= xmax):\n",
    "                    S_list.append(i)\n",
    "                elif (xMIN[i] <= xmin and xMAX[i] >= xmax):\n",
    "                    S_list.append(i)\n",
    "                elif (xMIN[i] >= xmin and xMAX[i] <= xmax):\n",
    "                    S_list.append(i)\n",
    "                elif (xMIN[i] >= xmin and xMAX[i] >= xmax):\n",
    "                    S_list.append(i)\n",
    "                elif (xMIN[i] == xmin and xMAX[i] == xmax):\n",
    "                    S_list.append(i)\n",
    "\n",
    "    if S_list:\n",
    "        consec = S_list[0]\n",
    "        for j in S_list:\n",
    "            if(yMAX[j] > yMAX[consec]):\n",
    "                consec = j\n",
    "        return consec\n",
    "\n",
    "    return -1\n",
    "\n",
    "\n",
    "def findDown(df, df_ind, xMIN, xMAX, yMIN, yMAX):\n",
    "    S_list = []\n",
    "    xmin = xMIN[df_ind]\n",
    "    xmax = xMAX[df_ind]\n",
    "    ymax = yMAX[df_ind]\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        if(yMIN[i] > ymax):\n",
    "            if not (xMAX[i] < xmin or xMIN[i] > xmax):\n",
    "                if(xMIN[i] <= xmin and xMAX[i] <= xmax):\n",
    "                    S_list.append(i)\n",
    "                elif (xMIN[i] <= xmin and xMAX[i] >= xmax):\n",
    "                    S_list.append(i)\n",
    "                elif (xMIN[i] >= xmin and xMAX[i] <= xmax):\n",
    "                    S_list.append(i)\n",
    "                elif (xMIN[i] >= xmin and xMAX[i] >= xmax):\n",
    "                    S_list.append(i)\n",
    "                elif (xMIN[i] == xmin and xMAX[i] == xmax):\n",
    "                    S_list.append(i)\n",
    "\n",
    "    if S_list:\n",
    "        consec = S_list[0]\n",
    "        for j in S_list:\n",
    "            if(yMIN[j] < yMIN[consec]):\n",
    "                consec = j\n",
    "        return consec\n",
    "\n",
    "    return -1\n",
    "\n",
    "\n",
    "def makeGraph(df):\n",
    "    G = nx.Graph()\n",
    "    xMIN = df['xmin']\n",
    "    xMAX = df['xmax']\n",
    "    yMIN = df['ymin']\n",
    "    yMAX = df['ymax']\n",
    "    Text = df['Object']\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        if findUp(df, i, xMIN, xMAX, yMIN, yMAX):\n",
    "            l = findUp(df, i, xMIN, xMAX, yMIN, yMAX)\n",
    "            if(l != -1):\n",
    "                text = Text[l]\n",
    "                G.add_edge(Text[i], text)\n",
    "        if findRight(df, i, xMIN, xMAX, yMIN, yMAX):\n",
    "            l = findRight(df, i, xMIN, xMAX, yMIN, yMAX)\n",
    "            if (l != -1):\n",
    "                text = Text[l]\n",
    "                G.add_edge(Text[i], text)\n",
    "        if findDown(df, i, xMIN, xMAX, yMIN, yMAX):\n",
    "            l = findDown(df, i, xMIN, xMAX, yMIN, yMAX)\n",
    "            if (l != -1):\n",
    "                text = Text[l]\n",
    "                G.add_edge(Text[i], text)\n",
    "        if findLeft(df, i, xMIN, xMAX, yMIN, yMAX):\n",
    "            l = findLeft(df, i, xMIN, xMAX, yMIN, yMAX)\n",
    "            if (l != -1):\n",
    "                text = Text[l]\n",
    "                G.add_edge(Text[i], text)\n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrices Over Diagonal\n",
    "Place an incident Matrix over diagonal with existing matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import SupportsAbs\n",
    "from numpy.core.fromnumeric import shape\n",
    "import numpy as np\n",
    "\n",
    "# Test Matrices\n",
    "# mat1 = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "# mat2 = np.array([[3, 2, 1, 7], [6, 5, 4, 9], [9, 8, 7, 4], [1, 5, 7, 2]])\n",
    "\n",
    "\n",
    "def alignDiagonally(M1, M2, prev_Len):\n",
    "    for i in range(prev_Len, np.shape(M1)[0]):\n",
    "        for j in range(prev_Len, np.shape(M1)[0]):\n",
    "            x = i - prev_Len\n",
    "            y = j - prev_Len\n",
    "            M1[i][j] = M2[x][y]\n",
    "\n",
    "    return M1\n",
    "\n",
    "\n",
    "def resizeMatrix(M, I):\n",
    "    oldMat_Len = np.shape(M)[0]\n",
    "    z = np.zeros((oldMat_Len, np.shape(I)[0]), dtype=np.int64)\n",
    "    newArray = np.append(M, z, axis=1)\n",
    "    M = newArray\n",
    "\n",
    "    # Appending 1D arrays of zeros in the original Matrix\n",
    "    # (i.e. the matrix in which we want to align othe rmatrices diagonally)\n",
    "    appZero = np.shape(I)[0]\n",
    "    x = oldMat_Len + appZero\n",
    "    L = np.zeros((np.shape(I)[0], x), dtype=np.int64)\n",
    "    newArray = np.append(M, L, axis=0)\n",
    "    M = newArray\n",
    "\n",
    "    M = alignDiagonally(M, I, oldMat_Len)\n",
    "    return M\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Batch-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  1\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  2\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  3\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  4\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  5\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  6\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  7\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  8\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  9\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  10\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  11\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  12\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  13\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  14\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  15\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  16\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  17\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  18\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  19\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  20\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  21\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  22\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  23\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  24\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  25\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  26\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  27\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  28\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  29\n",
      "Dimentions of Batch 1 matrix is:  (3718, 3718)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from numpy.core.fromnumeric import shape\n",
    "\n",
    "Z_file = lab_files_path + Batch_1[0]\n",
    "df = pd.read_csv(Z_file)\n",
    "G = makeGraph(df)\n",
    "M1 = nx.to_numpy_array(G, dtype=np.int32)\n",
    "\n",
    "for i in range(1, len(Batch_1)):\n",
    "    print(\"\\n= = = = = = = = = = = = = = = =\")\n",
    "    print(\"Iteration No.: \", i)\n",
    "    # Getting file\n",
    "    f = lab_files_path + Batch_1[i]\n",
    "    # Making dataframe of file\n",
    "    df = pd.read_csv(f)\n",
    "    # Making graph of the dataframe.\n",
    "    G = makeGraph(df)\n",
    "    # Storing the graph as an incident matrix(an adjacency matrix)\n",
    "    I = nx.to_numpy_array(G, dtype=np.int32)\n",
    "    # Now resizing the original sparce matrix with the new incident matrix\n",
    "    M1 = resizeMatrix(M1, I)\n",
    "\n",
    "print(\"Dimentions of Batch 1 matrix is: \", np.shape(M1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Batch-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  1\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  2\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  3\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  4\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  5\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  6\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  7\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  8\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  9\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  10\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  11\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  12\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  13\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  14\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  15\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  16\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  17\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  18\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  19\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  20\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  21\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  22\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  23\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  24\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  25\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  26\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  27\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  28\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  29\n",
      "Dimentions of Batch 2 matrix is:  (3680, 3680)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from numpy.core.fromnumeric import shape\n",
    "\n",
    "Z1_file = lab_files_path + Batch_2[0]\n",
    "df = pd.read_csv(Z1_file)\n",
    "G = makeGraph(df)\n",
    "M2 = nx.to_numpy_array(G, dtype=np.int32)\n",
    "\n",
    "for i in range(1, len(Batch_2)):\n",
    "    print(\"\\n= = = = = = = = = = = = = = = =\")\n",
    "    print(\"Iteration No.: \", i)\n",
    "    # Getting file\n",
    "    f = lab_files_path + Batch_2[i]\n",
    "    # Making dataframe of file\n",
    "    df = pd.read_csv(f)\n",
    "    # Making graph of the dataframe.\n",
    "    G = makeGraph(df)\n",
    "    # Storing the graph as an incident matrix(an adjacency matrix)\n",
    "    I = nx.to_numpy_array(G, dtype=np.int32)\n",
    "    # Now resizing the original sparce matrix with the new incident matrix\n",
    "    M2 = resizeMatrix(M2, I)\n",
    "\n",
    "print(\"Dimentions of Batch 2 matrix is: \", np.shape(M2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainig Batch-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  1\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  2\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  3\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  4\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  5\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  6\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  7\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  8\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  9\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  10\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  11\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  12\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  13\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  14\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  15\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  16\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  17\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  18\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  19\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  20\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  21\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  22\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  23\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  24\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  25\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  26\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  27\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  28\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  29\n",
      "Dimentions of Batch 3 matrix is:  (3960, 3960)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from numpy.core.fromnumeric import shape\n",
    "\n",
    "Z2_file = lab_files_path + Batch_3[0]\n",
    "df = pd.read_csv(Z2_file)\n",
    "G = makeGraph(df)\n",
    "M3 = nx.to_numpy_array(G, dtype=np.int32)\n",
    "\n",
    "for i in range(1, len(Batch_3)):\n",
    "    print(\"\\n= = = = = = = = = = = = = = = =\")\n",
    "    print(\"Iteration No.: \", i)\n",
    "    # Getting file\n",
    "    f = lab_files_path + Batch_3[i]\n",
    "    # Making dataframe of file\n",
    "    df = pd.read_csv(f)\n",
    "    # Making graph of the dataframe.\n",
    "    G = makeGraph(df)\n",
    "    # Storing the graph as an incident matrix(an adjacency matrix)\n",
    "    I = nx.to_numpy_array(G, dtype=np.int32)\n",
    "    # Now resizing the original sparce matrix with the new incident matrix\n",
    "    M3 = resizeMatrix(M3, I)\n",
    "\n",
    "print(\"Dimentions of Batch 3 matrix is: \", np.shape(M3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  1\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  2\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  3\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  4\n",
      "Dimentions of Validation matrix is:  (1052, 1052)\n"
     ]
    }
   ],
   "source": [
    "Z3_file = lab_files_path + valid_set[0]\n",
    "df = pd.read_csv(Z3_file)\n",
    "G = makeGraph(df)\n",
    "M4 = nx.to_numpy_array(G, dtype=np.int32)\n",
    "\n",
    "for i in range(1, len(valid_set)):\n",
    "    print(\"\\n= = = = = = = = = = = = = = = =\")\n",
    "    print(\"Iteration No.: \", i)\n",
    "    # Getting file\n",
    "    f = lab_files_path + valid_set[i]\n",
    "    # Making dataframe of file\n",
    "    df = pd.read_csv(f)\n",
    "    # Making graph of the dataframe.\n",
    "    G = makeGraph(df)\n",
    "    # Storing the graph as an incident matrix(an adjacency matrix)\n",
    "    I = nx.to_numpy_array(G, dtype=np.int32)\n",
    "    # Now resizing the original sparce matrix with the new incident matrix\n",
    "    M4 = resizeMatrix(M4, I)\n",
    "\n",
    "print(\"Dimentions of Validation matrix is: \", np.shape(M4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  1\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  2\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  3\n",
      "\n",
      "= = = = = = = = = = = = = = = =\n",
      "Iteration No.:  4\n",
      "Dimentions of Batch 3 matrix is:  (1012, 1012)\n"
     ]
    }
   ],
   "source": [
    "Z4_file = lab_files_path + test[0]\n",
    "df = pd.read_csv(Z4_file)\n",
    "G = makeGraph(df)\n",
    "M5 = nx.to_numpy_array(G, dtype=np.int32)\n",
    "\n",
    "for i in range(1, len(test)):\n",
    "    print(\"\\n= = = = = = = = = = = = = = = =\")\n",
    "    print(\"Iteration No.: \", i)\n",
    "    # Getting file\n",
    "    f = lab_files_path + test[i]\n",
    "    # Making dataframe of file\n",
    "    df = pd.read_csv(f)\n",
    "    # Making graph of the dataframe.\n",
    "    G = makeGraph(df)\n",
    "    # Storing the graph as an incident matrix(an adjacency matrix)\n",
    "    I = nx.to_numpy_array(G, dtype=np.int32)\n",
    "    # Now resizing the original sparce matrix with the new incident matrix\n",
    "    M5 = resizeMatrix(M5, I)\n",
    "\n",
    "print(\"Dimentions of Batch 3 matrix is: \", np.shape(M5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Matrix\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving Matrix\")\n",
    "# np.save(\"./Matrices/Matrix_b1.npy\", M1)\n",
    "np.save(\"./IDS Model File Matrices/Train_B1.npy\", M1)\n",
    "np.save(\"./IDS Model File Matrices/Train_B2.npy\", M2)\n",
    "np.save(\"./IDS Model File Matrices/Train_B3.npy\", M3)\n",
    "np.save(\"./IDS Model File Matrices/Validation.npy\", M4)\n",
    "np.save(\"./IDS Model File Matrices/Test.npy\", M5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Training Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing Training Batch_1\n",
      "[[1 1 0 ... 0 0 0]\n",
      " [1 0 1 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 1 0 1]\n",
      " [0 0 0 ... 0 1 0]]\n",
      "(3718, 3718)\n",
      "Printing Training Batch_2\n",
      "[[0 1 1 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 1 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "(3680, 3680)\n",
      "Printing Training Batch_3\n",
      "[[0 1 1 ... 0 0 0]\n",
      " [1 0 1 ... 0 0 0]\n",
      " [1 1 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 0 1 0]]\n",
      "(3960, 3960)\n",
      "Printing Validation Batch\n",
      "[[0 1 1 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "(1052, 1052)\n",
      "Printing Test Batch\n",
      "[[0 1 1 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " [1 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 0 1 0]]\n",
      "(1012, 1012)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A1 = np.load(\"./IDS Model File Matrices/Train_B1.npy\")\n",
    "A2 = np.load(\"./IDS Model File Matrices/Train_B2.npy\")\n",
    "A3 = np.load(\"./IDS Model File Matrices/Train_B3.npy\")\n",
    "A4 = np.load(\"./IDS Model File Matrices/Validation.npy\")\n",
    "A5 = np.load(\"./IDS Model File Matrices/Test.npy\")\n",
    "\n",
    "print(\"Printing Training Batch_1\")\n",
    "print(A1)\n",
    "print(np.shape(A1))\n",
    "print(\"Printing Training Batch_2\")\n",
    "print(A2)\n",
    "print(np.shape(A2))\n",
    "print(\"Printing Training Batch_3\")\n",
    "print(A3)\n",
    "print(np.shape(A3))\n",
    "print(\"Printing Validation Batch\")\n",
    "print(A4)\n",
    "print(np.shape(A4))\n",
    "print(\"Printing Test Batch\")\n",
    "print(A5)\n",
    "print(np.shape(A5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Train Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3718, 3718)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.matshow(A1)\n",
    "print(np.shape(A1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import nan\n",
    "\n",
    "\n",
    "def returnLabels(direc, files):\n",
    "    train_text = []\n",
    "    labels = []\n",
    "    for f in range(len(files)):\n",
    "        seenList = []\n",
    "        df = pd.read_csv(direc + files[f])\n",
    "        text = df['Object'].to_list()\n",
    "        T_labels = df['labels'].to_list()\n",
    "\n",
    "        for i in range(len(text)):\n",
    "            if(text[i] not in seenList):\n",
    "                labels.append(T_labels[i])\n",
    "                train_text.append(text[i])\n",
    "            seenList.append(text[i])\n",
    "\n",
    "    return labels, train_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding Labels\n",
    "1. Get a list of labels from CSV files.\n",
    "2. Catagorically encode the labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total train labels are:  3718\n",
      "Total train text is:  3718\n",
      "Total train encodings are:  3718\n",
      "Unique train encodings are:  {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n",
      "=============================================\n",
      "Total train labels are:  3680\n",
      "Total train text is:  3680\n",
      "Total train encodings are:  3680\n",
      "Unique train encodings are:  {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n",
      "=============================================\n",
      "Total train labels are:  3960\n",
      "Total train text is:  3960\n",
      "Total train encodings are:  3960\n",
      "Unique train encodings are:  {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n",
      "=============================================\n",
      "Total validation labels are:  1052\n",
      "Total train encodings are:  1052\n",
      "Unique validation encodings are:  {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}\n",
      "=============================================\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "b1_labels, b1_text = returnLabels(lab_files_path, Batch_1)\n",
    "print(\"Total train labels are: \", len(b1_labels))\n",
    "print(\"Total train text is: \", len(b1_text))\n",
    "\n",
    "lab_encoder = LabelEncoder()\n",
    "b1_encodings = lab_encoder.fit_transform(b1_labels)\n",
    "print(\"Total train encodings are: \", len(b1_encodings))\n",
    "\n",
    "\n",
    "uni_b1_enc = set(b1_encodings)\n",
    "print(\"Unique train encodings are: \", uni_b1_enc)\n",
    "print(\"=============================================\")\n",
    "\n",
    "b2_labels, b2_text = returnLabels(lab_files_path, Batch_2)\n",
    "print(\"Total train labels are: \", len(b2_labels))\n",
    "print(\"Total train text is: \", len(b2_text))\n",
    "\n",
    "lab_encoder = LabelEncoder()\n",
    "b2_encodings = lab_encoder.fit_transform(b2_labels)\n",
    "print(\"Total train encodings are: \", len(b2_encodings))\n",
    "\n",
    "\n",
    "uni_b2_enc = set(b2_encodings)\n",
    "print(\"Unique train encodings are: \", uni_b2_enc)\n",
    "print(\"=============================================\")\n",
    "\n",
    "b3_labels, b3_text = returnLabels(lab_files_path, Batch_3)\n",
    "print(\"Total train labels are: \", len(b3_labels))\n",
    "print(\"Total train text is: \", len(b3_text))\n",
    "\n",
    "lab_encoder = LabelEncoder()\n",
    "b3_encodings = lab_encoder.fit_transform(b3_labels)\n",
    "print(\"Total train encodings are: \", len(b3_encodings))\n",
    "\n",
    "\n",
    "uni_b3_enc = set(b3_encodings)\n",
    "print(\"Unique train encodings are: \", uni_b3_enc)\n",
    "print(\"=============================================\")\n",
    "\n",
    "\n",
    "valid_labels, valid_text = returnLabels(lab_files_path, valid_set)\n",
    "print(\"Total validation labels are: \", len(valid_labels))\n",
    "\n",
    "lab_encoder = LabelEncoder()\n",
    "valid_encodings = lab_encoder.fit_transform(valid_labels)\n",
    "print(\"Total train encodings are: \", len(valid_encodings))\n",
    "\n",
    "\n",
    "uni_valid_enc = set(valid_encodings)\n",
    "print(\"Unique validation encodings are: \", uni_valid_enc)\n",
    "print(\"=============================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries Imported..\n"
     ]
    }
   ],
   "source": [
    "# from tensorflow.keras import Sequential\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dropout, Dense, Flatten, Activation\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "\n",
    "from spektral.utils.sparse import sp_matrix_to_sp_tensor\n",
    "from spektral.layers import GCNConv\n",
    "from sklearn.metrics import classification_report\n",
    "from spektral.utils import normalized_laplacian\n",
    "from spektral.utils.sparse import sp_matrix_to_sp_tensor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "print(\"Libraries Imported..\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "channels = 32\n",
    "dropout = 0.5\n",
    "learning_rate = 5e-4\n",
    "l2_reg = 0.001\n",
    "batch_size = 16\n",
    "es_patience = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings for train matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3718\n",
      "Total train test is:  3718\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "Emb_Model = \"./Models/Word2Vec_Model.bin\"\n",
    "Loaded_model = Word2Vec.load(Emb_Model)\n",
    "\n",
    "train_embeddings = []\n",
    "for t in b1_text:\n",
    "    train_embeddings.append(Loaded_model.wv[t])\n",
    "    \n",
    "print(len(train_embeddings))\n",
    "    \n",
    "print(\"Total train test is: \", len(b1_text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "Total classes are:  1211\n",
      "Shape of X-in is:  (None, 100)\n",
      "Shape of A-in is:  (None, None)\n",
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_11 (InputLayer)          [(None, 100)]        0           []                               \n",
      "                                                                                                  \n",
      " input_12 (InputLayer)          [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " gcn_conv_10 (GCNConv)          (None, 32)           3232        ['input_11[0][0]',               \n",
      "                                                                  'input_12[0][0]']               \n",
      "                                                                                                  \n",
      " gcn_conv_11 (GCNConv)          (None, 32)           1056        ['gcn_conv_10[0][0]',            \n",
      "                                                                  'input_12[0][0]']               \n",
      "                                                                                                  \n",
      " flatten_5 (Flatten)            (None, 32)           0           ['gcn_conv_11[0][0]']            \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 128)          4224        ['flatten_5[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 128)          0           ['dense_15[0][0]']               \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 32)           4128        ['dropout_10[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 32)           0           ['dense_16[0][0]']               \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 1211)         39963       ['dropout_11[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 52,603\n",
      "Trainable params: 52,603\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "N = A1.shape[0]\n",
    "F = np.shape(train_embeddings)[1]\n",
    "train_embeddings = np.array(train_embeddings)\n",
    "print(F)\n",
    "\n",
    "classes = len(set(b1_text))\n",
    "print(\"Total classes are: \", classes)\n",
    "\n",
    "# X_in = Input(batch_size=N, shape=(F))\n",
    "X_in = Input(batch_size=None, shape=(F,))\n",
    "print(\"Shape of X-in is: \", np.shape(X_in))\n",
    "\n",
    "# A_in = Input(tensor=sp_matrix_to_sp_tensor(fltr), sparse=True)\n",
    "# A_in = Input(shape=(None,), sparse=True)\n",
    "A_in = Input(shape=(None,))\n",
    "print(\"Shape of A-in is: \", np.shape(A_in))\n",
    "\n",
    "# D1 = Dropout(dropout)(X_in)\n",
    "# G1 = GCNConv(channels, activation='LeakyReLU',\n",
    "            #  kernel_regularizer=l2(l2_reg), use_bias=True)([X_in, A_in])\n",
    "G1 = GCNConv(channels, activation='LeakyReLU')([X_in, A_in])\n",
    "# D2 = Dropout(dropout)(G1)\n",
    "# G2 = GCNConv(channels, activation='LeakyReLU',\n",
    "#              kernel_regularizer=l2(l2_reg), use_bias=True)([G1, A_in])\n",
    "G2 = GCNConv(channels, activation='LeakyReLU')([G1, A_in])\n",
    "\n",
    "# # BN = BatchNormalization()(G2)\n",
    "flatten = Flatten()(G2)\n",
    "\n",
    "D1 = Dense(128, activation='relu')(flatten)\n",
    "Dr1 = Dropout(dropout)(D1)\n",
    "D2 = Dense(32, activation='relu')(Dr1)\n",
    "Dr2 = Dropout(dropout)(D2)\n",
    "# D3 = Dense(16, activation='LeakyReLU')(D2)\n",
    "\n",
    "D2_out = Dense(classes, activation='softmax')(Dr2)\n",
    "\n",
    "model = Model(inputs=[X_in, A_in], outputs=D2_out)\n",
    "# other loss = sparse_categorical_crossentropy\n",
    "# model.compile(optimizer='Adagrad',loss='sparse_categorical_crossentropy', weighted_metrics=['acc'])\n",
    "model.compile(optimizer='Adam',\n",
    "              loss='sparse_categorical_crossentropy', weighted_metrics=['acc'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "1/1 [==============================] - 1s 982ms/step - loss: 7.4984 - acc: 5.3792e-04 - val_loss: 6.8520 - val_acc: 0.0000e+00\n",
      "Epoch 2/500\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 7.2495 - acc: 0.0013 - val_loss: 6.7249 - val_acc: 0.0019\n",
      "Epoch 3/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 7.0650 - acc: 0.0032 - val_loss: 6.6169 - val_acc: 0.0076\n",
      "Epoch 4/500\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 6.9371 - acc: 0.0059 - val_loss: 6.5098 - val_acc: 0.0323\n",
      "Epoch 5/500\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 6.8112 - acc: 0.0145 - val_loss: 6.3939 - val_acc: 0.0646\n",
      "Epoch 6/500\n",
      "1/1 [==============================] - 0s 231ms/step - loss: 6.7145 - acc: 0.0204 - val_loss: 6.2572 - val_acc: 0.0941\n",
      "Epoch 7/500\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 6.5697 - acc: 0.0401 - val_loss: 6.0923 - val_acc: 0.1150\n",
      "Epoch 8/500\n",
      "1/1 [==============================] - 0s 235ms/step - loss: 6.4854 - acc: 0.0522 - val_loss: 5.8952 - val_acc: 0.1416\n",
      "Epoch 9/500\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 6.3494 - acc: 0.0640 - val_loss: 5.6625 - val_acc: 0.1578\n",
      "Epoch 10/500\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 6.1901 - acc: 0.0828 - val_loss: 5.3968 - val_acc: 0.1530\n",
      "Epoch 11/500\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 5.9854 - acc: 0.0958 - val_loss: 5.1018 - val_acc: 0.1559\n",
      "Epoch 12/500\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 5.7910 - acc: 0.1108 - val_loss: 4.7901 - val_acc: 0.1587\n",
      "Epoch 13/500\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 5.5793 - acc: 0.1183 - val_loss: 4.4780 - val_acc: 0.1597\n",
      "Epoch 14/500\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 5.3749 - acc: 0.1165 - val_loss: 4.1866 - val_acc: 0.1616\n",
      "Epoch 15/500\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 5.1641 - acc: 0.1283 - val_loss: 3.9270 - val_acc: 0.1625\n",
      "Epoch 16/500\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 4.9614 - acc: 0.1229 - val_loss: 3.7048 - val_acc: 0.1635\n",
      "Epoch 17/500\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 4.8709 - acc: 0.1269 - val_loss: 3.5162 - val_acc: 0.1673\n",
      "Epoch 18/500\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 4.6675 - acc: 0.1318 - val_loss: 3.3590 - val_acc: 0.1692\n",
      "Epoch 19/500\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 4.5448 - acc: 0.1393 - val_loss: 3.2192 - val_acc: 0.1778\n",
      "Epoch 20/500\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 4.4369 - acc: 0.1315 - val_loss: 3.0948 - val_acc: 0.1692\n",
      "Epoch 21/500\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 4.3382 - acc: 0.1331 - val_loss: 2.9837 - val_acc: 0.1663\n",
      "Epoch 22/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 4.2202 - acc: 0.1380 - val_loss: 2.8885 - val_acc: 0.1492\n",
      "Epoch 23/500\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 4.0103 - acc: 0.1358 - val_loss: 2.8118 - val_acc: 0.1312\n",
      "Epoch 24/500\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 3.8902 - acc: 0.1409 - val_loss: 2.7531 - val_acc: 0.1055\n",
      "Epoch 25/500\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 3.8183 - acc: 0.1313 - val_loss: 2.7156 - val_acc: 0.0884\n",
      "Epoch 26/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 3.7328 - acc: 0.1415 - val_loss: 2.7027 - val_acc: 0.1055\n",
      "Epoch 27/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 3.6213 - acc: 0.1417 - val_loss: 2.7104 - val_acc: 0.1226\n",
      "Epoch 28/500\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 3.6003 - acc: 0.1455 - val_loss: 2.7334 - val_acc: 0.1331\n",
      "Epoch 29/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 3.5360 - acc: 0.1555 - val_loss: 2.7509 - val_acc: 0.1464\n",
      "Epoch 30/500\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 3.4812 - acc: 0.1560 - val_loss: 2.7589 - val_acc: 0.1568\n",
      "Epoch 31/500\n",
      "1/1 [==============================] - 0s 251ms/step - loss: 3.4046 - acc: 0.1528 - val_loss: 2.7496 - val_acc: 0.1606\n",
      "Epoch 32/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 3.3498 - acc: 0.1549 - val_loss: 2.7259 - val_acc: 0.1692\n",
      "Epoch 33/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 3.2917 - acc: 0.1452 - val_loss: 2.6900 - val_acc: 0.1749\n",
      "Epoch 34/500\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 3.2695 - acc: 0.1619 - val_loss: 2.6490 - val_acc: 0.1702\n",
      "Epoch 35/500\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 3.2121 - acc: 0.1460 - val_loss: 2.6045 - val_acc: 0.1606\n",
      "Epoch 36/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 3.1015 - acc: 0.1641 - val_loss: 2.5613 - val_acc: 0.1559\n",
      "Epoch 37/500\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 3.0602 - acc: 0.1428 - val_loss: 2.5209 - val_acc: 0.1445\n",
      "Epoch 38/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 3.0531 - acc: 0.1417 - val_loss: 2.4860 - val_acc: 0.1293\n",
      "Epoch 39/500\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 2.9900 - acc: 0.1407 - val_loss: 2.4566 - val_acc: 0.1150\n",
      "Epoch 40/500\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 2.9336 - acc: 0.1425 - val_loss: 2.4329 - val_acc: 0.1074\n",
      "Epoch 41/500\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 2.9067 - acc: 0.1423 - val_loss: 2.4118 - val_acc: 0.1112\n",
      "Epoch 42/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 2.8767 - acc: 0.1369 - val_loss: 2.3934 - val_acc: 0.1065\n",
      "Epoch 43/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 2.8541 - acc: 0.1538 - val_loss: 2.3737 - val_acc: 0.1255\n",
      "Epoch 44/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 2.7678 - acc: 0.1536 - val_loss: 2.3508 - val_acc: 0.1340\n",
      "Epoch 45/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 2.7789 - acc: 0.1525 - val_loss: 2.3275 - val_acc: 0.1369\n",
      "Epoch 46/500\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 2.7590 - acc: 0.1444 - val_loss: 2.3048 - val_acc: 0.1416\n",
      "Epoch 47/500\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 2.7299 - acc: 0.1450 - val_loss: 2.2870 - val_acc: 0.1559\n",
      "Epoch 48/500\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 2.6911 - acc: 0.1509 - val_loss: 2.2749 - val_acc: 0.1711\n",
      "Epoch 49/500\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 2.6750 - acc: 0.1541 - val_loss: 2.2708 - val_acc: 0.1892\n",
      "Epoch 50/500\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 2.6444 - acc: 0.1544 - val_loss: 2.2728 - val_acc: 0.1939\n",
      "Epoch 51/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 2.6183 - acc: 0.1538 - val_loss: 2.2776 - val_acc: 0.1996\n",
      "Epoch 52/500\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 2.5810 - acc: 0.1560 - val_loss: 2.2839 - val_acc: 0.1987\n",
      "Epoch 53/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 2.5927 - acc: 0.1517 - val_loss: 2.2872 - val_acc: 0.1958\n",
      "Epoch 54/500\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 2.5470 - acc: 0.1630 - val_loss: 2.2901 - val_acc: 0.1939\n",
      "Epoch 55/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 2.5449 - acc: 0.1608 - val_loss: 2.2897 - val_acc: 0.1901\n",
      "Epoch 56/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 2.5064 - acc: 0.1520 - val_loss: 2.2875 - val_acc: 0.1759\n",
      "Epoch 57/500\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 2.4884 - acc: 0.1501 - val_loss: 2.2823 - val_acc: 0.1721\n",
      "Epoch 58/500\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 2.4665 - acc: 0.1622 - val_loss: 2.2778 - val_acc: 0.1616\n",
      "Epoch 59/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 2.4887 - acc: 0.1538 - val_loss: 2.2718 - val_acc: 0.1521\n",
      "Epoch 60/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 2.4420 - acc: 0.1565 - val_loss: 2.2658 - val_acc: 0.1350\n",
      "Epoch 61/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 2.3969 - acc: 0.1520 - val_loss: 2.2639 - val_acc: 0.1160\n",
      "Epoch 62/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 2.4219 - acc: 0.1568 - val_loss: 2.2585 - val_acc: 0.1236\n",
      "Epoch 63/500\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 2.3936 - acc: 0.1530 - val_loss: 2.2545 - val_acc: 0.1198\n",
      "Epoch 64/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 2.3738 - acc: 0.1474 - val_loss: 2.2550 - val_acc: 0.1255\n",
      "Epoch 65/500\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 2.3942 - acc: 0.1382 - val_loss: 2.2588 - val_acc: 0.1350\n",
      "Epoch 66/500\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 2.3469 - acc: 0.1643 - val_loss: 2.2649 - val_acc: 0.1549\n",
      "Epoch 67/500\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 2.3381 - acc: 0.1568 - val_loss: 2.2691 - val_acc: 0.1635\n",
      "Epoch 68/500\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 2.3236 - acc: 0.1487 - val_loss: 2.2733 - val_acc: 0.1759\n",
      "Epoch 69/500\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 2.3173 - acc: 0.1649 - val_loss: 2.2760 - val_acc: 0.1873\n",
      "Epoch 70/500\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 2.3287 - acc: 0.1606 - val_loss: 2.2777 - val_acc: 0.1863\n",
      "Epoch 71/500\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 2.3164 - acc: 0.1466 - val_loss: 2.2739 - val_acc: 0.1844\n",
      "Epoch 72/500\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 2.2973 - acc: 0.1525 - val_loss: 2.2666 - val_acc: 0.1759\n",
      "Epoch 73/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 2.2611 - acc: 0.1595 - val_loss: 2.2552 - val_acc: 0.1749\n",
      "Epoch 74/500\n",
      "1/1 [==============================] - 0s 231ms/step - loss: 2.2708 - acc: 0.1611 - val_loss: 2.2424 - val_acc: 0.1597\n",
      "Epoch 75/500\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 2.2545 - acc: 0.1600 - val_loss: 2.2353 - val_acc: 0.1369\n",
      "Epoch 76/500\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 2.2522 - acc: 0.1668 - val_loss: 2.2300 - val_acc: 0.1217\n",
      "Epoch 77/500\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 2.2515 - acc: 0.1581 - val_loss: 2.2278 - val_acc: 0.1407\n",
      "Epoch 78/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 2.2447 - acc: 0.1598 - val_loss: 2.2276 - val_acc: 0.1483\n",
      "Epoch 79/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 2.2353 - acc: 0.1638 - val_loss: 2.2322 - val_acc: 0.1378\n",
      "Epoch 80/500\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 2.2203 - acc: 0.1689 - val_loss: 2.2397 - val_acc: 0.1397\n",
      "Epoch 81/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 2.2166 - acc: 0.1703 - val_loss: 2.2433 - val_acc: 0.1416\n",
      "Epoch 82/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 2.2197 - acc: 0.1509 - val_loss: 2.2493 - val_acc: 0.1492\n",
      "Epoch 83/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 2.2101 - acc: 0.1581 - val_loss: 2.2483 - val_acc: 0.1578\n",
      "Epoch 84/500\n",
      "1/1 [==============================] - 0s 306ms/step - loss: 2.2069 - acc: 0.1665 - val_loss: 2.2421 - val_acc: 0.1625\n",
      "Epoch 85/500\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 2.2022 - acc: 0.1692 - val_loss: 2.2361 - val_acc: 0.1568\n",
      "Epoch 86/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 2.1888 - acc: 0.1678 - val_loss: 2.2317 - val_acc: 0.1483\n",
      "Epoch 87/500\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 2.1882 - acc: 0.1678 - val_loss: 2.2293 - val_acc: 0.1397\n",
      "Epoch 88/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 2.1806 - acc: 0.1600 - val_loss: 2.2396 - val_acc: 0.1340\n",
      "Epoch 89/500\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 2.1835 - acc: 0.1735 - val_loss: 2.2497 - val_acc: 0.1245\n",
      "Epoch 90/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 2.1767 - acc: 0.1641 - val_loss: 2.2542 - val_acc: 0.1141\n",
      "Epoch 91/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 2.1666 - acc: 0.1703 - val_loss: 2.2460 - val_acc: 0.1150\n",
      "Epoch 92/500\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 2.1726 - acc: 0.1646 - val_loss: 2.2262 - val_acc: 0.1350\n",
      "Epoch 93/500\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 2.1592 - acc: 0.1651 - val_loss: 2.2097 - val_acc: 0.1721\n",
      "Epoch 94/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 2.1641 - acc: 0.1659 - val_loss: 2.2048 - val_acc: 0.2015\n",
      "Epoch 95/500\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 2.1697 - acc: 0.1573 - val_loss: 2.2103 - val_acc: 0.2063\n",
      "Epoch 96/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 2.1656 - acc: 0.1633 - val_loss: 2.2201 - val_acc: 0.2006\n",
      "Epoch 97/500\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 2.1518 - acc: 0.1649 - val_loss: 2.2313 - val_acc: 0.1654\n",
      "Epoch 98/500\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 2.1533 - acc: 0.1759 - val_loss: 2.2431 - val_acc: 0.1274\n",
      "Epoch 99/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 2.1568 - acc: 0.1711 - val_loss: 2.2455 - val_acc: 0.1179\n",
      "Epoch 100/500\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 2.1601 - acc: 0.1686 - val_loss: 2.2318 - val_acc: 0.1321\n",
      "Epoch 101/500\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 2.1555 - acc: 0.1716 - val_loss: 2.2118 - val_acc: 0.1587\n",
      "Epoch 102/500\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 2.1507 - acc: 0.1665 - val_loss: 2.2024 - val_acc: 0.1740\n",
      "Epoch 103/500\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 2.1436 - acc: 0.1732 - val_loss: 2.1991 - val_acc: 0.1920\n",
      "Epoch 104/500\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 2.1470 - acc: 0.1654 - val_loss: 2.2083 - val_acc: 0.1730\n",
      "Epoch 105/500\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 2.1546 - acc: 0.1673 - val_loss: 2.2196 - val_acc: 0.1578\n",
      "Epoch 106/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 2.1469 - acc: 0.1676 - val_loss: 2.2256 - val_acc: 0.1426\n",
      "Epoch 107/500\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 2.1383 - acc: 0.1678 - val_loss: 2.2249 - val_acc: 0.1416\n",
      "Epoch 108/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 2.1532 - acc: 0.1754 - val_loss: 2.2094 - val_acc: 0.1559\n",
      "Epoch 109/500\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 2.1365 - acc: 0.1633 - val_loss: 2.1975 - val_acc: 0.1721\n",
      "Epoch 110/500\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 2.1388 - acc: 0.1781 - val_loss: 2.1933 - val_acc: 0.1797\n",
      "Epoch 111/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 2.1525 - acc: 0.1625 - val_loss: 2.1970 - val_acc: 0.1616\n",
      "Epoch 112/500\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 2.1442 - acc: 0.1670 - val_loss: 2.2056 - val_acc: 0.1435\n",
      "Epoch 113/500\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 2.1417 - acc: 0.1681 - val_loss: 2.2158 - val_acc: 0.1293\n",
      "Epoch 114/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 2.1264 - acc: 0.1746 - val_loss: 2.2258 - val_acc: 0.1255\n",
      "Epoch 115/500\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 2.1279 - acc: 0.1622 - val_loss: 2.2363 - val_acc: 0.1378\n",
      "Epoch 116/500\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 2.1360 - acc: 0.1764 - val_loss: 2.2497 - val_acc: 0.1625\n",
      "Epoch 117/500\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 2.1389 - acc: 0.1625 - val_loss: 2.2395 - val_acc: 0.1740\n",
      "Epoch 118/500\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 2.1300 - acc: 0.1713 - val_loss: 2.2263 - val_acc: 0.1559\n",
      "Epoch 119/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 2.1254 - acc: 0.1662 - val_loss: 2.2183 - val_acc: 0.1454\n",
      "Epoch 120/500\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 2.1332 - acc: 0.1721 - val_loss: 2.2005 - val_acc: 0.1264\n",
      "Epoch 121/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 2.1333 - acc: 0.1678 - val_loss: 2.2024 - val_acc: 0.1226\n",
      "Epoch 122/500\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 2.1333 - acc: 0.1627 - val_loss: 2.2069 - val_acc: 0.1312\n",
      "Epoch 123/500\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 2.1352 - acc: 0.1622 - val_loss: 2.2181 - val_acc: 0.1502\n",
      "Epoch 124/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 2.1268 - acc: 0.1697 - val_loss: 2.2233 - val_acc: 0.1616\n",
      "Epoch 125/500\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 2.1310 - acc: 0.1740 - val_loss: 2.2193 - val_acc: 0.1502\n",
      "Epoch 126/500\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 2.1228 - acc: 0.1692 - val_loss: 2.2029 - val_acc: 0.1483\n",
      "Epoch 127/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 2.1313 - acc: 0.1746 - val_loss: 2.1948 - val_acc: 0.1264\n",
      "Epoch 128/500\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 2.1227 - acc: 0.1684 - val_loss: 2.2001 - val_acc: 0.1226\n",
      "Epoch 129/500\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 2.1207 - acc: 0.1692 - val_loss: 2.2170 - val_acc: 0.1245\n",
      "Epoch 130/500\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 2.1281 - acc: 0.1692 - val_loss: 2.2441 - val_acc: 0.1378\n",
      "Epoch 131/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 2.1323 - acc: 0.1662 - val_loss: 2.2551 - val_acc: 0.1464\n",
      "Epoch 132/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 2.1269 - acc: 0.1713 - val_loss: 2.2326 - val_acc: 0.1673\n",
      "Epoch 133/500\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 2.1247 - acc: 0.1729 - val_loss: 2.2034 - val_acc: 0.1663\n",
      "Epoch 134/500\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 2.1282 - acc: 0.1732 - val_loss: 2.1856 - val_acc: 0.1255\n",
      "Epoch 135/500\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 2.1328 - acc: 0.1547 - val_loss: 2.1820 - val_acc: 0.1131\n",
      "Epoch 136/500\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 2.1345 - acc: 0.1716 - val_loss: 2.1961 - val_acc: 0.1131\n",
      "Epoch 137/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 2.1318 - acc: 0.1697 - val_loss: 2.2220 - val_acc: 0.1245\n",
      "Epoch 138/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 2.1293 - acc: 0.1697 - val_loss: 2.2278 - val_acc: 0.1702\n",
      "Epoch 139/500\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 2.1284 - acc: 0.1759 - val_loss: 2.2231 - val_acc: 0.1549\n",
      "Epoch 140/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 2.1234 - acc: 0.1719 - val_loss: 2.2139 - val_acc: 0.1150\n",
      "Epoch 141/500\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 2.1282 - acc: 0.1670 - val_loss: 2.2106 - val_acc: 0.1122\n",
      "Epoch 142/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 2.1322 - acc: 0.1694 - val_loss: 2.2134 - val_acc: 0.1131\n",
      "Epoch 143/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 2.1293 - acc: 0.1721 - val_loss: 2.2077 - val_acc: 0.1188\n",
      "Epoch 144/500\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 2.1233 - acc: 0.1786 - val_loss: 2.2088 - val_acc: 0.1464\n",
      "Epoch 145/500\n",
      "1/1 [==============================] - 0s 228ms/step - loss: 2.1203 - acc: 0.1716 - val_loss: 2.2124 - val_acc: 0.1625\n",
      "Epoch 146/500\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 2.1245 - acc: 0.1732 - val_loss: 2.2108 - val_acc: 0.1511\n",
      "Epoch 147/500\n",
      "1/1 [==============================] - 0s 245ms/step - loss: 2.1203 - acc: 0.1764 - val_loss: 2.2120 - val_acc: 0.1131\n",
      "Epoch 148/500\n",
      "1/1 [==============================] - 0s 249ms/step - loss: 2.1230 - acc: 0.1678 - val_loss: 2.2120 - val_acc: 0.1131\n",
      "Epoch 149/500\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 2.1299 - acc: 0.1686 - val_loss: 2.2129 - val_acc: 0.1141\n",
      "Epoch 150/500\n",
      "1/1 [==============================] - 0s 225ms/step - loss: 2.1158 - acc: 0.1703 - val_loss: 2.2289 - val_acc: 0.1283\n",
      "Epoch 151/500\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 2.1185 - acc: 0.1697 - val_loss: 2.2423 - val_acc: 0.1511\n",
      "Epoch 152/500\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 2.1292 - acc: 0.1775 - val_loss: 2.2345 - val_acc: 0.1502\n",
      "Epoch 153/500\n",
      "1/1 [==============================] - 0s 235ms/step - loss: 2.1161 - acc: 0.1705 - val_loss: 2.2102 - val_acc: 0.1454\n",
      "Epoch 154/500\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 2.1239 - acc: 0.1616 - val_loss: 2.1942 - val_acc: 0.1179\n",
      "Epoch 155/500\n",
      "1/1 [==============================] - 0s 226ms/step - loss: 2.1303 - acc: 0.1625 - val_loss: 2.1965 - val_acc: 0.1141\n",
      "Epoch 156/500\n",
      "1/1 [==============================] - 0s 224ms/step - loss: 2.1192 - acc: 0.1670 - val_loss: 2.2046 - val_acc: 0.1217\n",
      "Epoch 157/500\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 2.1254 - acc: 0.1735 - val_loss: 2.2235 - val_acc: 0.1568\n",
      "Epoch 158/500\n",
      "1/1 [==============================] - 0s 224ms/step - loss: 2.1255 - acc: 0.1797 - val_loss: 2.2232 - val_acc: 0.1549\n",
      "Epoch 159/500\n",
      "1/1 [==============================] - 0s 231ms/step - loss: 2.1225 - acc: 0.1772 - val_loss: 2.2059 - val_acc: 0.1312\n",
      "Epoch 160/500\n",
      "1/1 [==============================] - 0s 235ms/step - loss: 2.1255 - acc: 0.1724 - val_loss: 2.1958 - val_acc: 0.1150\n",
      "Epoch 161/500\n",
      "1/1 [==============================] - 0s 233ms/step - loss: 2.1269 - acc: 0.1662 - val_loss: 2.2036 - val_acc: 0.1160\n",
      "Epoch 162/500\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 2.1259 - acc: 0.1716 - val_loss: 2.2183 - val_acc: 0.1331\n",
      "Epoch 163/500\n",
      "1/1 [==============================] - 0s 232ms/step - loss: 2.1218 - acc: 0.1743 - val_loss: 2.2362 - val_acc: 0.1559\n",
      "Epoch 164/500\n",
      "1/1 [==============================] - 0s 243ms/step - loss: 2.1262 - acc: 0.1737 - val_loss: 2.2355 - val_acc: 0.1511\n",
      "Epoch 165/500\n",
      "1/1 [==============================] - 0s 230ms/step - loss: 2.1231 - acc: 0.1759 - val_loss: 2.2173 - val_acc: 0.1483\n",
      "Epoch 166/500\n",
      "1/1 [==============================] - 0s 228ms/step - loss: 2.1290 - acc: 0.1686 - val_loss: 2.1902 - val_acc: 0.1131\n",
      "Epoch 167/500\n",
      "1/1 [==============================] - 0s 230ms/step - loss: 2.1263 - acc: 0.1700 - val_loss: 2.1869 - val_acc: 0.1131\n",
      "Epoch 168/500\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 2.1223 - acc: 0.1721 - val_loss: 2.2004 - val_acc: 0.1131\n",
      "Epoch 169/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 2.1161 - acc: 0.1743 - val_loss: 2.2237 - val_acc: 0.1179\n",
      "Epoch 170/500\n",
      "1/1 [==============================] - 0s 224ms/step - loss: 2.1156 - acc: 0.1713 - val_loss: 2.2444 - val_acc: 0.1293\n",
      "Epoch 171/500\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 2.1261 - acc: 0.1716 - val_loss: 2.2264 - val_acc: 0.1312\n",
      "Epoch 172/500\n",
      "1/1 [==============================] - 0s 227ms/step - loss: 2.1206 - acc: 0.1708 - val_loss: 2.1956 - val_acc: 0.1293\n",
      "Epoch 173/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 2.1193 - acc: 0.1694 - val_loss: 2.1803 - val_acc: 0.1188\n",
      "Epoch 174/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 2.1202 - acc: 0.1630 - val_loss: 2.1917 - val_acc: 0.1207\n",
      "Epoch 175/500\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 2.1205 - acc: 0.1662 - val_loss: 2.2237 - val_acc: 0.1217\n",
      "Epoch 176/500\n",
      "1/1 [==============================] - 0s 245ms/step - loss: 2.1257 - acc: 0.1697 - val_loss: 2.2399 - val_acc: 0.1274\n",
      "Epoch 177/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 2.1236 - acc: 0.1670 - val_loss: 2.2290 - val_acc: 0.1274\n",
      "Epoch 178/500\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 2.1206 - acc: 0.1676 - val_loss: 2.1984 - val_acc: 0.1426\n",
      "Epoch 179/500\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 2.1216 - acc: 0.1861 - val_loss: 2.1833 - val_acc: 0.1274\n",
      "Epoch 180/500\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 2.1158 - acc: 0.1729 - val_loss: 2.1914 - val_acc: 0.1169\n",
      "Epoch 181/500\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 2.1204 - acc: 0.1692 - val_loss: 2.2128 - val_acc: 0.1131\n",
      "Epoch 182/500\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 2.1209 - acc: 0.1740 - val_loss: 2.2357 - val_acc: 0.1122\n",
      "Epoch 183/500\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 2.1279 - acc: 0.1670 - val_loss: 2.2307 - val_acc: 0.1179\n",
      "Epoch 184/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 2.1163 - acc: 0.1689 - val_loss: 2.2124 - val_acc: 0.1521\n",
      "Epoch 185/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 2.1240 - acc: 0.1746 - val_loss: 2.2091 - val_acc: 0.1502\n",
      "Epoch 186/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 2.1209 - acc: 0.1740 - val_loss: 2.2117 - val_acc: 0.1188\n",
      "Epoch 187/500\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 2.1256 - acc: 0.1649 - val_loss: 2.2158 - val_acc: 0.1188\n",
      "Epoch 188/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 2.1222 - acc: 0.1676 - val_loss: 2.2205 - val_acc: 0.1188\n",
      "Epoch 189/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 2.1241 - acc: 0.1724 - val_loss: 2.2137 - val_acc: 0.1521\n",
      "Epoch 190/500\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 2.1125 - acc: 0.1719 - val_loss: 2.2228 - val_acc: 0.1606\n",
      "Epoch 191/500\n",
      "1/1 [==============================] - 0s 238ms/step - loss: 2.1112 - acc: 0.1791 - val_loss: 2.2311 - val_acc: 0.1426\n",
      "Epoch 192/500\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 2.1239 - acc: 0.1772 - val_loss: 2.2223 - val_acc: 0.1274\n",
      "Epoch 193/500\n",
      "1/1 [==============================] - 0s 232ms/step - loss: 2.1223 - acc: 0.1743 - val_loss: 2.2135 - val_acc: 0.1264\n",
      "Epoch 194/500\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 2.1205 - acc: 0.1708 - val_loss: 2.2155 - val_acc: 0.1397\n",
      "Epoch 195/500\n",
      "1/1 [==============================] - 0s 226ms/step - loss: 2.1195 - acc: 0.1737 - val_loss: 2.2184 - val_acc: 0.1426\n",
      "Epoch 196/500\n",
      "1/1 [==============================] - 0s 224ms/step - loss: 2.1183 - acc: 0.1678 - val_loss: 2.2157 - val_acc: 0.1426\n",
      "Epoch 197/500\n",
      "1/1 [==============================] - 0s 241ms/step - loss: 2.1152 - acc: 0.1794 - val_loss: 2.2114 - val_acc: 0.1492\n",
      "Epoch 198/500\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 2.1206 - acc: 0.1694 - val_loss: 2.2037 - val_acc: 0.1511\n",
      "Epoch 199/500\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 2.1251 - acc: 0.1767 - val_loss: 2.2029 - val_acc: 0.1568\n",
      "Epoch 200/500\n",
      "1/1 [==============================] - 0s 237ms/step - loss: 2.1154 - acc: 0.1829 - val_loss: 2.2152 - val_acc: 0.1397\n",
      "Epoch 201/500\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 2.1184 - acc: 0.1684 - val_loss: 2.2376 - val_acc: 0.1255\n",
      "Epoch 202/500\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 2.1177 - acc: 0.1703 - val_loss: 2.2508 - val_acc: 0.1122\n",
      "Epoch 203/500\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 2.1207 - acc: 0.1684 - val_loss: 2.2280 - val_acc: 0.1198\n",
      "Epoch 204/500\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 2.1125 - acc: 0.1719 - val_loss: 2.2072 - val_acc: 0.1416\n",
      "Epoch 205/500\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 2.1241 - acc: 0.1821 - val_loss: 2.1974 - val_acc: 0.1492\n",
      "Epoch 206/500\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 2.1166 - acc: 0.1735 - val_loss: 2.2039 - val_acc: 0.1483\n",
      "Epoch 207/500\n",
      "1/1 [==============================] - 0s 236ms/step - loss: 2.1272 - acc: 0.1740 - val_loss: 2.2138 - val_acc: 0.1331\n",
      "Epoch 208/500\n",
      "1/1 [==============================] - 0s 241ms/step - loss: 2.1242 - acc: 0.1724 - val_loss: 2.2269 - val_acc: 0.1359\n",
      "Epoch 209/500\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 2.1182 - acc: 0.1778 - val_loss: 2.2278 - val_acc: 0.1426\n",
      "Epoch 210/500\n",
      "1/1 [==============================] - 0s 233ms/step - loss: 2.1225 - acc: 0.1805 - val_loss: 2.2178 - val_acc: 0.1312\n",
      "Epoch 211/500\n",
      "1/1 [==============================] - 0s 238ms/step - loss: 2.1113 - acc: 0.1735 - val_loss: 2.2090 - val_acc: 0.1264\n",
      "Epoch 212/500\n",
      "1/1 [==============================] - 0s 237ms/step - loss: 2.1165 - acc: 0.1727 - val_loss: 2.2123 - val_acc: 0.1217\n",
      "Epoch 213/500\n",
      "1/1 [==============================] - 0s 235ms/step - loss: 2.1157 - acc: 0.1756 - val_loss: 2.2196 - val_acc: 0.1445\n",
      "Epoch 214/500\n",
      "1/1 [==============================] - 0s 229ms/step - loss: 2.1121 - acc: 0.1754 - val_loss: 2.2289 - val_acc: 0.1521\n",
      "Epoch 215/500\n",
      "1/1 [==============================] - 0s 232ms/step - loss: 2.1171 - acc: 0.1778 - val_loss: 2.2289 - val_acc: 0.1568\n",
      "Epoch 216/500\n",
      "1/1 [==============================] - 0s 247ms/step - loss: 2.1151 - acc: 0.1824 - val_loss: 2.2271 - val_acc: 0.1654\n",
      "Epoch 217/500\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 2.1252 - acc: 0.1783 - val_loss: 2.2135 - val_acc: 0.1597\n",
      "Epoch 218/500\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 2.1145 - acc: 0.1842 - val_loss: 2.2009 - val_acc: 0.1559\n",
      "Epoch 219/500\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 2.1124 - acc: 0.1767 - val_loss: 2.2085 - val_acc: 0.1483\n",
      "Epoch 220/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 2.1131 - acc: 0.1716 - val_loss: 2.2222 - val_acc: 0.1473\n",
      "Epoch 221/500\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 2.1123 - acc: 0.1786 - val_loss: 2.2398 - val_acc: 0.1426\n",
      "Epoch 222/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 2.1148 - acc: 0.1802 - val_loss: 2.2535 - val_acc: 0.1511\n",
      "Epoch 223/500\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 2.1171 - acc: 0.1762 - val_loss: 2.2455 - val_acc: 0.1730\n",
      "Epoch 224/500\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 2.1138 - acc: 0.1807 - val_loss: 2.2298 - val_acc: 0.1740\n",
      "Epoch 225/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 2.1088 - acc: 0.1837 - val_loss: 2.2183 - val_acc: 0.1568\n",
      "Epoch 226/500\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 2.1136 - acc: 0.1861 - val_loss: 2.2171 - val_acc: 0.1293\n",
      "Epoch 227/500\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 2.1222 - acc: 0.1743 - val_loss: 2.2134 - val_acc: 0.1179\n",
      "Epoch 228/500\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 2.1154 - acc: 0.1737 - val_loss: 2.2155 - val_acc: 0.1245\n",
      "Epoch 229/500\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 2.1136 - acc: 0.1783 - val_loss: 2.2195 - val_acc: 0.1711\n",
      "Epoch 230/500\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 2.1192 - acc: 0.1880 - val_loss: 2.2133 - val_acc: 0.1711\n",
      "Epoch 231/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 2.1176 - acc: 0.1834 - val_loss: 2.2121 - val_acc: 0.1644\n",
      "Epoch 232/500\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 2.1176 - acc: 0.1813 - val_loss: 2.2175 - val_acc: 0.1179\n",
      "Epoch 233/500\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 2.1168 - acc: 0.1802 - val_loss: 2.2303 - val_acc: 0.1122\n",
      "Epoch 234/500\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 2.1152 - acc: 0.1746 - val_loss: 2.2334 - val_acc: 0.1112\n",
      "Epoch 235/500\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 2.1227 - acc: 0.1719 - val_loss: 2.2301 - val_acc: 0.1559\n",
      "Epoch 236/500\n",
      "1/1 [==============================] - 0s 250ms/step - loss: 2.1206 - acc: 0.1791 - val_loss: 2.2255 - val_acc: 0.1749\n",
      "Epoch 237/500\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 2.1175 - acc: 0.1837 - val_loss: 2.2182 - val_acc: 0.1759\n",
      "Epoch 238/500\n",
      "1/1 [==============================] - 0s 238ms/step - loss: 2.1123 - acc: 0.1950 - val_loss: 2.2139 - val_acc: 0.1559\n",
      "Epoch 239/500\n",
      "1/1 [==============================] - 0s 228ms/step - loss: 2.1194 - acc: 0.1826 - val_loss: 2.2086 - val_acc: 0.1169\n",
      "Epoch 240/500\n",
      "1/1 [==============================] - 0s 231ms/step - loss: 2.1070 - acc: 0.1818 - val_loss: 2.2084 - val_acc: 0.1188\n",
      "Epoch 241/500\n",
      "1/1 [==============================] - 0s 242ms/step - loss: 2.1051 - acc: 0.1770 - val_loss: 2.2032 - val_acc: 0.1340\n",
      "Epoch 242/500\n",
      "1/1 [==============================] - 0s 227ms/step - loss: 2.1190 - acc: 0.1700 - val_loss: 2.2043 - val_acc: 0.1625\n",
      "Epoch 243/500\n",
      "1/1 [==============================] - 0s 257ms/step - loss: 2.1062 - acc: 0.1689 - val_loss: 2.2229 - val_acc: 0.1692\n",
      "Epoch 244/500\n",
      "1/1 [==============================] - 0s 245ms/step - loss: 2.1143 - acc: 0.1775 - val_loss: 2.2474 - val_acc: 0.1683\n",
      "Epoch 245/500\n",
      "1/1 [==============================] - 0s 229ms/step - loss: 2.1152 - acc: 0.1810 - val_loss: 2.2519 - val_acc: 0.1407\n",
      "Epoch 246/500\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 2.1197 - acc: 0.1826 - val_loss: 2.2372 - val_acc: 0.1331\n",
      "Epoch 247/500\n",
      "1/1 [==============================] - 0s 341ms/step - loss: 2.1117 - acc: 0.1781 - val_loss: 2.2126 - val_acc: 0.1350\n",
      "Epoch 248/500\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 2.1281 - acc: 0.1751 - val_loss: 2.1904 - val_acc: 0.1625\n",
      "Epoch 249/500\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 2.1127 - acc: 0.1708 - val_loss: 2.1972 - val_acc: 0.1673\n",
      "Epoch 250/500\n",
      "1/1 [==============================] - 0s 226ms/step - loss: 2.1170 - acc: 0.1740 - val_loss: 2.2217 - val_acc: 0.1616\n",
      "Epoch 251/500\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 2.1132 - acc: 0.1786 - val_loss: 2.2523 - val_acc: 0.1492\n",
      "Epoch 252/500\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 2.1086 - acc: 0.1799 - val_loss: 2.2725 - val_acc: 0.1407\n",
      "Epoch 253/500\n",
      "1/1 [==============================] - 0s 229ms/step - loss: 2.1138 - acc: 0.1815 - val_loss: 2.2599 - val_acc: 0.1502\n",
      "Epoch 254/500\n",
      "1/1 [==============================] - 0s 237ms/step - loss: 2.1102 - acc: 0.1891 - val_loss: 2.2267 - val_acc: 0.1540\n",
      "Epoch 255/500\n",
      "1/1 [==============================] - 0s 238ms/step - loss: 2.1152 - acc: 0.1783 - val_loss: 2.1999 - val_acc: 0.1606\n",
      "Epoch 256/500\n",
      "1/1 [==============================] - 0s 233ms/step - loss: 2.1173 - acc: 0.1759 - val_loss: 2.1893 - val_acc: 0.1616\n",
      "Epoch 257/500\n",
      "1/1 [==============================] - 0s 229ms/step - loss: 2.1151 - acc: 0.1665 - val_loss: 2.2053 - val_acc: 0.1521\n",
      "Epoch 258/500\n",
      "1/1 [==============================] - 0s 238ms/step - loss: 2.1068 - acc: 0.1743 - val_loss: 2.2423 - val_acc: 0.1369\n",
      "Epoch 259/500\n",
      "1/1 [==============================] - 0s 229ms/step - loss: 2.1199 - acc: 0.1762 - val_loss: 2.2749 - val_acc: 0.1397\n",
      "Epoch 260/500\n",
      "1/1 [==============================] - 0s 237ms/step - loss: 2.1148 - acc: 0.1832 - val_loss: 2.2725 - val_acc: 0.1663\n",
      "Epoch 261/500\n",
      "1/1 [==============================] - 0s 235ms/step - loss: 2.1099 - acc: 0.1754 - val_loss: 2.2547 - val_acc: 0.1816\n",
      "Epoch 262/500\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 2.1075 - acc: 0.1931 - val_loss: 2.2291 - val_acc: 0.1797\n",
      "Epoch 263/500\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 2.1104 - acc: 0.1815 - val_loss: 2.2179 - val_acc: 0.1663\n",
      "Epoch 264/500\n",
      "1/1 [==============================] - 0s 235ms/step - loss: 2.1194 - acc: 0.1729 - val_loss: 2.2165 - val_acc: 0.1454\n",
      "Epoch 265/500\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 2.1063 - acc: 0.1799 - val_loss: 2.2292 - val_acc: 0.1302\n",
      "Epoch 266/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 2.1123 - acc: 0.1764 - val_loss: 2.2383 - val_acc: 0.1625\n",
      "Epoch 267/500\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 2.1075 - acc: 0.1845 - val_loss: 2.2485 - val_acc: 0.1787\n",
      "Epoch 268/500\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 2.1042 - acc: 0.1888 - val_loss: 2.2510 - val_acc: 0.1787\n",
      "Epoch 269/500\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 2.1124 - acc: 0.1861 - val_loss: 2.2404 - val_acc: 0.1673\n",
      "Epoch 270/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 2.1132 - acc: 0.1810 - val_loss: 2.2216 - val_acc: 0.1635\n",
      "Epoch 271/500\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 2.1129 - acc: 0.1832 - val_loss: 2.2096 - val_acc: 0.1492\n",
      "Epoch 272/500\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 2.1127 - acc: 0.1727 - val_loss: 2.2074 - val_acc: 0.1378\n",
      "Epoch 273/500\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 2.1041 - acc: 0.1748 - val_loss: 2.2205 - val_acc: 0.1416\n",
      "Epoch 274/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 2.1125 - acc: 0.1781 - val_loss: 2.2452 - val_acc: 0.1492\n",
      "Epoch 275/500\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 2.1072 - acc: 0.1797 - val_loss: 2.2600 - val_acc: 0.1587\n",
      "Epoch 276/500\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 2.1131 - acc: 0.1818 - val_loss: 2.2550 - val_acc: 0.1530\n",
      "Epoch 277/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 2.1128 - acc: 0.1883 - val_loss: 2.2375 - val_acc: 0.1635\n",
      "Epoch 278/500\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 2.1138 - acc: 0.1912 - val_loss: 2.2215 - val_acc: 0.1568\n",
      "Epoch 279/500\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 2.1084 - acc: 0.1832 - val_loss: 2.2180 - val_acc: 0.1378\n",
      "Epoch 280/500\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 2.1139 - acc: 0.1789 - val_loss: 2.2248 - val_acc: 0.1312\n",
      "Epoch 281/500\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 2.1065 - acc: 0.1837 - val_loss: 2.2417 - val_acc: 0.1226\n",
      "Epoch 282/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 2.1064 - acc: 0.1732 - val_loss: 2.2565 - val_acc: 0.1350\n",
      "Epoch 283/500\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 2.1115 - acc: 0.1848 - val_loss: 2.2595 - val_acc: 0.1568\n",
      "Epoch 284/500\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 2.1070 - acc: 0.1872 - val_loss: 2.2495 - val_acc: 0.1683\n",
      "Epoch 285/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 2.1077 - acc: 0.1778 - val_loss: 2.2412 - val_acc: 0.1692\n",
      "Epoch 286/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 2.1108 - acc: 0.1899 - val_loss: 2.2308 - val_acc: 0.1616\n",
      "Epoch 287/500\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 2.1115 - acc: 0.1756 - val_loss: 2.2290 - val_acc: 0.1331\n",
      "Epoch 288/500\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 2.1092 - acc: 0.1751 - val_loss: 2.2362 - val_acc: 0.1283\n",
      "Epoch 289/500\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 2.1114 - acc: 0.1791 - val_loss: 2.2504 - val_acc: 0.1302\n",
      "Epoch 290/500\n",
      "1/1 [==============================] - 0s 225ms/step - loss: 2.1183 - acc: 0.1732 - val_loss: 2.2450 - val_acc: 0.1625\n",
      "Epoch 291/500\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 2.1095 - acc: 0.1848 - val_loss: 2.2376 - val_acc: 0.1721\n",
      "Epoch 292/500\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 2.1079 - acc: 0.1915 - val_loss: 2.2336 - val_acc: 0.1768\n",
      "Epoch 293/500\n",
      "1/1 [==============================] - 0s 224ms/step - loss: 2.1136 - acc: 0.1791 - val_loss: 2.2360 - val_acc: 0.1740\n",
      "Epoch 294/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 2.1045 - acc: 0.1888 - val_loss: 2.2497 - val_acc: 0.1683\n",
      "Epoch 295/500\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 2.1055 - acc: 0.1864 - val_loss: 2.2633 - val_acc: 0.1388\n",
      "Epoch 296/500\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 2.1054 - acc: 0.1821 - val_loss: 2.2705 - val_acc: 0.1321\n",
      "Epoch 297/500\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 2.1061 - acc: 0.1797 - val_loss: 2.2711 - val_acc: 0.1540\n",
      "Epoch 298/500\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 2.1141 - acc: 0.1805 - val_loss: 2.2549 - val_acc: 0.1740\n",
      "Epoch 299/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 2.1115 - acc: 0.1872 - val_loss: 2.2431 - val_acc: 0.1787\n",
      "Epoch 300/500\n",
      "1/1 [==============================] - 0s 225ms/step - loss: 2.1157 - acc: 0.1842 - val_loss: 2.2339 - val_acc: 0.1797\n",
      "Epoch 301/500\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 2.1137 - acc: 0.1867 - val_loss: 2.2279 - val_acc: 0.1663\n",
      "Epoch 302/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 2.1170 - acc: 0.1805 - val_loss: 2.2270 - val_acc: 0.1407\n",
      "Epoch 303/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 2.1098 - acc: 0.1853 - val_loss: 2.2312 - val_acc: 0.1293\n",
      "Epoch 304/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 2.1061 - acc: 0.1772 - val_loss: 2.2390 - val_acc: 0.1321\n",
      "Epoch 305/500\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 2.1105 - acc: 0.1767 - val_loss: 2.2430 - val_acc: 0.1568\n",
      "Epoch 306/500\n",
      "1/1 [==============================] - 0s 292ms/step - loss: 2.1137 - acc: 0.1797 - val_loss: 2.2436 - val_acc: 0.1692\n",
      "Epoch 307/500\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 2.1150 - acc: 0.1891 - val_loss: 2.2423 - val_acc: 0.1768\n",
      "Epoch 308/500\n",
      "1/1 [==============================] - 0s 224ms/step - loss: 2.1093 - acc: 0.1945 - val_loss: 2.2482 - val_acc: 0.1759\n",
      "Epoch 309/500\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 2.1102 - acc: 0.1869 - val_loss: 2.2511 - val_acc: 0.1663\n",
      "Epoch 310/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 2.1119 - acc: 0.1837 - val_loss: 2.2499 - val_acc: 0.1597\n",
      "Epoch 311/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 2.1077 - acc: 0.1832 - val_loss: 2.2403 - val_acc: 0.1549\n",
      "Epoch 312/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 2.1077 - acc: 0.1794 - val_loss: 2.2326 - val_acc: 0.1530\n",
      "Epoch 313/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 2.1076 - acc: 0.1842 - val_loss: 2.2264 - val_acc: 0.1578\n",
      "Epoch 314/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 2.1122 - acc: 0.1856 - val_loss: 2.2274 - val_acc: 0.1606\n",
      "Epoch 315/500\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 2.1034 - acc: 0.1899 - val_loss: 2.2361 - val_acc: 0.1606\n",
      "Epoch 316/500\n",
      "1/1 [==============================] - 0s 230ms/step - loss: 2.1110 - acc: 0.1848 - val_loss: 2.2456 - val_acc: 0.1606\n",
      "Epoch 317/500\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 2.1126 - acc: 0.1950 - val_loss: 2.2516 - val_acc: 0.1625\n",
      "Epoch 318/500\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 2.1112 - acc: 0.1885 - val_loss: 2.2495 - val_acc: 0.1511\n",
      "Epoch 319/500\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 2.1036 - acc: 0.1775 - val_loss: 2.2455 - val_acc: 0.1511\n",
      "Epoch 320/500\n",
      "1/1 [==============================] - 0s 236ms/step - loss: 2.1097 - acc: 0.1794 - val_loss: 2.2425 - val_acc: 0.1597\n",
      "Epoch 321/500\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 2.1107 - acc: 0.1807 - val_loss: 2.2428 - val_acc: 0.1625\n",
      "Epoch 322/500\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 2.1020 - acc: 0.1807 - val_loss: 2.2564 - val_acc: 0.1644\n",
      "Epoch 323/500\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 2.1090 - acc: 0.1853 - val_loss: 2.2611 - val_acc: 0.1606\n",
      "Epoch 324/500\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 2.1098 - acc: 0.1794 - val_loss: 2.2618 - val_acc: 0.1587\n",
      "Epoch 325/500\n",
      "1/1 [==============================] - 0s 183ms/step - loss: 2.1028 - acc: 0.1789 - val_loss: 2.2533 - val_acc: 0.1654\n",
      "Epoch 326/500\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 2.1082 - acc: 0.1883 - val_loss: 2.2443 - val_acc: 0.1673\n",
      "Epoch 327/500\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 2.1033 - acc: 0.1883 - val_loss: 2.2336 - val_acc: 0.1673\n",
      "Epoch 328/500\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 2.1039 - acc: 0.1885 - val_loss: 2.2287 - val_acc: 0.1683\n",
      "Epoch 329/500\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 2.1031 - acc: 0.1880 - val_loss: 2.2420 - val_acc: 0.1625\n",
      "Epoch 330/500\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 2.1077 - acc: 0.1840 - val_loss: 2.2558 - val_acc: 0.1407\n",
      "Epoch 331/500\n",
      "1/1 [==============================] - 0s 254ms/step - loss: 2.1065 - acc: 0.1815 - val_loss: 2.2622 - val_acc: 0.1416\n",
      "Epoch 332/500\n",
      "1/1 [==============================] - 0s 254ms/step - loss: 2.1095 - acc: 0.1826 - val_loss: 2.2575 - val_acc: 0.1635\n",
      "Epoch 333/500\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 2.0995 - acc: 0.1848 - val_loss: 2.2567 - val_acc: 0.1721\n",
      "Epoch 334/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 2.1128 - acc: 0.1848 - val_loss: 2.2476 - val_acc: 0.1778\n",
      "Epoch 335/500\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 2.1003 - acc: 0.1996 - val_loss: 2.2439 - val_acc: 0.1806\n",
      "Epoch 336/500\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 2.1152 - acc: 0.1872 - val_loss: 2.2494 - val_acc: 0.1787\n",
      "Epoch 337/500\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 2.1058 - acc: 0.1861 - val_loss: 2.2604 - val_acc: 0.1740\n",
      "Epoch 338/500\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 2.1063 - acc: 0.1845 - val_loss: 2.2738 - val_acc: 0.1730\n",
      "Epoch 339/500\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 2.1066 - acc: 0.1813 - val_loss: 2.2819 - val_acc: 0.1749\n",
      "Epoch 340/500\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 2.1092 - acc: 0.1864 - val_loss: 2.2721 - val_acc: 0.1749\n",
      "Epoch 341/500\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 2.0951 - acc: 0.1861 - val_loss: 2.2603 - val_acc: 0.1778\n",
      "Epoch 342/500\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 2.1087 - acc: 0.1746 - val_loss: 2.2508 - val_acc: 0.1806\n",
      "Epoch 343/500\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 2.1032 - acc: 0.1818 - val_loss: 2.2544 - val_acc: 0.1835\n",
      "Epoch 344/500\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 2.1040 - acc: 0.1942 - val_loss: 2.2694 - val_acc: 0.1711\n",
      "Epoch 345/500\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 2.1100 - acc: 0.1824 - val_loss: 2.2854 - val_acc: 0.1511\n",
      "Epoch 346/500\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 2.1042 - acc: 0.1746 - val_loss: 2.2955 - val_acc: 0.1388\n",
      "Epoch 347/500\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 2.1077 - acc: 0.1845 - val_loss: 2.2882 - val_acc: 0.1435\n",
      "Epoch 348/500\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 2.1080 - acc: 0.1719 - val_loss: 2.2701 - val_acc: 0.1625\n",
      "Epoch 349/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 2.0987 - acc: 0.1928 - val_loss: 2.2572 - val_acc: 0.1740\n",
      "Epoch 350/500\n",
      "1/1 [==============================] - 0s 228ms/step - loss: 2.1011 - acc: 0.1861 - val_loss: 2.2535 - val_acc: 0.1683\n",
      "Epoch 351/500\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 2.1034 - acc: 0.1947 - val_loss: 2.2555 - val_acc: 0.1606\n",
      "Epoch 352/500\n",
      "1/1 [==============================] - 0s 230ms/step - loss: 2.1071 - acc: 0.1867 - val_loss: 2.2625 - val_acc: 0.1454\n",
      "Epoch 353/500\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 2.0996 - acc: 0.1980 - val_loss: 2.2714 - val_acc: 0.1321\n",
      "Epoch 354/500\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 2.1076 - acc: 0.1789 - val_loss: 2.2705 - val_acc: 0.1302\n",
      "Epoch 355/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 2.1094 - acc: 0.1832 - val_loss: 2.2634 - val_acc: 0.1407\n",
      "Epoch 356/500\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 2.1033 - acc: 0.1840 - val_loss: 2.2607 - val_acc: 0.1511\n",
      "Epoch 357/500\n",
      "1/1 [==============================] - 0s 231ms/step - loss: 2.1104 - acc: 0.1818 - val_loss: 2.2661 - val_acc: 0.1568\n",
      "Epoch 358/500\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 2.1035 - acc: 0.1813 - val_loss: 2.2729 - val_acc: 0.1578\n",
      "Epoch 359/500\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 2.1061 - acc: 0.1950 - val_loss: 2.2855 - val_acc: 0.1473\n",
      "Epoch 360/500\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 2.0972 - acc: 0.1867 - val_loss: 2.2924 - val_acc: 0.1388\n",
      "Epoch 361/500\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 2.0956 - acc: 0.1802 - val_loss: 2.2940 - val_acc: 0.1359\n",
      "Epoch 362/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 2.1095 - acc: 0.1799 - val_loss: 2.2931 - val_acc: 0.1397\n",
      "Epoch 363/500\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 2.1066 - acc: 0.1907 - val_loss: 2.2880 - val_acc: 0.1530\n",
      "Epoch 364/500\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 2.1044 - acc: 0.1885 - val_loss: 2.2801 - val_acc: 0.1683\n",
      "Epoch 365/500\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 2.1001 - acc: 0.1891 - val_loss: 2.2821 - val_acc: 0.1692\n",
      "Epoch 366/500\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 2.1003 - acc: 0.1859 - val_loss: 2.2888 - val_acc: 0.1606\n",
      "Epoch 367/500\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 2.1106 - acc: 0.1840 - val_loss: 2.2854 - val_acc: 0.1568\n",
      "Epoch 368/500\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 2.1109 - acc: 0.1920 - val_loss: 2.2836 - val_acc: 0.1578\n",
      "Epoch 369/500\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 2.1020 - acc: 0.1867 - val_loss: 2.2827 - val_acc: 0.1606\n",
      "Epoch 370/500\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 2.1044 - acc: 0.1794 - val_loss: 2.2786 - val_acc: 0.1625\n",
      "Epoch 371/500\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 2.1011 - acc: 0.1864 - val_loss: 2.2852 - val_acc: 0.1692\n",
      "Epoch 372/500\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 2.1022 - acc: 0.1840 - val_loss: 2.2921 - val_acc: 0.1740\n",
      "Epoch 373/500\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 2.1084 - acc: 0.1848 - val_loss: 2.2953 - val_acc: 0.1711\n",
      "Epoch 374/500\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 2.0997 - acc: 0.2049 - val_loss: 2.2943 - val_acc: 0.1740\n",
      "Epoch 375/500\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 2.1004 - acc: 0.1880 - val_loss: 2.2900 - val_acc: 0.1730\n",
      "Epoch 376/500\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 2.0982 - acc: 0.1945 - val_loss: 2.2851 - val_acc: 0.1692\n",
      "Epoch 377/500\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 2.1137 - acc: 0.1888 - val_loss: 2.2662 - val_acc: 0.1730\n",
      "Epoch 378/500\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 2.1017 - acc: 0.1920 - val_loss: 2.2631 - val_acc: 0.1711\n",
      "Epoch 379/500\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 2.1076 - acc: 0.1928 - val_loss: 2.2638 - val_acc: 0.1721\n",
      "Epoch 380/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 2.0983 - acc: 0.2009 - val_loss: 2.2745 - val_acc: 0.1768\n",
      "Epoch 381/500\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 2.1063 - acc: 0.1880 - val_loss: 2.2824 - val_acc: 0.1730\n",
      "Epoch 382/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 2.0975 - acc: 0.1945 - val_loss: 2.2817 - val_acc: 0.1749\n",
      "Epoch 383/500\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 2.0993 - acc: 0.1907 - val_loss: 2.2751 - val_acc: 0.1797\n",
      "Epoch 384/500\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 2.1065 - acc: 0.1893 - val_loss: 2.2640 - val_acc: 0.1806\n",
      "Epoch 385/500\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 2.1044 - acc: 0.1904 - val_loss: 2.2551 - val_acc: 0.1797\n",
      "Epoch 386/500\n",
      "1/1 [==============================] - 0s 191ms/step - loss: 2.1016 - acc: 0.1891 - val_loss: 2.2521 - val_acc: 0.1778\n",
      "Epoch 387/500\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 2.1014 - acc: 0.1990 - val_loss: 2.2563 - val_acc: 0.1816\n",
      "Epoch 388/500\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 2.1017 - acc: 0.1947 - val_loss: 2.2684 - val_acc: 0.1816\n",
      "Epoch 389/500\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 2.1031 - acc: 0.1896 - val_loss: 2.2788 - val_acc: 0.1797\n",
      "Epoch 390/500\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 2.0966 - acc: 0.1891 - val_loss: 2.2861 - val_acc: 0.1768\n",
      "Epoch 391/500\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 2.0992 - acc: 0.1859 - val_loss: 2.2876 - val_acc: 0.1778\n",
      "Epoch 392/500\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 2.0968 - acc: 0.1872 - val_loss: 2.2848 - val_acc: 0.1797\n",
      "Epoch 393/500\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 2.1024 - acc: 0.1853 - val_loss: 2.2708 - val_acc: 0.1778\n",
      "Epoch 394/500\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 2.0956 - acc: 0.1885 - val_loss: 2.2602 - val_acc: 0.1797\n",
      "Epoch 395/500\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 2.1048 - acc: 0.1885 - val_loss: 2.2604 - val_acc: 0.1749\n",
      "Epoch 396/500\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 2.1054 - acc: 0.1859 - val_loss: 2.2666 - val_acc: 0.1740\n",
      "Epoch 397/500\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 2.0972 - acc: 0.1937 - val_loss: 2.2785 - val_acc: 0.1759\n",
      "Epoch 398/500\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 2.1075 - acc: 0.1802 - val_loss: 2.2891 - val_acc: 0.1778\n",
      "Epoch 399/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 2.0971 - acc: 0.1869 - val_loss: 2.2923 - val_acc: 0.1740\n",
      "Epoch 400/500\n",
      "1/1 [==============================] - 0s 233ms/step - loss: 2.1001 - acc: 0.1907 - val_loss: 2.2915 - val_acc: 0.1749\n",
      "Epoch 401/500\n",
      "1/1 [==============================] - 0s 239ms/step - loss: 2.1034 - acc: 0.1869 - val_loss: 2.2820 - val_acc: 0.1768\n",
      "Epoch 402/500\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 2.1016 - acc: 0.1883 - val_loss: 2.2715 - val_acc: 0.1759\n",
      "Epoch 403/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 2.1034 - acc: 0.1947 - val_loss: 2.2660 - val_acc: 0.1730\n",
      "Epoch 404/500\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 2.1005 - acc: 0.1896 - val_loss: 2.2662 - val_acc: 0.1768\n",
      "Epoch 405/500\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 2.0928 - acc: 0.1947 - val_loss: 2.2750 - val_acc: 0.1730\n",
      "Epoch 406/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 2.0960 - acc: 0.1928 - val_loss: 2.2821 - val_acc: 0.1759\n",
      "Epoch 407/500\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 2.0950 - acc: 0.1966 - val_loss: 2.2951 - val_acc: 0.1759\n",
      "Epoch 408/500\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 2.1029 - acc: 0.1850 - val_loss: 2.3005 - val_acc: 0.1768\n",
      "Epoch 409/500\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 2.0939 - acc: 0.1907 - val_loss: 2.3039 - val_acc: 0.1787\n",
      "Epoch 410/500\n",
      "1/1 [==============================] - 0s 226ms/step - loss: 2.0970 - acc: 0.1980 - val_loss: 2.3052 - val_acc: 0.1797\n",
      "Epoch 411/500\n",
      "1/1 [==============================] - 0s 236ms/step - loss: 2.0976 - acc: 0.1939 - val_loss: 2.3055 - val_acc: 0.1768\n",
      "Epoch 412/500\n",
      "1/1 [==============================] - 0s 233ms/step - loss: 2.1010 - acc: 0.1982 - val_loss: 2.3080 - val_acc: 0.1759\n",
      "Epoch 413/500\n",
      "1/1 [==============================] - 0s 227ms/step - loss: 2.1059 - acc: 0.1848 - val_loss: 2.3088 - val_acc: 0.1768\n",
      "Epoch 414/500\n",
      "1/1 [==============================] - 0s 233ms/step - loss: 2.1000 - acc: 0.1910 - val_loss: 2.3032 - val_acc: 0.1749\n",
      "Epoch 415/500\n",
      "1/1 [==============================] - 0s 237ms/step - loss: 2.0995 - acc: 0.1998 - val_loss: 2.2962 - val_acc: 0.1759\n",
      "Epoch 416/500\n",
      "1/1 [==============================] - 0s 227ms/step - loss: 2.0991 - acc: 0.1966 - val_loss: 2.2951 - val_acc: 0.1759\n",
      "Epoch 417/500\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 2.0959 - acc: 0.1867 - val_loss: 2.2994 - val_acc: 0.1759\n",
      "Epoch 418/500\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 2.0967 - acc: 0.1966 - val_loss: 2.3048 - val_acc: 0.1749\n",
      "Epoch 419/500\n",
      "1/1 [==============================] - 0s 252ms/step - loss: 2.0995 - acc: 0.1840 - val_loss: 2.3073 - val_acc: 0.1759\n",
      "Epoch 420/500\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 2.0954 - acc: 0.1885 - val_loss: 2.3079 - val_acc: 0.1749\n",
      "Epoch 421/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 2.0964 - acc: 0.1990 - val_loss: 2.3091 - val_acc: 0.1730\n",
      "Epoch 422/500\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 2.0889 - acc: 0.2031 - val_loss: 2.3130 - val_acc: 0.1730\n",
      "Epoch 423/500\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 2.0915 - acc: 0.1950 - val_loss: 2.3210 - val_acc: 0.1740\n",
      "Epoch 424/500\n",
      "1/1 [==============================] - 0s 226ms/step - loss: 2.1006 - acc: 0.1875 - val_loss: 2.3137 - val_acc: 0.1740\n",
      "Epoch 425/500\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 2.0981 - acc: 0.1815 - val_loss: 2.3125 - val_acc: 0.1711\n",
      "Epoch 426/500\n",
      "1/1 [==============================] - 0s 229ms/step - loss: 2.0987 - acc: 0.1966 - val_loss: 2.3134 - val_acc: 0.1740\n",
      "Epoch 427/500\n",
      "1/1 [==============================] - 0s 225ms/step - loss: 2.0987 - acc: 0.1915 - val_loss: 2.3026 - val_acc: 0.1768\n",
      "Epoch 428/500\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 2.0941 - acc: 0.1920 - val_loss: 2.3004 - val_acc: 0.1768\n",
      "Epoch 429/500\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 2.0998 - acc: 0.1880 - val_loss: 2.2961 - val_acc: 0.1749\n",
      "Epoch 430/500\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 2.0940 - acc: 0.1853 - val_loss: 2.3010 - val_acc: 0.1768\n",
      "Epoch 431/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 2.0929 - acc: 0.1937 - val_loss: 2.3096 - val_acc: 0.1711\n",
      "Epoch 432/500\n",
      "1/1 [==============================] - 0s 226ms/step - loss: 2.0915 - acc: 0.1996 - val_loss: 2.3205 - val_acc: 0.1692\n",
      "Epoch 433/500\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 2.0959 - acc: 0.2004 - val_loss: 2.3440 - val_acc: 0.1692\n",
      "Epoch 434/500\n",
      "1/1 [==============================] - 0s 240ms/step - loss: 2.0972 - acc: 0.1912 - val_loss: 2.3525 - val_acc: 0.1692\n",
      "Epoch 435/500\n",
      "1/1 [==============================] - 0s 239ms/step - loss: 2.0955 - acc: 0.1988 - val_loss: 2.3442 - val_acc: 0.1692\n",
      "Epoch 436/500\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 2.1023 - acc: 0.1877 - val_loss: 2.3287 - val_acc: 0.1740\n",
      "Epoch 437/500\n",
      "1/1 [==============================] - 0s 372ms/step - loss: 2.0998 - acc: 0.1934 - val_loss: 2.3107 - val_acc: 0.1692\n",
      "Epoch 438/500\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 2.0936 - acc: 0.1937 - val_loss: 2.2995 - val_acc: 0.1702\n",
      "Epoch 439/500\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 2.0950 - acc: 0.1902 - val_loss: 2.2947 - val_acc: 0.1673\n",
      "Epoch 440/500\n",
      "1/1 [==============================] - 0s 343ms/step - loss: 2.0960 - acc: 0.1950 - val_loss: 2.2931 - val_acc: 0.1692\n",
      "Epoch 441/500\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 2.0943 - acc: 0.1861 - val_loss: 2.2931 - val_acc: 0.1692\n",
      "Epoch 442/500\n",
      "1/1 [==============================] - 0s 240ms/step - loss: 2.0951 - acc: 0.1955 - val_loss: 2.2942 - val_acc: 0.1721\n",
      "Epoch 443/500\n",
      "1/1 [==============================] - 0s 253ms/step - loss: 2.1024 - acc: 0.1853 - val_loss: 2.2914 - val_acc: 0.1711\n",
      "Epoch 444/500\n",
      "1/1 [==============================] - 0s 239ms/step - loss: 2.0963 - acc: 0.1902 - val_loss: 2.2865 - val_acc: 0.1721\n",
      "Epoch 445/500\n",
      "1/1 [==============================] - 0s 230ms/step - loss: 2.0997 - acc: 0.1872 - val_loss: 2.2874 - val_acc: 0.1730\n",
      "Epoch 446/500\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 2.0902 - acc: 0.1902 - val_loss: 2.2912 - val_acc: 0.1683\n",
      "Epoch 447/500\n",
      "1/1 [==============================] - 0s 226ms/step - loss: 2.1005 - acc: 0.1920 - val_loss: 2.2963 - val_acc: 0.1702\n",
      "Epoch 448/500\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 2.0948 - acc: 0.1904 - val_loss: 2.2949 - val_acc: 0.1702\n",
      "Epoch 449/500\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 2.0983 - acc: 0.1899 - val_loss: 2.2963 - val_acc: 0.1702\n",
      "Epoch 450/500\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 2.0984 - acc: 0.1910 - val_loss: 2.2949 - val_acc: 0.1673\n",
      "Epoch 451/500\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 2.0924 - acc: 0.1955 - val_loss: 2.3011 - val_acc: 0.1654\n",
      "Epoch 452/500\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 2.0941 - acc: 0.1955 - val_loss: 2.3113 - val_acc: 0.1673\n",
      "Epoch 453/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 2.0909 - acc: 0.1928 - val_loss: 2.3212 - val_acc: 0.1683\n",
      "Epoch 454/500\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 2.0977 - acc: 0.1902 - val_loss: 2.3266 - val_acc: 0.1692\n",
      "Epoch 455/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 2.0991 - acc: 0.1888 - val_loss: 2.3184 - val_acc: 0.1663\n",
      "Epoch 456/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 2.0950 - acc: 0.2025 - val_loss: 2.3061 - val_acc: 0.1683\n",
      "Epoch 457/500\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 2.0955 - acc: 0.1904 - val_loss: 2.2929 - val_acc: 0.1663\n",
      "Epoch 458/500\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 2.0958 - acc: 0.1934 - val_loss: 2.2798 - val_acc: 0.1654\n",
      "Epoch 459/500\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 2.0913 - acc: 0.1939 - val_loss: 2.2822 - val_acc: 0.1654\n",
      "Epoch 460/500\n",
      "1/1 [==============================] - 0s 227ms/step - loss: 2.0951 - acc: 0.1955 - val_loss: 2.2941 - val_acc: 0.1663\n",
      "Epoch 461/500\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 2.0978 - acc: 0.1810 - val_loss: 2.3149 - val_acc: 0.1663\n",
      "Epoch 462/500\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 2.0985 - acc: 0.1899 - val_loss: 2.3325 - val_acc: 0.1673\n",
      "Epoch 463/500\n",
      "1/1 [==============================] - 0s 225ms/step - loss: 2.0947 - acc: 0.1877 - val_loss: 2.3408 - val_acc: 0.1616\n",
      "Epoch 464/500\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 2.0901 - acc: 0.1969 - val_loss: 2.3441 - val_acc: 0.1587\n",
      "Epoch 465/500\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 2.0901 - acc: 0.2004 - val_loss: 2.3434 - val_acc: 0.1606\n",
      "Epoch 466/500\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 2.0960 - acc: 0.1945 - val_loss: 2.3290 - val_acc: 0.1616\n",
      "Epoch 467/500\n",
      "1/1 [==============================] - 0s 227ms/step - loss: 2.0963 - acc: 0.1963 - val_loss: 2.3168 - val_acc: 0.1654\n",
      "Epoch 468/500\n",
      "1/1 [==============================] - 0s 225ms/step - loss: 2.0966 - acc: 0.1934 - val_loss: 2.3059 - val_acc: 0.1683\n",
      "Epoch 469/500\n",
      "1/1 [==============================] - 0s 228ms/step - loss: 2.0889 - acc: 0.1985 - val_loss: 2.3053 - val_acc: 0.1692\n",
      "Epoch 470/500\n",
      "1/1 [==============================] - 0s 225ms/step - loss: 2.0912 - acc: 0.1939 - val_loss: 2.3173 - val_acc: 0.1692\n",
      "Epoch 471/500\n",
      "1/1 [==============================] - 0s 226ms/step - loss: 2.0902 - acc: 0.1923 - val_loss: 2.3334 - val_acc: 0.1692\n",
      "Epoch 472/500\n",
      "1/1 [==============================] - 0s 243ms/step - loss: 2.0900 - acc: 0.1950 - val_loss: 2.3474 - val_acc: 0.1692\n",
      "Epoch 473/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 2.0952 - acc: 0.1937 - val_loss: 2.3540 - val_acc: 0.1663\n",
      "Epoch 474/500\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 2.0966 - acc: 0.1942 - val_loss: 2.3362 - val_acc: 0.1663\n",
      "Epoch 475/500\n",
      "1/1 [==============================] - 0s 257ms/step - loss: 2.0972 - acc: 0.1920 - val_loss: 2.3213 - val_acc: 0.1654\n",
      "Epoch 476/500\n",
      "1/1 [==============================] - 0s 233ms/step - loss: 2.0898 - acc: 0.1928 - val_loss: 2.3114 - val_acc: 0.1616\n",
      "Epoch 477/500\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 2.0961 - acc: 0.2004 - val_loss: 2.3024 - val_acc: 0.1606\n",
      "Epoch 478/500\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 2.0936 - acc: 0.1963 - val_loss: 2.3040 - val_acc: 0.1606\n",
      "Epoch 479/500\n",
      "1/1 [==============================] - 0s 192ms/step - loss: 2.1008 - acc: 0.1891 - val_loss: 2.3049 - val_acc: 0.1635\n",
      "Epoch 480/500\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 2.0897 - acc: 0.2001 - val_loss: 2.3101 - val_acc: 0.1635\n",
      "Epoch 481/500\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 2.0949 - acc: 0.1955 - val_loss: 2.3144 - val_acc: 0.1625\n",
      "Epoch 482/500\n",
      "1/1 [==============================] - 0s 186ms/step - loss: 2.1012 - acc: 0.2017 - val_loss: 2.3089 - val_acc: 0.1644\n",
      "Epoch 483/500\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 2.0872 - acc: 0.1955 - val_loss: 2.3048 - val_acc: 0.1644\n",
      "Epoch 484/500\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 2.0967 - acc: 0.1942 - val_loss: 2.2996 - val_acc: 0.1635\n",
      "Epoch 485/500\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 2.0958 - acc: 0.1945 - val_loss: 2.3002 - val_acc: 0.1635\n",
      "Epoch 486/500\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 2.0884 - acc: 0.1961 - val_loss: 2.3125 - val_acc: 0.1635\n",
      "Epoch 487/500\n",
      "1/1 [==============================] - 0s 227ms/step - loss: 2.0913 - acc: 0.1963 - val_loss: 2.3210 - val_acc: 0.1606\n",
      "Epoch 488/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 2.0885 - acc: 0.1988 - val_loss: 2.3266 - val_acc: 0.1606\n",
      "Epoch 489/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 2.0923 - acc: 0.1953 - val_loss: 2.3290 - val_acc: 0.1587\n",
      "Epoch 490/500\n",
      "1/1 [==============================] - 0s 243ms/step - loss: 2.0978 - acc: 0.1848 - val_loss: 2.3231 - val_acc: 0.1578\n",
      "Epoch 491/500\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 2.0903 - acc: 0.1961 - val_loss: 2.3209 - val_acc: 0.1587\n",
      "Epoch 492/500\n",
      "1/1 [==============================] - 0s 237ms/step - loss: 2.0910 - acc: 0.1939 - val_loss: 2.3172 - val_acc: 0.1597\n",
      "Epoch 493/500\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 2.0980 - acc: 0.1953 - val_loss: 2.3135 - val_acc: 0.1587\n",
      "Epoch 494/500\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 2.0914 - acc: 0.1980 - val_loss: 2.3076 - val_acc: 0.1606\n",
      "Epoch 495/500\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 2.0899 - acc: 0.1977 - val_loss: 2.3091 - val_acc: 0.1654\n",
      "Epoch 496/500\n",
      "1/1 [==============================] - 0s 197ms/step - loss: 2.0899 - acc: 0.1939 - val_loss: 2.3117 - val_acc: 0.1663\n",
      "Epoch 497/500\n",
      "1/1 [==============================] - 0s 184ms/step - loss: 2.0932 - acc: 0.1840 - val_loss: 2.3223 - val_acc: 0.1606\n",
      "Epoch 498/500\n",
      "1/1 [==============================] - 0s 181ms/step - loss: 2.0888 - acc: 0.1953 - val_loss: 2.3296 - val_acc: 0.1606\n",
      "Epoch 499/500\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 2.0876 - acc: 0.1971 - val_loss: 2.3273 - val_acc: 0.1597\n",
      "Epoch 500/500\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 2.0909 - acc: 0.1918 - val_loss: 2.3232 - val_acc: 0.1578\n"
     ]
    }
   ],
   "source": [
    "validation_text = []\n",
    "for i in range(len(valid_set)):\n",
    "    df = pd.read_csv(lab_files_path+valid_set[i])\n",
    "    Text = df['Object'].to_list()\n",
    "\n",
    "    for T in Text:\n",
    "        validation_text.append(T)\n",
    "\n",
    "\n",
    "valid_embeddings = []\n",
    "for t in valid_text:\n",
    "    valid_embeddings.append(Loaded_model.wv[t])\n",
    "    \n",
    "valid_embeddings = np.array(valid_embeddings)\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10)\n",
    "\n",
    "# print(\"Shape of A1 is: \", np.shape(A1))\n",
    "# print(\"Shape of embeddings is: \", np.shape(train_embeddings))\n",
    "# print(\"Shape of train embeddings is: \", np.shape(b1_text))\n",
    "\n",
    "\n",
    "# print(\"Shape of A4 is: \", np.shape(A4))\n",
    "# print(\"Shape of embeddings is: \", np.shape(valid_embeddings))\n",
    "# print(\"Shape of train embeddings is: \", np.shape(valid_text))\n",
    "\n",
    "history = model.fit([train_embeddings, A1],\n",
    "                    b1_encodings,\n",
    "                    epochs=500,\n",
    "                    batch_size=N,\n",
    "                    # class_weight=W,\n",
    "                    validation_data=([valid_embeddings, A4], valid_encodings),\n",
    "                    # callbacks=[callback]\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on Batch-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3680\n",
      "(3680, 100)\n",
      "(3680, 3680)\n",
      "Epoch 1/500\n",
      "1/1 [==============================] - 1s 692ms/step - loss: 2.1665 - acc: 0.1609 - val_loss: 2.2529 - val_acc: 0.1606\n",
      "Epoch 2/500\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 2.1589 - acc: 0.1590 - val_loss: 2.1847 - val_acc: 0.2034\n",
      "Epoch 3/500\n",
      "1/1 [==============================] - 0s 234ms/step - loss: 2.1491 - acc: 0.1595 - val_loss: 2.1475 - val_acc: 0.1635\n",
      "Epoch 4/500\n",
      "1/1 [==============================] - 0s 231ms/step - loss: 2.1457 - acc: 0.1505 - val_loss: 2.1330 - val_acc: 0.1492\n",
      "Epoch 5/500\n",
      "1/1 [==============================] - 0s 231ms/step - loss: 2.1437 - acc: 0.1565 - val_loss: 2.1281 - val_acc: 0.1483\n",
      "Epoch 6/500\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 2.1547 - acc: 0.1524 - val_loss: 2.1210 - val_acc: 0.1473\n",
      "Epoch 7/500\n",
      "1/1 [==============================] - 0s 228ms/step - loss: 2.1538 - acc: 0.1514 - val_loss: 2.1131 - val_acc: 0.1435\n",
      "Epoch 8/500\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 2.1613 - acc: 0.1503 - val_loss: 2.1065 - val_acc: 0.1283\n",
      "Epoch 9/500\n",
      "1/1 [==============================] - 0s 227ms/step - loss: 2.1499 - acc: 0.1568 - val_loss: 2.1092 - val_acc: 0.1369\n",
      "Epoch 10/500\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 2.1445 - acc: 0.1644 - val_loss: 2.1228 - val_acc: 0.1835\n",
      "Epoch 11/500\n",
      "1/1 [==============================] - 0s 230ms/step - loss: 2.1504 - acc: 0.1644 - val_loss: 2.1436 - val_acc: 0.2129\n",
      "Epoch 12/500\n",
      "1/1 [==============================] - 0s 240ms/step - loss: 2.1427 - acc: 0.1655 - val_loss: 2.1638 - val_acc: 0.1692\n",
      "Epoch 13/500\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 2.1455 - acc: 0.1685 - val_loss: 2.1687 - val_acc: 0.1702\n",
      "Epoch 14/500\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 2.1472 - acc: 0.1611 - val_loss: 2.1610 - val_acc: 0.1711\n",
      "Epoch 15/500\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 2.1508 - acc: 0.1625 - val_loss: 2.1433 - val_acc: 0.2120\n",
      "Epoch 16/500\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 2.1488 - acc: 0.1671 - val_loss: 2.1230 - val_acc: 0.2529\n",
      "Epoch 17/500\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 2.1412 - acc: 0.1644 - val_loss: 2.1112 - val_acc: 0.2500\n",
      "Epoch 18/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 2.1405 - acc: 0.1688 - val_loss: 2.1073 - val_acc: 0.2272\n",
      "Epoch 19/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 2.1433 - acc: 0.1625 - val_loss: 2.1102 - val_acc: 0.2253\n",
      "Epoch 20/500\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 2.1427 - acc: 0.1625 - val_loss: 2.1143 - val_acc: 0.2281\n",
      "Epoch 21/500\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 2.1417 - acc: 0.1739 - val_loss: 2.1178 - val_acc: 0.2376\n",
      "Epoch 22/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 2.1419 - acc: 0.1772 - val_loss: 2.1266 - val_acc: 0.2624\n",
      "Epoch 23/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 2.1396 - acc: 0.1655 - val_loss: 2.1383 - val_acc: 0.2643\n",
      "Epoch 24/500\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 2.1502 - acc: 0.1614 - val_loss: 2.1461 - val_acc: 0.2548\n",
      "Epoch 25/500\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 2.1447 - acc: 0.1758 - val_loss: 2.1504 - val_acc: 0.2500\n",
      "Epoch 26/500\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 2.1397 - acc: 0.1761 - val_loss: 2.1511 - val_acc: 0.2500\n",
      "Epoch 27/500\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 2.1432 - acc: 0.1674 - val_loss: 2.1486 - val_acc: 0.2567\n",
      "Epoch 28/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 2.1410 - acc: 0.1647 - val_loss: 2.1436 - val_acc: 0.2681\n",
      "Epoch 29/500\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 2.1367 - acc: 0.1780 - val_loss: 2.1402 - val_acc: 0.2595\n",
      "Epoch 30/500\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 2.1373 - acc: 0.1693 - val_loss: 2.1397 - val_acc: 0.2548\n",
      "Epoch 31/500\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 2.1383 - acc: 0.1753 - val_loss: 2.1377 - val_acc: 0.2490\n",
      "Epoch 32/500\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 2.1390 - acc: 0.1671 - val_loss: 2.1356 - val_acc: 0.2452\n",
      "Epoch 33/500\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 2.1459 - acc: 0.1690 - val_loss: 2.1325 - val_acc: 0.2614\n",
      "Epoch 34/500\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 2.1424 - acc: 0.1720 - val_loss: 2.1299 - val_acc: 0.2557\n",
      "Epoch 35/500\n",
      "1/1 [==============================] - 0s 248ms/step - loss: 2.1406 - acc: 0.1783 - val_loss: 2.1306 - val_acc: 0.2595\n",
      "Epoch 36/500\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 2.1383 - acc: 0.1726 - val_loss: 2.1361 - val_acc: 0.2633\n",
      "Epoch 37/500\n",
      "1/1 [==============================] - 0s 238ms/step - loss: 2.1357 - acc: 0.1720 - val_loss: 2.1410 - val_acc: 0.2624\n",
      "Epoch 38/500\n",
      "1/1 [==============================] - 0s 248ms/step - loss: 2.1436 - acc: 0.1668 - val_loss: 2.1423 - val_acc: 0.2614\n",
      "Epoch 39/500\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 2.1391 - acc: 0.1712 - val_loss: 2.1428 - val_acc: 0.2614\n",
      "Epoch 40/500\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 2.1357 - acc: 0.1685 - val_loss: 2.1398 - val_acc: 0.2700\n",
      "Epoch 41/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 2.1338 - acc: 0.1731 - val_loss: 2.1381 - val_acc: 0.2605\n",
      "Epoch 42/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 2.1408 - acc: 0.1698 - val_loss: 2.1358 - val_acc: 0.2614\n",
      "Epoch 43/500\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 2.1373 - acc: 0.1726 - val_loss: 2.1345 - val_acc: 0.2576\n",
      "Epoch 44/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 2.1347 - acc: 0.1783 - val_loss: 2.1321 - val_acc: 0.2567\n",
      "Epoch 45/500\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 2.1378 - acc: 0.1739 - val_loss: 2.1309 - val_acc: 0.2576\n",
      "Epoch 46/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 2.1373 - acc: 0.1810 - val_loss: 2.1293 - val_acc: 0.2614\n",
      "Epoch 47/500\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 2.1409 - acc: 0.1804 - val_loss: 2.1270 - val_acc: 0.2614\n",
      "Epoch 48/500\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 2.1405 - acc: 0.1745 - val_loss: 2.1263 - val_acc: 0.2681\n",
      "Epoch 49/500\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 2.1354 - acc: 0.1810 - val_loss: 2.1306 - val_acc: 0.2671\n",
      "Epoch 50/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 2.1445 - acc: 0.1663 - val_loss: 2.1306 - val_acc: 0.2595\n",
      "Epoch 51/500\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 2.1392 - acc: 0.1682 - val_loss: 2.1322 - val_acc: 0.2595\n",
      "Epoch 52/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 2.1385 - acc: 0.1783 - val_loss: 2.1343 - val_acc: 0.2614\n",
      "Epoch 53/500\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 2.1433 - acc: 0.1745 - val_loss: 2.1306 - val_acc: 0.2576\n",
      "Epoch 54/500\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 2.1364 - acc: 0.1736 - val_loss: 2.1241 - val_acc: 0.2548\n",
      "Epoch 55/500\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 2.1430 - acc: 0.1688 - val_loss: 2.1194 - val_acc: 0.2624\n",
      "Epoch 56/500\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 2.1399 - acc: 0.1758 - val_loss: 2.1166 - val_acc: 0.2605\n",
      "Epoch 57/500\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 2.1355 - acc: 0.1769 - val_loss: 2.1170 - val_acc: 0.2586\n",
      "Epoch 58/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 2.1306 - acc: 0.1688 - val_loss: 2.1205 - val_acc: 0.2576\n",
      "Epoch 59/500\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 2.1422 - acc: 0.1728 - val_loss: 2.1221 - val_acc: 0.2567\n",
      "Epoch 60/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 2.1366 - acc: 0.1793 - val_loss: 2.1240 - val_acc: 0.2567\n",
      "Epoch 61/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 2.1328 - acc: 0.1758 - val_loss: 2.1233 - val_acc: 0.2586\n",
      "Epoch 62/500\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 2.1390 - acc: 0.1726 - val_loss: 2.1208 - val_acc: 0.2586\n",
      "Epoch 63/500\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 2.1309 - acc: 0.1821 - val_loss: 2.1173 - val_acc: 0.2548\n",
      "Epoch 64/500\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 2.1326 - acc: 0.1726 - val_loss: 2.1154 - val_acc: 0.2557\n",
      "Epoch 65/500\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 2.1440 - acc: 0.1696 - val_loss: 2.1127 - val_acc: 0.2490\n",
      "Epoch 66/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 2.1410 - acc: 0.1726 - val_loss: 2.1145 - val_acc: 0.2576\n",
      "Epoch 67/500\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 2.1374 - acc: 0.1728 - val_loss: 2.1178 - val_acc: 0.2548\n",
      "Epoch 68/500\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 2.1355 - acc: 0.1750 - val_loss: 2.1201 - val_acc: 0.2605\n",
      "Epoch 69/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 2.1400 - acc: 0.1696 - val_loss: 2.1201 - val_acc: 0.2576\n",
      "Epoch 70/500\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 2.1373 - acc: 0.1804 - val_loss: 2.1222 - val_acc: 0.2567\n",
      "Epoch 71/500\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 2.1398 - acc: 0.1707 - val_loss: 2.1244 - val_acc: 0.2557\n",
      "Epoch 72/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 2.1370 - acc: 0.1745 - val_loss: 2.1288 - val_acc: 0.2595\n",
      "Epoch 73/500\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 2.1340 - acc: 0.1845 - val_loss: 2.1278 - val_acc: 0.2576\n",
      "Epoch 74/500\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 2.1331 - acc: 0.1720 - val_loss: 2.1256 - val_acc: 0.2557\n",
      "Epoch 75/500\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 2.1277 - acc: 0.1840 - val_loss: 2.1247 - val_acc: 0.2538\n",
      "Epoch 76/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 2.1325 - acc: 0.1758 - val_loss: 2.1224 - val_acc: 0.2481\n",
      "Epoch 77/500\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 2.1437 - acc: 0.1655 - val_loss: 2.1187 - val_acc: 0.2510\n",
      "Epoch 78/500\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 2.1330 - acc: 0.1851 - val_loss: 2.1132 - val_acc: 0.2471\n",
      "Epoch 79/500\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 2.1413 - acc: 0.1758 - val_loss: 2.1067 - val_acc: 0.2433\n",
      "Epoch 80/500\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 2.1408 - acc: 0.1647 - val_loss: 2.1016 - val_acc: 0.2452\n",
      "Epoch 81/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 2.1385 - acc: 0.1677 - val_loss: 2.1012 - val_acc: 0.2500\n",
      "Epoch 82/500\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 2.1403 - acc: 0.1774 - val_loss: 2.1074 - val_acc: 0.2529\n",
      "Epoch 83/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 2.1392 - acc: 0.1671 - val_loss: 2.1148 - val_acc: 0.2614\n",
      "Epoch 84/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 2.1372 - acc: 0.1731 - val_loss: 2.1218 - val_acc: 0.2605\n",
      "Epoch 85/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 2.1375 - acc: 0.1810 - val_loss: 2.1248 - val_acc: 0.2576\n",
      "Epoch 86/500\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 2.1377 - acc: 0.1731 - val_loss: 2.1238 - val_acc: 0.2567\n",
      "Epoch 87/500\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 2.1364 - acc: 0.1834 - val_loss: 2.1177 - val_acc: 0.2576\n",
      "Epoch 88/500\n",
      "1/1 [==============================] - 0s 224ms/step - loss: 2.1363 - acc: 0.1813 - val_loss: 2.1103 - val_acc: 0.2595\n",
      "Epoch 89/500\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 2.1341 - acc: 0.1777 - val_loss: 2.1062 - val_acc: 0.2548\n",
      "Epoch 90/500\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 2.1390 - acc: 0.1715 - val_loss: 2.1044 - val_acc: 0.2471\n",
      "Epoch 91/500\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 2.1349 - acc: 0.1753 - val_loss: 2.1052 - val_acc: 0.2548\n",
      "Epoch 92/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 2.1431 - acc: 0.1815 - val_loss: 2.1067 - val_acc: 0.2519\n",
      "Epoch 93/500\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 2.1358 - acc: 0.1791 - val_loss: 2.1084 - val_acc: 0.2414\n",
      "Epoch 94/500\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 2.1324 - acc: 0.1791 - val_loss: 2.1110 - val_acc: 0.2471\n",
      "Epoch 95/500\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 2.1394 - acc: 0.1747 - val_loss: 2.1158 - val_acc: 0.2452\n",
      "Epoch 96/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 2.1376 - acc: 0.1696 - val_loss: 2.1183 - val_acc: 0.2538\n",
      "Epoch 97/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 2.1383 - acc: 0.1769 - val_loss: 2.1163 - val_acc: 0.2557\n",
      "Epoch 98/500\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 2.1334 - acc: 0.1728 - val_loss: 2.1191 - val_acc: 0.2643\n",
      "Epoch 99/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 2.1314 - acc: 0.1894 - val_loss: 2.1186 - val_acc: 0.2614\n",
      "Epoch 100/500\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 2.1388 - acc: 0.1693 - val_loss: 2.1164 - val_acc: 0.2576\n",
      "Epoch 101/500\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 2.1329 - acc: 0.1769 - val_loss: 2.1115 - val_acc: 0.2605\n",
      "Epoch 102/500\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 2.1364 - acc: 0.1818 - val_loss: 2.1044 - val_acc: 0.2586\n",
      "Epoch 103/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 2.1365 - acc: 0.1804 - val_loss: 2.0979 - val_acc: 0.2452\n",
      "Epoch 104/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 2.1385 - acc: 0.1614 - val_loss: 2.0968 - val_acc: 0.2367\n",
      "Epoch 105/500\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 2.1347 - acc: 0.1739 - val_loss: 2.0974 - val_acc: 0.2319\n",
      "Epoch 106/500\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 2.1374 - acc: 0.1747 - val_loss: 2.0989 - val_acc: 0.2357\n",
      "Epoch 107/500\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 2.1384 - acc: 0.1780 - val_loss: 2.1014 - val_acc: 0.2462\n",
      "Epoch 108/500\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 2.1339 - acc: 0.1755 - val_loss: 2.1018 - val_acc: 0.2557\n",
      "Epoch 109/500\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 2.1375 - acc: 0.1889 - val_loss: 2.1018 - val_acc: 0.2557\n",
      "Epoch 110/500\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 2.1357 - acc: 0.1810 - val_loss: 2.0996 - val_acc: 0.2605\n",
      "Epoch 111/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 2.1342 - acc: 0.1791 - val_loss: 2.0959 - val_acc: 0.2567\n",
      "Epoch 112/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 2.1352 - acc: 0.1712 - val_loss: 2.0923 - val_acc: 0.2548\n",
      "Epoch 113/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 2.1335 - acc: 0.1799 - val_loss: 2.0915 - val_acc: 0.2538\n",
      "Epoch 114/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 2.1339 - acc: 0.1734 - val_loss: 2.0906 - val_acc: 0.2624\n",
      "Epoch 115/500\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 2.1322 - acc: 0.1766 - val_loss: 2.0910 - val_acc: 0.2576\n",
      "Epoch 116/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 2.1351 - acc: 0.1677 - val_loss: 2.0922 - val_acc: 0.2605\n",
      "Epoch 117/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 2.1318 - acc: 0.1772 - val_loss: 2.0917 - val_acc: 0.2586\n",
      "Epoch 118/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 2.1375 - acc: 0.1677 - val_loss: 2.0884 - val_acc: 0.2605\n",
      "Epoch 119/500\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 2.1309 - acc: 0.1886 - val_loss: 2.0884 - val_acc: 0.2681\n",
      "Epoch 120/500\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 2.1330 - acc: 0.1742 - val_loss: 2.0877 - val_acc: 0.2424\n",
      "Epoch 121/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 2.1372 - acc: 0.1859 - val_loss: 2.0849 - val_acc: 0.2452\n",
      "Epoch 122/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 2.1342 - acc: 0.1690 - val_loss: 2.0833 - val_acc: 0.2471\n",
      "Epoch 123/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 2.1425 - acc: 0.1630 - val_loss: 2.0838 - val_acc: 0.2576\n",
      "Epoch 124/500\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 2.1305 - acc: 0.1796 - val_loss: 2.0860 - val_acc: 0.2586\n",
      "Epoch 125/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 2.1321 - acc: 0.1745 - val_loss: 2.0887 - val_acc: 0.2643\n",
      "Epoch 126/500\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 2.1333 - acc: 0.1834 - val_loss: 2.0888 - val_acc: 0.2652\n",
      "Epoch 127/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 2.1322 - acc: 0.1853 - val_loss: 2.0871 - val_acc: 0.2633\n",
      "Epoch 128/500\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 2.1327 - acc: 0.1666 - val_loss: 2.0875 - val_acc: 0.2576\n",
      "Epoch 129/500\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 2.1367 - acc: 0.1851 - val_loss: 2.0893 - val_acc: 0.2519\n",
      "Epoch 130/500\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 2.1334 - acc: 0.1826 - val_loss: 2.0903 - val_acc: 0.2471\n",
      "Epoch 131/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 2.1337 - acc: 0.1864 - val_loss: 2.0902 - val_acc: 0.2595\n",
      "Epoch 132/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 2.1271 - acc: 0.1899 - val_loss: 2.0903 - val_acc: 0.2576\n",
      "Epoch 133/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 2.1328 - acc: 0.1845 - val_loss: 2.0902 - val_acc: 0.2614\n",
      "Epoch 134/500\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 2.1316 - acc: 0.1761 - val_loss: 2.0897 - val_acc: 0.2662\n",
      "Epoch 135/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 2.1323 - acc: 0.1802 - val_loss: 2.0886 - val_acc: 0.2681\n",
      "Epoch 136/500\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 2.1304 - acc: 0.1821 - val_loss: 2.0870 - val_acc: 0.2643\n",
      "Epoch 137/500\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 2.1404 - acc: 0.1742 - val_loss: 2.0856 - val_acc: 0.2605\n",
      "Epoch 138/500\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 2.1311 - acc: 0.1755 - val_loss: 2.0872 - val_acc: 0.2452\n",
      "Epoch 139/500\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 2.1330 - acc: 0.1791 - val_loss: 2.0903 - val_acc: 0.2405\n",
      "Epoch 140/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 2.1347 - acc: 0.1880 - val_loss: 2.0914 - val_acc: 0.2300\n",
      "Epoch 141/500\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 2.1374 - acc: 0.1840 - val_loss: 2.0886 - val_acc: 0.2433\n",
      "Epoch 142/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 2.1268 - acc: 0.1905 - val_loss: 2.0871 - val_acc: 0.2462\n",
      "Epoch 143/500\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 2.1343 - acc: 0.1804 - val_loss: 2.0866 - val_acc: 0.2462\n",
      "Epoch 144/500\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 2.1307 - acc: 0.1804 - val_loss: 2.0846 - val_acc: 0.2586\n",
      "Epoch 145/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 2.1350 - acc: 0.1815 - val_loss: 2.0837 - val_acc: 0.2595\n",
      "Epoch 146/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 2.1330 - acc: 0.1818 - val_loss: 2.0813 - val_acc: 0.2557\n",
      "Epoch 147/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 2.1341 - acc: 0.1742 - val_loss: 2.0787 - val_acc: 0.2462\n",
      "Epoch 148/500\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 2.1331 - acc: 0.1813 - val_loss: 2.0779 - val_acc: 0.2433\n",
      "Epoch 149/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 2.1348 - acc: 0.1734 - val_loss: 2.0815 - val_acc: 0.2348\n",
      "Epoch 150/500\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 2.1300 - acc: 0.1715 - val_loss: 2.0855 - val_acc: 0.2376\n",
      "Epoch 151/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 2.1383 - acc: 0.1842 - val_loss: 2.0859 - val_acc: 0.2462\n",
      "Epoch 152/500\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 2.1327 - acc: 0.1867 - val_loss: 2.0840 - val_acc: 0.2548\n",
      "Epoch 153/500\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 2.1349 - acc: 0.1818 - val_loss: 2.0832 - val_acc: 0.2586\n",
      "Epoch 154/500\n",
      "1/1 [==============================] - 0s 246ms/step - loss: 2.1350 - acc: 0.1783 - val_loss: 2.0832 - val_acc: 0.2557\n",
      "Epoch 155/500\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 2.1311 - acc: 0.1745 - val_loss: 2.0838 - val_acc: 0.2557\n",
      "Epoch 156/500\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 2.1306 - acc: 0.1777 - val_loss: 2.0836 - val_acc: 0.2567\n",
      "Epoch 157/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 2.1342 - acc: 0.1774 - val_loss: 2.0855 - val_acc: 0.2471\n",
      "Epoch 158/500\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 2.1322 - acc: 0.1859 - val_loss: 2.0871 - val_acc: 0.2462\n",
      "Epoch 159/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 2.1346 - acc: 0.1777 - val_loss: 2.0886 - val_acc: 0.2433\n",
      "Epoch 160/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 2.1358 - acc: 0.1832 - val_loss: 2.0857 - val_acc: 0.2529\n",
      "Epoch 161/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 2.1380 - acc: 0.1717 - val_loss: 2.0847 - val_acc: 0.2462\n",
      "Epoch 162/500\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 2.1336 - acc: 0.1742 - val_loss: 2.0818 - val_acc: 0.2510\n",
      "Epoch 163/500\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 2.1386 - acc: 0.1755 - val_loss: 2.0787 - val_acc: 0.2538\n",
      "Epoch 164/500\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 2.1350 - acc: 0.1726 - val_loss: 2.0774 - val_acc: 0.2567\n",
      "Epoch 165/500\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 2.1336 - acc: 0.1810 - val_loss: 2.0779 - val_acc: 0.2557\n",
      "Epoch 166/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 2.1370 - acc: 0.1793 - val_loss: 2.0795 - val_acc: 0.2652\n",
      "Epoch 167/500\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 2.1311 - acc: 0.1804 - val_loss: 2.0843 - val_acc: 0.2557\n",
      "Epoch 168/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 2.1301 - acc: 0.1755 - val_loss: 2.0905 - val_acc: 0.2605\n",
      "Epoch 169/500\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 2.1369 - acc: 0.1750 - val_loss: 2.0968 - val_acc: 0.2481\n",
      "Epoch 170/500\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 2.1310 - acc: 0.1815 - val_loss: 2.1023 - val_acc: 0.2433\n",
      "Epoch 171/500\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 2.1385 - acc: 0.1793 - val_loss: 2.1017 - val_acc: 0.2405\n",
      "Epoch 172/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 2.1276 - acc: 0.1870 - val_loss: 2.0987 - val_acc: 0.2662\n",
      "Epoch 173/500\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 2.1329 - acc: 0.1867 - val_loss: 2.0904 - val_acc: 0.2490\n",
      "Epoch 174/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 2.1270 - acc: 0.1932 - val_loss: 2.0849 - val_acc: 0.2433\n",
      "Epoch 175/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 2.1313 - acc: 0.1783 - val_loss: 2.0843 - val_acc: 0.2405\n",
      "Epoch 176/500\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 2.1395 - acc: 0.1734 - val_loss: 2.0858 - val_acc: 0.2443\n",
      "Epoch 177/500\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 2.1363 - acc: 0.1791 - val_loss: 2.0881 - val_acc: 0.2500\n",
      "Epoch 178/500\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 2.1273 - acc: 0.1853 - val_loss: 2.0928 - val_acc: 0.2481\n",
      "Epoch 179/500\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 2.1278 - acc: 0.1932 - val_loss: 2.0983 - val_acc: 0.2452\n",
      "Epoch 180/500\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 2.1326 - acc: 0.1807 - val_loss: 2.1034 - val_acc: 0.2405\n",
      "Epoch 181/500\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 2.1344 - acc: 0.1826 - val_loss: 2.1001 - val_acc: 0.2424\n",
      "Epoch 182/500\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 2.1283 - acc: 0.1899 - val_loss: 2.0926 - val_acc: 0.2433\n",
      "Epoch 183/500\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 2.1343 - acc: 0.1829 - val_loss: 2.0865 - val_acc: 0.2481\n",
      "Epoch 184/500\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 2.1301 - acc: 0.1804 - val_loss: 2.0845 - val_acc: 0.2395\n",
      "Epoch 185/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 2.1327 - acc: 0.1799 - val_loss: 2.0835 - val_acc: 0.2376\n",
      "Epoch 186/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 2.1284 - acc: 0.1870 - val_loss: 2.0828 - val_acc: 0.2405\n",
      "Epoch 187/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 2.1322 - acc: 0.1793 - val_loss: 2.0833 - val_acc: 0.2481\n",
      "Epoch 188/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 2.1268 - acc: 0.1780 - val_loss: 2.0845 - val_acc: 0.2424\n",
      "Epoch 189/500\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 2.1299 - acc: 0.1832 - val_loss: 2.0867 - val_acc: 0.2405\n",
      "Epoch 190/500\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 2.1342 - acc: 0.1891 - val_loss: 2.0875 - val_acc: 0.2338\n",
      "Epoch 191/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 2.1343 - acc: 0.1864 - val_loss: 2.0862 - val_acc: 0.2452\n",
      "Epoch 192/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 2.1371 - acc: 0.1845 - val_loss: 2.0847 - val_acc: 0.2443\n",
      "Epoch 193/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 2.1298 - acc: 0.1774 - val_loss: 2.0830 - val_acc: 0.2481\n",
      "Epoch 194/500\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 2.1356 - acc: 0.1799 - val_loss: 2.0831 - val_acc: 0.2490\n",
      "Epoch 195/500\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 2.1341 - acc: 0.1832 - val_loss: 2.0813 - val_acc: 0.2481\n",
      "Epoch 196/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 2.1353 - acc: 0.1761 - val_loss: 2.0810 - val_acc: 0.2481\n",
      "Epoch 197/500\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 2.1332 - acc: 0.1701 - val_loss: 2.0815 - val_acc: 0.2462\n",
      "Epoch 198/500\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 2.1296 - acc: 0.1902 - val_loss: 2.0823 - val_acc: 0.2452\n",
      "Epoch 199/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 2.1308 - acc: 0.1823 - val_loss: 2.0836 - val_acc: 0.2510\n",
      "Epoch 200/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 2.1341 - acc: 0.1780 - val_loss: 2.0838 - val_acc: 0.2481\n",
      "Epoch 201/500\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 2.1333 - acc: 0.1845 - val_loss: 2.0815 - val_acc: 0.2433\n",
      "Epoch 202/500\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 2.1321 - acc: 0.1815 - val_loss: 2.0800 - val_acc: 0.2510\n",
      "Epoch 203/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 2.1316 - acc: 0.1769 - val_loss: 2.0784 - val_acc: 0.2529\n",
      "Epoch 204/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 2.1325 - acc: 0.1813 - val_loss: 2.0785 - val_acc: 0.2490\n",
      "Epoch 205/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 2.1295 - acc: 0.1889 - val_loss: 2.0800 - val_acc: 0.2443\n",
      "Epoch 206/500\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 2.1298 - acc: 0.1832 - val_loss: 2.0796 - val_acc: 0.2452\n",
      "Epoch 207/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 2.1303 - acc: 0.1688 - val_loss: 2.0803 - val_acc: 0.2462\n",
      "Epoch 208/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 2.1285 - acc: 0.1845 - val_loss: 2.0821 - val_acc: 0.2471\n",
      "Epoch 209/500\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 2.1309 - acc: 0.1851 - val_loss: 2.0832 - val_acc: 0.2529\n",
      "Epoch 210/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 2.1292 - acc: 0.1910 - val_loss: 2.0833 - val_acc: 0.2510\n",
      "Epoch 211/500\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 2.1320 - acc: 0.1690 - val_loss: 2.0823 - val_acc: 0.2529\n",
      "Epoch 212/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 2.1307 - acc: 0.1867 - val_loss: 2.0830 - val_acc: 0.2481\n",
      "Epoch 213/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 2.1303 - acc: 0.1796 - val_loss: 2.0829 - val_acc: 0.2548\n",
      "Epoch 214/500\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 2.1338 - acc: 0.1715 - val_loss: 2.0832 - val_acc: 0.2414\n",
      "Epoch 215/500\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 2.1290 - acc: 0.1840 - val_loss: 2.0817 - val_acc: 0.2376\n",
      "Epoch 216/500\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 2.1312 - acc: 0.1793 - val_loss: 2.0772 - val_acc: 0.2414\n",
      "Epoch 217/500\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 2.1280 - acc: 0.1840 - val_loss: 2.0771 - val_acc: 0.2500\n",
      "Epoch 218/500\n",
      "1/1 [==============================] - 0s 229ms/step - loss: 2.1295 - acc: 0.1823 - val_loss: 2.0802 - val_acc: 0.2529\n",
      "Epoch 219/500\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 2.1265 - acc: 0.1886 - val_loss: 2.0857 - val_acc: 0.2576\n",
      "Epoch 220/500\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 2.1281 - acc: 0.1796 - val_loss: 2.0945 - val_acc: 0.2576\n",
      "Epoch 221/500\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 2.1342 - acc: 0.1807 - val_loss: 2.1007 - val_acc: 0.2500\n",
      "Epoch 222/500\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 2.1350 - acc: 0.1696 - val_loss: 2.1015 - val_acc: 0.2471\n",
      "Epoch 223/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 2.1321 - acc: 0.1818 - val_loss: 2.1007 - val_acc: 0.2262\n",
      "Epoch 224/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 2.1263 - acc: 0.1848 - val_loss: 2.0980 - val_acc: 0.2272\n",
      "Epoch 225/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 2.1329 - acc: 0.1736 - val_loss: 2.0899 - val_acc: 0.2348\n",
      "Epoch 226/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 2.1320 - acc: 0.1793 - val_loss: 2.0853 - val_acc: 0.2433\n",
      "Epoch 227/500\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 2.1295 - acc: 0.1883 - val_loss: 2.0836 - val_acc: 0.2500\n",
      "Epoch 228/500\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 2.1298 - acc: 0.1823 - val_loss: 2.0853 - val_acc: 0.2490\n",
      "Epoch 229/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 2.1354 - acc: 0.1810 - val_loss: 2.0876 - val_acc: 0.2471\n",
      "Epoch 230/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 2.1282 - acc: 0.1810 - val_loss: 2.0883 - val_acc: 0.2452\n",
      "Epoch 231/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 2.1293 - acc: 0.1889 - val_loss: 2.0859 - val_acc: 0.2510\n",
      "Epoch 232/500\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 2.1292 - acc: 0.1842 - val_loss: 2.0837 - val_acc: 0.2433\n",
      "Epoch 233/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 2.1264 - acc: 0.1832 - val_loss: 2.0785 - val_acc: 0.2433\n",
      "Epoch 234/500\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 2.1317 - acc: 0.1834 - val_loss: 2.0734 - val_acc: 0.2424\n",
      "Epoch 235/500\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 2.1320 - acc: 0.1845 - val_loss: 2.0687 - val_acc: 0.2405\n",
      "Epoch 236/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 2.1288 - acc: 0.1853 - val_loss: 2.0661 - val_acc: 0.2367\n",
      "Epoch 237/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 2.1292 - acc: 0.1761 - val_loss: 2.0680 - val_acc: 0.2414\n",
      "Epoch 238/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 2.1310 - acc: 0.1875 - val_loss: 2.0710 - val_acc: 0.2424\n",
      "Epoch 239/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 2.1295 - acc: 0.1742 - val_loss: 2.0743 - val_acc: 0.2452\n",
      "Epoch 240/500\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 2.1318 - acc: 0.1793 - val_loss: 2.0755 - val_acc: 0.2338\n",
      "Epoch 241/500\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 2.1329 - acc: 0.1785 - val_loss: 2.0742 - val_acc: 0.2357\n",
      "Epoch 242/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 2.1343 - acc: 0.1774 - val_loss: 2.0715 - val_acc: 0.2424\n",
      "Epoch 243/500\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 2.1325 - acc: 0.1780 - val_loss: 2.0706 - val_acc: 0.2348\n",
      "Epoch 244/500\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 2.1304 - acc: 0.1780 - val_loss: 2.0690 - val_acc: 0.2367\n",
      "Epoch 245/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 2.1347 - acc: 0.1774 - val_loss: 2.0669 - val_acc: 0.2376\n",
      "Epoch 246/500\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 2.1325 - acc: 0.1867 - val_loss: 2.0659 - val_acc: 0.2338\n",
      "Epoch 247/500\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 2.1320 - acc: 0.1856 - val_loss: 2.0648 - val_acc: 0.2329\n",
      "Epoch 248/500\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 2.1279 - acc: 0.1851 - val_loss: 2.0629 - val_acc: 0.2367\n",
      "Epoch 249/500\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 2.1334 - acc: 0.1793 - val_loss: 2.0630 - val_acc: 0.2433\n",
      "Epoch 250/500\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 2.1315 - acc: 0.1807 - val_loss: 2.0629 - val_acc: 0.2538\n",
      "Epoch 251/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 2.1358 - acc: 0.1826 - val_loss: 2.0634 - val_acc: 0.2548\n",
      "Epoch 252/500\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 2.1273 - acc: 0.1918 - val_loss: 2.0649 - val_acc: 0.2548\n",
      "Epoch 253/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 2.1275 - acc: 0.1872 - val_loss: 2.0682 - val_acc: 0.2433\n",
      "Epoch 254/500\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 2.1352 - acc: 0.1832 - val_loss: 2.0701 - val_acc: 0.2376\n",
      "Epoch 255/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 2.1320 - acc: 0.1810 - val_loss: 2.0691 - val_acc: 0.2319\n",
      "Epoch 256/500\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 2.1276 - acc: 0.1793 - val_loss: 2.0649 - val_acc: 0.2329\n",
      "Epoch 257/500\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 2.1329 - acc: 0.1818 - val_loss: 2.0612 - val_acc: 0.2443\n",
      "Epoch 258/500\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 2.1328 - acc: 0.1872 - val_loss: 2.0572 - val_acc: 0.2424\n",
      "Epoch 259/500\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 2.1302 - acc: 0.1867 - val_loss: 2.0582 - val_acc: 0.2471\n",
      "Epoch 260/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 2.1310 - acc: 0.1815 - val_loss: 2.0612 - val_acc: 0.2519\n",
      "Epoch 261/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 2.1339 - acc: 0.1853 - val_loss: 2.0650 - val_acc: 0.2605\n",
      "Epoch 262/500\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 2.1319 - acc: 0.1818 - val_loss: 2.0716 - val_acc: 0.2405\n",
      "Epoch 263/500\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 2.1302 - acc: 0.1802 - val_loss: 2.0770 - val_acc: 0.2329\n",
      "Epoch 264/500\n",
      "1/1 [==============================] - 0s 225ms/step - loss: 2.1297 - acc: 0.1899 - val_loss: 2.0789 - val_acc: 0.2310\n",
      "Epoch 265/500\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 2.1280 - acc: 0.1832 - val_loss: 2.0763 - val_acc: 0.2376\n",
      "Epoch 266/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 2.1286 - acc: 0.1848 - val_loss: 2.0698 - val_acc: 0.2386\n",
      "Epoch 267/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 2.1312 - acc: 0.1745 - val_loss: 2.0631 - val_acc: 0.2462\n",
      "Epoch 268/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 2.1351 - acc: 0.1761 - val_loss: 2.0575 - val_acc: 0.2433\n",
      "Epoch 269/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 2.1288 - acc: 0.1851 - val_loss: 2.0548 - val_acc: 0.2433\n",
      "Epoch 270/500\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 2.1293 - acc: 0.1772 - val_loss: 2.0550 - val_acc: 0.2414\n",
      "Epoch 271/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 2.1280 - acc: 0.1823 - val_loss: 2.0558 - val_acc: 0.2424\n",
      "Epoch 272/500\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 2.1336 - acc: 0.1832 - val_loss: 2.0590 - val_acc: 0.2357\n",
      "Epoch 273/500\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 2.1247 - acc: 0.1870 - val_loss: 2.0622 - val_acc: 0.2319\n",
      "Epoch 274/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 2.1306 - acc: 0.1813 - val_loss: 2.0623 - val_acc: 0.2452\n",
      "Epoch 275/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 2.1279 - acc: 0.1916 - val_loss: 2.0619 - val_acc: 0.2462\n",
      "Epoch 276/500\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 2.1338 - acc: 0.1788 - val_loss: 2.0594 - val_acc: 0.2548\n",
      "Epoch 277/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 2.1276 - acc: 0.1897 - val_loss: 2.0570 - val_acc: 0.2490\n",
      "Epoch 278/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 2.1275 - acc: 0.1859 - val_loss: 2.0558 - val_acc: 0.2405\n",
      "Epoch 279/500\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 2.1315 - acc: 0.1837 - val_loss: 2.0557 - val_acc: 0.2357\n",
      "Epoch 280/500\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 2.1332 - acc: 0.1832 - val_loss: 2.0579 - val_acc: 0.2357\n",
      "Epoch 281/500\n",
      "1/1 [==============================] - 0s 243ms/step - loss: 2.1296 - acc: 0.1872 - val_loss: 2.0585 - val_acc: 0.2281\n",
      "Epoch 282/500\n",
      "1/1 [==============================] - 0s 224ms/step - loss: 2.1269 - acc: 0.1910 - val_loss: 2.0581 - val_acc: 0.2319\n",
      "Epoch 283/500\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 2.1264 - acc: 0.1799 - val_loss: 2.0587 - val_acc: 0.2348\n",
      "Epoch 284/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 2.1216 - acc: 0.1927 - val_loss: 2.0572 - val_acc: 0.2481\n",
      "Epoch 285/500\n",
      "1/1 [==============================] - 0s 225ms/step - loss: 2.1288 - acc: 0.1807 - val_loss: 2.0569 - val_acc: 0.2519\n",
      "Epoch 286/500\n",
      "1/1 [==============================] - 0s 232ms/step - loss: 2.1305 - acc: 0.1815 - val_loss: 2.0566 - val_acc: 0.2529\n",
      "Epoch 287/500\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 2.1292 - acc: 0.1780 - val_loss: 2.0581 - val_acc: 0.2443\n",
      "Epoch 288/500\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 2.1262 - acc: 0.1802 - val_loss: 2.0606 - val_acc: 0.2300\n",
      "Epoch 289/500\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 2.1305 - acc: 0.1793 - val_loss: 2.0623 - val_acc: 0.2177\n",
      "Epoch 290/500\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 2.1295 - acc: 0.1834 - val_loss: 2.0636 - val_acc: 0.2177\n",
      "Epoch 291/500\n",
      "1/1 [==============================] - 0s 188ms/step - loss: 2.1294 - acc: 0.1821 - val_loss: 2.0637 - val_acc: 0.2205\n",
      "Epoch 292/500\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 2.1254 - acc: 0.1853 - val_loss: 2.0622 - val_acc: 0.2348\n",
      "Epoch 293/500\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 2.1321 - acc: 0.1832 - val_loss: 2.0621 - val_acc: 0.2395\n",
      "Epoch 294/500\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 2.1303 - acc: 0.1845 - val_loss: 2.0607 - val_acc: 0.2471\n",
      "Epoch 295/500\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 2.1281 - acc: 0.1878 - val_loss: 2.0587 - val_acc: 0.2490\n",
      "Epoch 296/500\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 2.1312 - acc: 0.1802 - val_loss: 2.0590 - val_acc: 0.2471\n",
      "Epoch 297/500\n",
      "1/1 [==============================] - 0s 364ms/step - loss: 2.1278 - acc: 0.1832 - val_loss: 2.0593 - val_acc: 0.2424\n",
      "Epoch 298/500\n",
      "1/1 [==============================] - 0s 230ms/step - loss: 2.1283 - acc: 0.1894 - val_loss: 2.0609 - val_acc: 0.2357\n",
      "Epoch 299/500\n",
      "1/1 [==============================] - 0s 227ms/step - loss: 2.1272 - acc: 0.1755 - val_loss: 2.0646 - val_acc: 0.2319\n",
      "Epoch 300/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 2.1317 - acc: 0.1796 - val_loss: 2.0659 - val_acc: 0.2329\n",
      "Epoch 301/500\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 2.1351 - acc: 0.1853 - val_loss: 2.0637 - val_acc: 0.2348\n",
      "Epoch 302/500\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 2.1292 - acc: 0.1867 - val_loss: 2.0603 - val_acc: 0.2319\n",
      "Epoch 303/500\n",
      "1/1 [==============================] - 0s 394ms/step - loss: 2.1252 - acc: 0.1951 - val_loss: 2.0553 - val_acc: 0.2414\n",
      "Epoch 304/500\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 2.1340 - acc: 0.1872 - val_loss: 2.0514 - val_acc: 0.2395\n",
      "Epoch 305/500\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 2.1295 - acc: 0.1796 - val_loss: 2.0516 - val_acc: 0.2386\n",
      "Epoch 306/500\n",
      "1/1 [==============================] - 0s 234ms/step - loss: 2.1254 - acc: 0.1870 - val_loss: 2.0559 - val_acc: 0.2481\n",
      "Epoch 307/500\n",
      "1/1 [==============================] - 0s 238ms/step - loss: 2.1298 - acc: 0.1810 - val_loss: 2.0625 - val_acc: 0.2405\n",
      "Epoch 308/500\n",
      "1/1 [==============================] - 0s 252ms/step - loss: 2.1294 - acc: 0.1810 - val_loss: 2.0685 - val_acc: 0.2424\n",
      "Epoch 309/500\n",
      "1/1 [==============================] - 0s 255ms/step - loss: 2.1257 - acc: 0.1883 - val_loss: 2.0723 - val_acc: 0.2452\n",
      "Epoch 310/500\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 2.1297 - acc: 0.1883 - val_loss: 2.0687 - val_acc: 0.2386\n",
      "Epoch 311/500\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 2.1304 - acc: 0.1848 - val_loss: 2.0624 - val_acc: 0.2310\n",
      "Epoch 312/500\n",
      "1/1 [==============================] - 0s 249ms/step - loss: 2.1304 - acc: 0.1867 - val_loss: 2.0569 - val_acc: 0.2357\n",
      "Epoch 313/500\n",
      "1/1 [==============================] - 0s 236ms/step - loss: 2.1311 - acc: 0.1796 - val_loss: 2.0525 - val_acc: 0.2357\n",
      "Epoch 314/500\n",
      "1/1 [==============================] - 0s 232ms/step - loss: 2.1263 - acc: 0.1937 - val_loss: 2.0523 - val_acc: 0.2348\n",
      "Epoch 315/500\n",
      "1/1 [==============================] - 0s 246ms/step - loss: 2.1315 - acc: 0.1916 - val_loss: 2.0557 - val_acc: 0.2281\n",
      "Epoch 316/500\n",
      "1/1 [==============================] - 0s 232ms/step - loss: 2.1303 - acc: 0.1793 - val_loss: 2.0637 - val_acc: 0.2281\n",
      "Epoch 317/500\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 2.1266 - acc: 0.1785 - val_loss: 2.0699 - val_acc: 0.2348\n",
      "Epoch 318/500\n",
      "1/1 [==============================] - 0s 227ms/step - loss: 2.1262 - acc: 0.1837 - val_loss: 2.0715 - val_acc: 0.2319\n",
      "Epoch 319/500\n",
      "1/1 [==============================] - 0s 238ms/step - loss: 2.1271 - acc: 0.1788 - val_loss: 2.0670 - val_acc: 0.2272\n",
      "Epoch 320/500\n",
      "1/1 [==============================] - 0s 232ms/step - loss: 2.1288 - acc: 0.1761 - val_loss: 2.0622 - val_acc: 0.2272\n",
      "Epoch 321/500\n",
      "1/1 [==============================] - 0s 235ms/step - loss: 2.1279 - acc: 0.1845 - val_loss: 2.0601 - val_acc: 0.2215\n",
      "Epoch 322/500\n",
      "1/1 [==============================] - 0s 233ms/step - loss: 2.1252 - acc: 0.1813 - val_loss: 2.0578 - val_acc: 0.2300\n",
      "Epoch 323/500\n",
      "1/1 [==============================] - 0s 242ms/step - loss: 2.1300 - acc: 0.1810 - val_loss: 2.0565 - val_acc: 0.2367\n",
      "Epoch 324/500\n",
      "1/1 [==============================] - 0s 229ms/step - loss: 2.1229 - acc: 0.1908 - val_loss: 2.0563 - val_acc: 0.2395\n",
      "Epoch 325/500\n",
      "1/1 [==============================] - 0s 228ms/step - loss: 2.1270 - acc: 0.1948 - val_loss: 2.0582 - val_acc: 0.2300\n",
      "Epoch 326/500\n",
      "1/1 [==============================] - 0s 228ms/step - loss: 2.1268 - acc: 0.1889 - val_loss: 2.0592 - val_acc: 0.2262\n",
      "Epoch 327/500\n",
      "1/1 [==============================] - 0s 229ms/step - loss: 2.1293 - acc: 0.1769 - val_loss: 2.0572 - val_acc: 0.2291\n",
      "Epoch 328/500\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 2.1292 - acc: 0.1872 - val_loss: 2.0545 - val_acc: 0.2205\n",
      "Epoch 329/500\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 2.1266 - acc: 0.1845 - val_loss: 2.0535 - val_acc: 0.2253\n",
      "Epoch 330/500\n",
      "1/1 [==============================] - 0s 234ms/step - loss: 2.1297 - acc: 0.1837 - val_loss: 2.0538 - val_acc: 0.2253\n",
      "Epoch 331/500\n",
      "1/1 [==============================] - 0s 236ms/step - loss: 2.1256 - acc: 0.1880 - val_loss: 2.0553 - val_acc: 0.2310\n",
      "Epoch 332/500\n",
      "1/1 [==============================] - 0s 228ms/step - loss: 2.1254 - acc: 0.1886 - val_loss: 2.0597 - val_acc: 0.2395\n",
      "Epoch 333/500\n",
      "1/1 [==============================] - 0s 225ms/step - loss: 2.1257 - acc: 0.1880 - val_loss: 2.0663 - val_acc: 0.2357\n",
      "Epoch 334/500\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 2.1268 - acc: 0.1826 - val_loss: 2.0701 - val_acc: 0.2281\n",
      "Epoch 335/500\n",
      "1/1 [==============================] - 0s 227ms/step - loss: 2.1281 - acc: 0.1799 - val_loss: 2.0711 - val_acc: 0.2272\n",
      "Epoch 336/500\n",
      "1/1 [==============================] - 0s 227ms/step - loss: 2.1301 - acc: 0.1848 - val_loss: 2.0666 - val_acc: 0.2310\n",
      "Epoch 337/500\n",
      "1/1 [==============================] - 0s 226ms/step - loss: 2.1301 - acc: 0.1802 - val_loss: 2.0628 - val_acc: 0.2348\n",
      "Epoch 338/500\n",
      "1/1 [==============================] - 0s 236ms/step - loss: 2.1302 - acc: 0.1851 - val_loss: 2.0598 - val_acc: 0.2253\n",
      "Epoch 339/500\n",
      "1/1 [==============================] - 0s 243ms/step - loss: 2.1222 - acc: 0.1940 - val_loss: 2.0539 - val_acc: 0.2158\n",
      "Epoch 340/500\n",
      "1/1 [==============================] - 0s 238ms/step - loss: 2.1310 - acc: 0.1880 - val_loss: 2.0513 - val_acc: 0.2272\n",
      "Epoch 341/500\n",
      "1/1 [==============================] - 0s 246ms/step - loss: 2.1245 - acc: 0.1842 - val_loss: 2.0527 - val_acc: 0.2414\n",
      "Epoch 342/500\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 2.1283 - acc: 0.1957 - val_loss: 2.0558 - val_acc: 0.2414\n",
      "Epoch 343/500\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 2.1265 - acc: 0.1826 - val_loss: 2.0610 - val_acc: 0.2395\n",
      "Epoch 344/500\n",
      "1/1 [==============================] - 0s 240ms/step - loss: 2.1255 - acc: 0.1872 - val_loss: 2.0619 - val_acc: 0.2443\n",
      "Epoch 345/500\n",
      "1/1 [==============================] - 0s 239ms/step - loss: 2.1325 - acc: 0.1810 - val_loss: 2.0610 - val_acc: 0.2452\n",
      "Epoch 346/500\n",
      "1/1 [==============================] - 0s 241ms/step - loss: 2.1289 - acc: 0.1788 - val_loss: 2.0600 - val_acc: 0.2443\n",
      "Epoch 347/500\n",
      "1/1 [==============================] - 0s 225ms/step - loss: 2.1360 - acc: 0.1677 - val_loss: 2.0609 - val_acc: 0.2348\n",
      "Epoch 348/500\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 2.1208 - acc: 0.1965 - val_loss: 2.0623 - val_acc: 0.2158\n",
      "Epoch 349/500\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 2.1279 - acc: 0.1875 - val_loss: 2.0606 - val_acc: 0.2120\n",
      "Epoch 350/500\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 2.1324 - acc: 0.1774 - val_loss: 2.0591 - val_acc: 0.2177\n",
      "Epoch 351/500\n",
      "1/1 [==============================] - 0s 234ms/step - loss: 2.1266 - acc: 0.1856 - val_loss: 2.0576 - val_acc: 0.2319\n",
      "Epoch 352/500\n",
      "1/1 [==============================] - 0s 243ms/step - loss: 2.1297 - acc: 0.1859 - val_loss: 2.0578 - val_acc: 0.2348\n",
      "Epoch 353/500\n",
      "1/1 [==============================] - 0s 238ms/step - loss: 2.1221 - acc: 0.1927 - val_loss: 2.0607 - val_acc: 0.2405\n",
      "Epoch 354/500\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 2.1319 - acc: 0.1845 - val_loss: 2.0599 - val_acc: 0.2452\n",
      "Epoch 355/500\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 2.1226 - acc: 0.1921 - val_loss: 2.0591 - val_acc: 0.2481\n",
      "Epoch 356/500\n",
      "1/1 [==============================] - 0s 251ms/step - loss: 2.1283 - acc: 0.1889 - val_loss: 2.0577 - val_acc: 0.2519\n",
      "Epoch 357/500\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 2.1314 - acc: 0.1799 - val_loss: 2.0574 - val_acc: 0.2424\n",
      "Epoch 358/500\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 2.1242 - acc: 0.1840 - val_loss: 2.0601 - val_acc: 0.2272\n",
      "Epoch 359/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 2.1234 - acc: 0.1929 - val_loss: 2.0640 - val_acc: 0.2167\n",
      "Epoch 360/500\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 2.1279 - acc: 0.1840 - val_loss: 2.0656 - val_acc: 0.2215\n",
      "Epoch 361/500\n",
      "1/1 [==============================] - 0s 195ms/step - loss: 2.1301 - acc: 0.1845 - val_loss: 2.0650 - val_acc: 0.2338\n",
      "Epoch 362/500\n",
      "1/1 [==============================] - 0s 231ms/step - loss: 2.1290 - acc: 0.1935 - val_loss: 2.0640 - val_acc: 0.2395\n",
      "Epoch 363/500\n",
      "1/1 [==============================] - 0s 237ms/step - loss: 2.1271 - acc: 0.1889 - val_loss: 2.0597 - val_acc: 0.2443\n",
      "Epoch 364/500\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 2.1256 - acc: 0.1818 - val_loss: 2.0557 - val_acc: 0.2462\n",
      "Epoch 365/500\n",
      "1/1 [==============================] - 0s 227ms/step - loss: 2.1253 - acc: 0.1856 - val_loss: 2.0527 - val_acc: 0.2424\n",
      "Epoch 366/500\n",
      "1/1 [==============================] - 0s 219ms/step - loss: 2.1295 - acc: 0.1910 - val_loss: 2.0528 - val_acc: 0.2433\n",
      "Epoch 367/500\n",
      "1/1 [==============================] - 0s 226ms/step - loss: 2.1262 - acc: 0.1834 - val_loss: 2.0550 - val_acc: 0.2395\n",
      "Epoch 368/500\n",
      "1/1 [==============================] - 0s 225ms/step - loss: 2.1262 - acc: 0.1932 - val_loss: 2.0591 - val_acc: 0.2253\n",
      "Epoch 369/500\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 2.1317 - acc: 0.1804 - val_loss: 2.0618 - val_acc: 0.2300\n",
      "Epoch 370/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 2.1283 - acc: 0.1864 - val_loss: 2.0662 - val_acc: 0.2310\n",
      "Epoch 371/500\n",
      "1/1 [==============================] - 0s 231ms/step - loss: 2.1261 - acc: 0.1951 - val_loss: 2.0664 - val_acc: 0.2329\n",
      "Epoch 372/500\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 2.1232 - acc: 0.1937 - val_loss: 2.0667 - val_acc: 0.2395\n",
      "Epoch 373/500\n",
      "1/1 [==============================] - 0s 216ms/step - loss: 2.1271 - acc: 0.1842 - val_loss: 2.0670 - val_acc: 0.2386\n",
      "Epoch 374/500\n",
      "1/1 [==============================] - 0s 227ms/step - loss: 2.1320 - acc: 0.1864 - val_loss: 2.0661 - val_acc: 0.2357\n",
      "Epoch 375/500\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 2.1236 - acc: 0.1954 - val_loss: 2.0647 - val_acc: 0.2300\n",
      "Epoch 376/500\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 2.1252 - acc: 0.1780 - val_loss: 2.0650 - val_acc: 0.2291\n",
      "Epoch 377/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 2.1292 - acc: 0.1821 - val_loss: 2.0668 - val_acc: 0.2243\n",
      "Epoch 378/500\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 2.1269 - acc: 0.1845 - val_loss: 2.0686 - val_acc: 0.2253\n",
      "Epoch 379/500\n",
      "1/1 [==============================] - 0s 236ms/step - loss: 2.1265 - acc: 0.1834 - val_loss: 2.0708 - val_acc: 0.2348\n",
      "Epoch 380/500\n",
      "1/1 [==============================] - 0s 226ms/step - loss: 2.1283 - acc: 0.1856 - val_loss: 2.0690 - val_acc: 0.2300\n",
      "Epoch 381/500\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 2.1251 - acc: 0.1937 - val_loss: 2.0658 - val_acc: 0.2262\n",
      "Epoch 382/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 2.1220 - acc: 0.1984 - val_loss: 2.0635 - val_acc: 0.2367\n",
      "Epoch 383/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 2.1234 - acc: 0.1883 - val_loss: 2.0611 - val_acc: 0.2376\n",
      "Epoch 384/500\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 2.1263 - acc: 0.1894 - val_loss: 2.0616 - val_acc: 0.2376\n",
      "Epoch 385/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 2.1225 - acc: 0.1872 - val_loss: 2.0615 - val_acc: 0.2357\n",
      "Epoch 386/500\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 2.1272 - acc: 0.1870 - val_loss: 2.0618 - val_acc: 0.2376\n",
      "Epoch 387/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 2.1276 - acc: 0.1851 - val_loss: 2.0636 - val_acc: 0.2300\n",
      "Epoch 388/500\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 2.1275 - acc: 0.1932 - val_loss: 2.0624 - val_acc: 0.2281\n",
      "Epoch 389/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 2.1266 - acc: 0.1883 - val_loss: 2.0610 - val_acc: 0.2338\n",
      "Epoch 390/500\n",
      "1/1 [==============================] - 0s 215ms/step - loss: 2.1215 - acc: 0.1864 - val_loss: 2.0590 - val_acc: 0.2224\n",
      "Epoch 391/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 2.1343 - acc: 0.1815 - val_loss: 2.0560 - val_acc: 0.2243\n",
      "Epoch 392/500\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 2.1267 - acc: 0.1829 - val_loss: 2.0554 - val_acc: 0.2253\n",
      "Epoch 393/500\n",
      "1/1 [==============================] - 0s 230ms/step - loss: 2.1259 - acc: 0.1867 - val_loss: 2.0553 - val_acc: 0.2243\n",
      "Epoch 394/500\n",
      "1/1 [==============================] - 0s 220ms/step - loss: 2.1276 - acc: 0.1842 - val_loss: 2.0541 - val_acc: 0.2291\n",
      "Epoch 395/500\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 2.1309 - acc: 0.1889 - val_loss: 2.0533 - val_acc: 0.2357\n",
      "Epoch 396/500\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 2.1261 - acc: 0.1818 - val_loss: 2.0553 - val_acc: 0.2357\n",
      "Epoch 397/500\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 2.1275 - acc: 0.1910 - val_loss: 2.0576 - val_acc: 0.2338\n",
      "Epoch 398/500\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 2.1244 - acc: 0.1902 - val_loss: 2.0619 - val_acc: 0.2329\n",
      "Epoch 399/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 2.1291 - acc: 0.1932 - val_loss: 2.0673 - val_acc: 0.2348\n",
      "Epoch 400/500\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 2.1286 - acc: 0.1891 - val_loss: 2.0711 - val_acc: 0.2367\n",
      "Epoch 401/500\n",
      "1/1 [==============================] - 0s 221ms/step - loss: 2.1207 - acc: 0.1889 - val_loss: 2.0705 - val_acc: 0.2348\n",
      "Epoch 402/500\n",
      "1/1 [==============================] - 0s 223ms/step - loss: 2.1264 - acc: 0.1878 - val_loss: 2.0636 - val_acc: 0.2338\n",
      "Epoch 403/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 2.1256 - acc: 0.1883 - val_loss: 2.0557 - val_acc: 0.2357\n",
      "Epoch 404/500\n",
      "1/1 [==============================] - 0s 224ms/step - loss: 2.1260 - acc: 0.1840 - val_loss: 2.0509 - val_acc: 0.2272\n",
      "Epoch 405/500\n",
      "1/1 [==============================] - 0s 233ms/step - loss: 2.1324 - acc: 0.1804 - val_loss: 2.0518 - val_acc: 0.2281\n",
      "Epoch 406/500\n",
      "1/1 [==============================] - 0s 187ms/step - loss: 2.1238 - acc: 0.1962 - val_loss: 2.0573 - val_acc: 0.2300\n",
      "Epoch 407/500\n",
      "1/1 [==============================] - 0s 194ms/step - loss: 2.1257 - acc: 0.1918 - val_loss: 2.0617 - val_acc: 0.2291\n",
      "Epoch 408/500\n",
      "1/1 [==============================] - 0s 214ms/step - loss: 2.1280 - acc: 0.1802 - val_loss: 2.0669 - val_acc: 0.2367\n",
      "Epoch 409/500\n",
      "1/1 [==============================] - 0s 193ms/step - loss: 2.1241 - acc: 0.1864 - val_loss: 2.0686 - val_acc: 0.2262\n",
      "Epoch 410/500\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 2.1230 - acc: 0.1878 - val_loss: 2.0653 - val_acc: 0.2272\n",
      "Epoch 411/500\n",
      "1/1 [==============================] - 0s 251ms/step - loss: 2.1267 - acc: 0.1813 - val_loss: 2.0612 - val_acc: 0.2395\n",
      "Epoch 412/500\n",
      "1/1 [==============================] - 0s 252ms/step - loss: 2.1319 - acc: 0.1891 - val_loss: 2.0588 - val_acc: 0.2414\n",
      "Epoch 413/500\n",
      "1/1 [==============================] - 0s 232ms/step - loss: 2.1261 - acc: 0.2008 - val_loss: 2.0573 - val_acc: 0.2357\n",
      "Epoch 414/500\n",
      "1/1 [==============================] - 0s 246ms/step - loss: 2.1229 - acc: 0.1883 - val_loss: 2.0613 - val_acc: 0.2262\n",
      "Epoch 415/500\n",
      "1/1 [==============================] - 0s 241ms/step - loss: 2.1211 - acc: 0.1954 - val_loss: 2.0646 - val_acc: 0.2196\n",
      "Epoch 416/500\n",
      "1/1 [==============================] - 0s 231ms/step - loss: 2.1271 - acc: 0.1872 - val_loss: 2.0676 - val_acc: 0.2291\n",
      "Epoch 417/500\n",
      "1/1 [==============================] - 0s 239ms/step - loss: 2.1186 - acc: 0.1997 - val_loss: 2.0708 - val_acc: 0.2319\n",
      "Epoch 418/500\n",
      "1/1 [==============================] - 0s 222ms/step - loss: 2.1277 - acc: 0.1908 - val_loss: 2.0702 - val_acc: 0.2376\n",
      "Epoch 419/500\n",
      "1/1 [==============================] - 0s 240ms/step - loss: 2.1321 - acc: 0.1837 - val_loss: 2.0682 - val_acc: 0.2310\n",
      "Epoch 420/500\n",
      "1/1 [==============================] - 0s 245ms/step - loss: 2.1333 - acc: 0.1728 - val_loss: 2.0633 - val_acc: 0.2367\n",
      "Epoch 421/500\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 2.1268 - acc: 0.1823 - val_loss: 2.0617 - val_acc: 0.2357\n",
      "Epoch 422/500\n",
      "1/1 [==============================] - 0s 230ms/step - loss: 2.1255 - acc: 0.1755 - val_loss: 2.0619 - val_acc: 0.2272\n",
      "Epoch 423/500\n",
      "1/1 [==============================] - 0s 226ms/step - loss: 2.1215 - acc: 0.1897 - val_loss: 2.0632 - val_acc: 0.2310\n",
      "Epoch 424/500\n",
      "1/1 [==============================] - 0s 236ms/step - loss: 2.1250 - acc: 0.1889 - val_loss: 2.0644 - val_acc: 0.2281\n",
      "Epoch 425/500\n",
      "1/1 [==============================] - 0s 242ms/step - loss: 2.1246 - acc: 0.1927 - val_loss: 2.0658 - val_acc: 0.2291\n",
      "Epoch 426/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 2.1257 - acc: 0.1810 - val_loss: 2.0669 - val_acc: 0.2272\n",
      "Epoch 427/500\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 2.1255 - acc: 0.1908 - val_loss: 2.0656 - val_acc: 0.2262\n",
      "Epoch 428/500\n",
      "1/1 [==============================] - 0s 218ms/step - loss: 2.1287 - acc: 0.1832 - val_loss: 2.0662 - val_acc: 0.2253\n",
      "Epoch 429/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 2.1236 - acc: 0.1948 - val_loss: 2.0653 - val_acc: 0.2310\n",
      "Epoch 430/500\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 2.1280 - acc: 0.1753 - val_loss: 2.0651 - val_acc: 0.2291\n",
      "Epoch 431/500\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 2.1251 - acc: 0.1902 - val_loss: 2.0643 - val_acc: 0.2329\n",
      "Epoch 432/500\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 2.1253 - acc: 0.1856 - val_loss: 2.0656 - val_acc: 0.2348\n",
      "Epoch 433/500\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 2.1260 - acc: 0.1859 - val_loss: 2.0691 - val_acc: 0.2329\n",
      "Epoch 434/500\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 2.1192 - acc: 0.1870 - val_loss: 2.0708 - val_acc: 0.2281\n",
      "Epoch 435/500\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 2.1261 - acc: 0.1739 - val_loss: 2.0708 - val_acc: 0.2262\n",
      "Epoch 436/500\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 2.1260 - acc: 0.1832 - val_loss: 2.0697 - val_acc: 0.2243\n",
      "Epoch 437/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 2.1251 - acc: 0.1859 - val_loss: 2.0698 - val_acc: 0.2281\n",
      "Epoch 438/500\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 2.1252 - acc: 0.1897 - val_loss: 2.0676 - val_acc: 0.2319\n",
      "Epoch 439/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 2.1232 - acc: 0.1905 - val_loss: 2.0639 - val_acc: 0.2319\n",
      "Epoch 440/500\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 2.1265 - acc: 0.1832 - val_loss: 2.0614 - val_acc: 0.2376\n",
      "Epoch 441/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 2.1313 - acc: 0.1764 - val_loss: 2.0582 - val_acc: 0.2414\n",
      "Epoch 442/500\n",
      "1/1 [==============================] - 0s 217ms/step - loss: 2.1243 - acc: 0.1837 - val_loss: 2.0582 - val_acc: 0.2386\n",
      "Epoch 443/500\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 2.1240 - acc: 0.1905 - val_loss: 2.0592 - val_acc: 0.2329\n",
      "Epoch 444/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 2.1184 - acc: 0.1899 - val_loss: 2.0645 - val_acc: 0.2272\n",
      "Epoch 445/500\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 2.1254 - acc: 0.1861 - val_loss: 2.0695 - val_acc: 0.2310\n",
      "Epoch 446/500\n",
      "1/1 [==============================] - 0s 232ms/step - loss: 2.1272 - acc: 0.1832 - val_loss: 2.0739 - val_acc: 0.2367\n",
      "Epoch 447/500\n",
      "1/1 [==============================] - 0s 324ms/step - loss: 2.1255 - acc: 0.1870 - val_loss: 2.0720 - val_acc: 0.2405\n",
      "Epoch 448/500\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 2.1286 - acc: 0.1804 - val_loss: 2.0676 - val_acc: 0.2386\n",
      "Epoch 449/500\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 2.1271 - acc: 0.1878 - val_loss: 2.0630 - val_acc: 0.2443\n",
      "Epoch 450/500\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 2.1242 - acc: 0.1943 - val_loss: 2.0612 - val_acc: 0.2405\n",
      "Epoch 451/500\n",
      "1/1 [==============================] - 0s 246ms/step - loss: 2.1282 - acc: 0.1799 - val_loss: 2.0628 - val_acc: 0.2414\n",
      "Epoch 452/500\n",
      "1/1 [==============================] - 0s 238ms/step - loss: 2.1300 - acc: 0.1793 - val_loss: 2.0677 - val_acc: 0.2433\n",
      "Epoch 453/500\n",
      "1/1 [==============================] - 0s 237ms/step - loss: 2.1219 - acc: 0.1916 - val_loss: 2.0752 - val_acc: 0.2338\n",
      "Epoch 454/500\n",
      "1/1 [==============================] - 0s 246ms/step - loss: 2.1254 - acc: 0.1875 - val_loss: 2.0803 - val_acc: 0.2329\n",
      "Epoch 455/500\n",
      "1/1 [==============================] - 0s 236ms/step - loss: 2.1259 - acc: 0.1889 - val_loss: 2.0801 - val_acc: 0.2291\n",
      "Epoch 456/500\n",
      "1/1 [==============================] - 0s 237ms/step - loss: 2.1288 - acc: 0.1935 - val_loss: 2.0754 - val_acc: 0.2329\n",
      "Epoch 457/500\n",
      "1/1 [==============================] - 0s 241ms/step - loss: 2.1257 - acc: 0.1943 - val_loss: 2.0697 - val_acc: 0.2452\n",
      "Epoch 458/500\n",
      "1/1 [==============================] - 0s 244ms/step - loss: 2.1191 - acc: 0.1976 - val_loss: 2.0644 - val_acc: 0.2443\n",
      "Epoch 459/500\n",
      "1/1 [==============================] - 0s 250ms/step - loss: 2.1266 - acc: 0.1791 - val_loss: 2.0620 - val_acc: 0.2452\n",
      "Epoch 460/500\n",
      "1/1 [==============================] - 0s 251ms/step - loss: 2.1222 - acc: 0.1916 - val_loss: 2.0641 - val_acc: 0.2433\n",
      "Epoch 461/500\n",
      "1/1 [==============================] - 0s 238ms/step - loss: 2.1262 - acc: 0.1910 - val_loss: 2.0684 - val_acc: 0.2281\n",
      "Epoch 462/500\n",
      "1/1 [==============================] - 0s 229ms/step - loss: 2.1273 - acc: 0.1842 - val_loss: 2.0711 - val_acc: 0.2310\n",
      "Epoch 463/500\n",
      "1/1 [==============================] - 0s 211ms/step - loss: 2.1277 - acc: 0.1875 - val_loss: 2.0682 - val_acc: 0.2291\n",
      "Epoch 464/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 2.1268 - acc: 0.1886 - val_loss: 2.0653 - val_acc: 0.2329\n",
      "Epoch 465/500\n",
      "1/1 [==============================] - 0s 210ms/step - loss: 2.1240 - acc: 0.1946 - val_loss: 2.0626 - val_acc: 0.2348\n",
      "Epoch 466/500\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 2.1264 - acc: 0.1826 - val_loss: 2.0602 - val_acc: 0.2395\n",
      "Epoch 467/500\n",
      "1/1 [==============================] - 0s 213ms/step - loss: 2.1271 - acc: 0.1897 - val_loss: 2.0614 - val_acc: 0.2405\n",
      "Epoch 468/500\n",
      "1/1 [==============================] - 0s 207ms/step - loss: 2.1282 - acc: 0.1834 - val_loss: 2.0653 - val_acc: 0.2357\n",
      "Epoch 469/500\n",
      "1/1 [==============================] - 0s 209ms/step - loss: 2.1280 - acc: 0.1921 - val_loss: 2.0698 - val_acc: 0.2329\n",
      "Epoch 470/500\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 2.1307 - acc: 0.1875 - val_loss: 2.0690 - val_acc: 0.2395\n",
      "Epoch 471/500\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 2.1229 - acc: 0.1870 - val_loss: 2.0674 - val_acc: 0.2386\n",
      "Epoch 472/500\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 2.1222 - acc: 0.1973 - val_loss: 2.0651 - val_acc: 0.2386\n",
      "Epoch 473/500\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 2.1276 - acc: 0.1859 - val_loss: 2.0624 - val_acc: 0.2433\n",
      "Epoch 474/500\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 2.1230 - acc: 0.1916 - val_loss: 2.0591 - val_acc: 0.2443\n",
      "Epoch 475/500\n",
      "1/1 [==============================] - 0s 208ms/step - loss: 2.1232 - acc: 0.1924 - val_loss: 2.0585 - val_acc: 0.2424\n",
      "Epoch 476/500\n",
      "1/1 [==============================] - 0s 212ms/step - loss: 2.1282 - acc: 0.1913 - val_loss: 2.0602 - val_acc: 0.2405\n",
      "Epoch 477/500\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 2.1189 - acc: 0.1937 - val_loss: 2.0608 - val_acc: 0.2424\n",
      "Epoch 478/500\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 2.1314 - acc: 0.1851 - val_loss: 2.0607 - val_acc: 0.2367\n",
      "Epoch 479/500\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 2.1290 - acc: 0.1807 - val_loss: 2.0579 - val_acc: 0.2395\n",
      "Epoch 480/500\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 2.1230 - acc: 0.1962 - val_loss: 2.0549 - val_acc: 0.2424\n",
      "Epoch 481/500\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 2.1207 - acc: 0.1921 - val_loss: 2.0546 - val_acc: 0.2433\n",
      "Epoch 482/500\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 2.1239 - acc: 0.1872 - val_loss: 2.0539 - val_acc: 0.2443\n",
      "Epoch 483/500\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 2.1277 - acc: 0.1804 - val_loss: 2.0579 - val_acc: 0.2443\n",
      "Epoch 484/500\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 2.1222 - acc: 0.1815 - val_loss: 2.0613 - val_acc: 0.2395\n",
      "Epoch 485/500\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 2.1221 - acc: 0.1891 - val_loss: 2.0648 - val_acc: 0.2348\n",
      "Epoch 486/500\n",
      "1/1 [==============================] - 0s 202ms/step - loss: 2.1208 - acc: 0.1883 - val_loss: 2.0710 - val_acc: 0.2433\n",
      "Epoch 487/500\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 2.1209 - acc: 0.1891 - val_loss: 2.0725 - val_acc: 0.2452\n",
      "Epoch 488/500\n",
      "1/1 [==============================] - 0s 206ms/step - loss: 2.1246 - acc: 0.1954 - val_loss: 2.0709 - val_acc: 0.2481\n",
      "Epoch 489/500\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 2.1218 - acc: 0.2005 - val_loss: 2.0654 - val_acc: 0.2452\n",
      "Epoch 490/500\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 2.1212 - acc: 0.2008 - val_loss: 2.0602 - val_acc: 0.2386\n",
      "Epoch 491/500\n",
      "1/1 [==============================] - 0s 198ms/step - loss: 2.1297 - acc: 0.1815 - val_loss: 2.0548 - val_acc: 0.2338\n",
      "Epoch 492/500\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 2.1277 - acc: 0.1886 - val_loss: 2.0512 - val_acc: 0.2376\n",
      "Epoch 493/500\n",
      "1/1 [==============================] - 0s 200ms/step - loss: 2.1310 - acc: 0.1804 - val_loss: 2.0517 - val_acc: 0.2367\n",
      "Epoch 494/500\n",
      "1/1 [==============================] - 0s 199ms/step - loss: 2.1219 - acc: 0.1853 - val_loss: 2.0575 - val_acc: 0.2329\n",
      "Epoch 495/500\n",
      "1/1 [==============================] - 0s 203ms/step - loss: 2.1258 - acc: 0.1899 - val_loss: 2.0659 - val_acc: 0.2424\n",
      "Epoch 496/500\n",
      "1/1 [==============================] - 0s 201ms/step - loss: 2.1257 - acc: 0.1967 - val_loss: 2.0758 - val_acc: 0.2367\n",
      "Epoch 497/500\n",
      "1/1 [==============================] - 0s 189ms/step - loss: 2.1216 - acc: 0.1878 - val_loss: 2.0800 - val_acc: 0.2405\n",
      "Epoch 498/500\n",
      "1/1 [==============================] - 0s 190ms/step - loss: 2.1261 - acc: 0.1905 - val_loss: 2.0755 - val_acc: 0.2329\n",
      "Epoch 499/500\n",
      "1/1 [==============================] - 0s 196ms/step - loss: 2.1284 - acc: 0.1821 - val_loss: 2.0658 - val_acc: 0.2348\n",
      "Epoch 500/500\n",
      "1/1 [==============================] - 0s 205ms/step - loss: 2.1261 - acc: 0.1940 - val_loss: 2.0546 - val_acc: 0.2376\n"
     ]
    }
   ],
   "source": [
    "b2_embeddings = []\n",
    "print(len(b2_text))\n",
    "\n",
    "for t2 in b2_text:\n",
    "    b2_embeddings.append(Loaded_model.wv[t2])\n",
    "\n",
    "b2_embeddings=np.array(b2_embeddings) \n",
    "print(np.shape(b2_embeddings))\n",
    "print(np.shape(A2))\n",
    "\n",
    "\n",
    "history = model.fit([b2_embeddings, A2],\n",
    "                    b2_encodings,\n",
    "                    epochs=500,\n",
    "                    batch_size=N,\n",
    "                    # class_weight=W,\n",
    "                    validation_data=([valid_embeddings, A4], valid_encodings),\n",
    "                    # callbacks=[callback]\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Batch-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of A3 is:  (3960, 3960)\n",
      "Shape of embeddings is:  (3960, 100)\n",
      "Shape of train encodings is:  (3960,)\n",
      "Epoch 1/500\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": " Matrix size-incompatible: In[0]: [3718,3960], In[1]: [3718,32]\n\t [[node model_5/gcn_conv_10/BiasAdd\n (defined at C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\keras\\backend.py:6134)\n]] [Op:__inference_train_function_87970]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node model_5/gcn_conv_10/BiasAdd:\nIn[0] model_5/gcn_conv_10/MatMul_1 (defined at C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\spektral\\layers\\ops\\matmul.py:58)\t\nIn[1] model_5/gcn_conv_10/BiasAdd/ReadVariableOp:\n\nOperation defined at: (most recent call last)\n>>>   File \"c:\\Users\\ahsan\\.vscode\\extensions\\ms-toolsai.jupyter-2021.11.1001550889\\pythonFiles\\vscode_datascience_helpers\\kernel_prewarm_starter.py\", line 31, in <module>\n>>>     runpy.run_module(module, run_name=\"__main__\", alter_sys=False)\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\runpy.py\", line 210, in run_module\n>>>     return _run_code(code, {}, init_globals, run_name, mod_spec)\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\runpy.py\", line 87, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\traitlets\\config\\application.py\", line 845, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 667, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\asyncio\\base_events.py\", line 570, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\asyncio\\base_events.py\", line 1859, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\asyncio\\events.py\", line 81, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 457, in dispatch_queue\n>>>     await self.process_one()\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 446, in process_one\n>>>     await dispatch(*args)\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 353, in dispatch_shell\n>>>     await result\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 648, in execute_request\n>>>     reply_content = await reply_content\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 345, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n>>>     return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2898, in run_cell\n>>>     result = self._run_cell(\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2944, in _run_cell\n>>>     return runner(coro)\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n>>>     coro.send(None)\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3169, in run_cell_async\n>>>     has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3361, in run_ast_nodes\n>>>     if (await self.run_code(code, result,  async_=asy)):\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3441, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"C:\\Users\\ahsan\\AppData\\Local\\Temp/ipykernel_8684/1322795186.py\", line 14, in <module>\n>>>     history = model.fit([b3_embeddings, A3],\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\keras\\engine\\training.py\", line 1216, in fit\n>>>     tmp_logs = self.train_function(iterator)\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\keras\\engine\\training.py\", line 878, in train_function\n>>>     return step_function(self, iterator)\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\keras\\engine\\training.py\", line 867, in step_function\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in run_step\n>>>     outputs = model.train_step(data)\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\keras\\engine\\training.py\", line 808, in train_step\n>>>     y_pred = self(x, training=True)\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1083, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\keras\\engine\\functional.py\", line 451, in call\n>>>     return self._run_internal_graph(\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\keras\\engine\\functional.py\", line 589, in _run_internal_graph\n>>>     outputs = node.layer(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1083, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\spektral\\layers\\convolutional\\conv.py\", line 100, in _inner_check_dtypes\n>>>     output = [_ for _ in [x, a, e] if _ is not None]\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\spektral\\layers\\convolutional\\gcn_conv.py\", line 103, in call\n>>>     if self.use_bias:\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\spektral\\layers\\convolutional\\gcn_conv.py\", line 104, in call\n>>>     output = K.bias_add(output, self.bias)\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\keras\\backend.py\", line 6134, in bias_add\n>>>     return tf.nn.bias_add(x, bias, data_format='NHWC')\n>>> ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8684/111015962.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m history = model.fit([b3_embeddings, A3],\n\u001b[0m\u001b[0;32m     15\u001b[0m                     \u001b[0mb3_encodings\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Graphs\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Graphs\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     56\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     60\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m:  Matrix size-incompatible: In[0]: [3718,3960], In[1]: [3718,32]\n\t [[node model_5/gcn_conv_10/BiasAdd\n (defined at C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\keras\\backend.py:6134)\n]] [Op:__inference_train_function_87970]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node model_5/gcn_conv_10/BiasAdd:\nIn[0] model_5/gcn_conv_10/MatMul_1 (defined at C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\spektral\\layers\\ops\\matmul.py:58)\t\nIn[1] model_5/gcn_conv_10/BiasAdd/ReadVariableOp:\n\nOperation defined at: (most recent call last)\n>>>   File \"c:\\Users\\ahsan\\.vscode\\extensions\\ms-toolsai.jupyter-2021.11.1001550889\\pythonFiles\\vscode_datascience_helpers\\kernel_prewarm_starter.py\", line 31, in <module>\n>>>     runpy.run_module(module, run_name=\"__main__\", alter_sys=False)\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\runpy.py\", line 210, in run_module\n>>>     return _run_code(code, {}, init_globals, run_name, mod_spec)\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\runpy.py\", line 87, in _run_code\n>>>     exec(code, run_globals)\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n>>>     app.launch_new_instance()\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\traitlets\\config\\application.py\", line 845, in launch_instance\n>>>     app.start()\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 667, in start\n>>>     self.io_loop.start()\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n>>>     self.asyncio_loop.run_forever()\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\asyncio\\base_events.py\", line 570, in run_forever\n>>>     self._run_once()\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\asyncio\\base_events.py\", line 1859, in _run_once\n>>>     handle._run()\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\asyncio\\events.py\", line 81, in _run\n>>>     self._context.run(self._callback, *self._args)\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 457, in dispatch_queue\n>>>     await self.process_one()\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 446, in process_one\n>>>     await dispatch(*args)\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 353, in dispatch_shell\n>>>     await result\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 648, in execute_request\n>>>     reply_content = await reply_content\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 345, in do_execute\n>>>     res = shell.run_cell(code, store_history=store_history, silent=silent)\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 532, in run_cell\n>>>     return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2898, in run_cell\n>>>     result = self._run_cell(\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2944, in _run_cell\n>>>     return runner(coro)\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n>>>     coro.send(None)\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3169, in run_cell_async\n>>>     has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3361, in run_ast_nodes\n>>>     if (await self.run_code(code, result,  async_=asy)):\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3441, in run_code\n>>>     exec(code_obj, self.user_global_ns, self.user_ns)\n>>> \n>>>   File \"C:\\Users\\ahsan\\AppData\\Local\\Temp/ipykernel_8684/1322795186.py\", line 14, in <module>\n>>>     history = model.fit([b3_embeddings, A3],\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\keras\\engine\\training.py\", line 1216, in fit\n>>>     tmp_logs = self.train_function(iterator)\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\keras\\engine\\training.py\", line 878, in train_function\n>>>     return step_function(self, iterator)\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\keras\\engine\\training.py\", line 867, in step_function\n>>>     outputs = model.distribute_strategy.run(run_step, args=(data,))\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\keras\\engine\\training.py\", line 860, in run_step\n>>>     outputs = model.train_step(data)\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\keras\\engine\\training.py\", line 808, in train_step\n>>>     y_pred = self(x, training=True)\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1083, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\keras\\engine\\functional.py\", line 451, in call\n>>>     return self._run_internal_graph(\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\keras\\engine\\functional.py\", line 589, in _run_internal_graph\n>>>     outputs = node.layer(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 64, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\keras\\engine\\base_layer.py\", line 1083, in __call__\n>>>     outputs = call_fn(inputs, *args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 92, in error_handler\n>>>     return fn(*args, **kwargs)\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\spektral\\layers\\convolutional\\conv.py\", line 100, in _inner_check_dtypes\n>>>     output = [_ for _ in [x, a, e] if _ is not None]\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\spektral\\layers\\convolutional\\gcn_conv.py\", line 103, in call\n>>>     if self.use_bias:\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\spektral\\layers\\convolutional\\gcn_conv.py\", line 104, in call\n>>>     output = K.bias_add(output, self.bias)\n>>> \n>>>   File \"C:\\Users\\ahsan\\anaconda3\\envs\\Graphs\\lib\\site-packages\\keras\\backend.py\", line 6134, in bias_add\n>>>     return tf.nn.bias_add(x, bias, data_format='NHWC')\n>>> "
     ]
    }
   ],
   "source": [
    "b3_embeddings = []\n",
    "for t3 in b3_text:\n",
    "    b3_embeddings.append(Loaded_model.wv[t3])\n",
    "\n",
    "# print(len(b3_embeddings))\n",
    "b3_embeddings = np.array(b3_embeddings)\n",
    "\n",
    "\n",
    "print(\"Shape of A3 is: \", np.shape(A3))\n",
    "print(\"Shape of embeddings is: \", np.shape(b3_embeddings))\n",
    "print(\"Shape of train encodings is: \", np.shape(b3_encodings))\n",
    "\n",
    "\n",
    "# history = model.fit([b3_embeddings, A3],\n",
    "#                     b3_encodings,\n",
    "#                     epochs=500,\n",
    "#                     batch_size=N,\n",
    "#                     # class_weight=W,\n",
    "#                     validation_data=([valid_embeddings, A4], valid_encodings),\n",
    "#                     # callbacks=[callback]\n",
    "#                     )\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "35146d2ce121e8e653791712ad16cca408ea08bd1c84e92938f653a743d84e6c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('Graphs': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
