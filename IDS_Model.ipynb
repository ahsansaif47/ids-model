{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data-Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Label files are:  110\n",
      "Training files are:  90\n",
      "Validation files are:  5\n",
      "Testing files are:  5\n"
     ]
    }
   ],
   "source": [
    "# Dataset loading\n",
    "import os\n",
    "\n",
    "dataset = \"./New-Dataset/\"\n",
    "# dataset = \"./Dataset (Labelled Images)/\"\n",
    "lab_files_path = dataset+\"labels/\"\n",
    "# lab_files_path = dataset+\"label/\"\n",
    "\n",
    "# total invoice files\n",
    "files = os.listdir(lab_files_path)\n",
    "\n",
    "\n",
    "# training set\n",
    "train = files[:90]\n",
    "\n",
    "# validation set\n",
    "valid_set = files[90:95]\n",
    "\n",
    "# test set\n",
    "test = files[95:100]\n",
    "\n",
    "print(\"Total Label files are: \", len(files))\n",
    "print(\"Training files are: \", len(train))\n",
    "print(\"Validation files are: \", len(valid_set))\n",
    "print(\"Testing files are: \", len(test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geometric Algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL.Image import Image\n",
    "from networkx.algorithms.shortest_paths.weighted import single_source_dijkstra\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.core.frame import DataFrame\n",
    "from PIL import Image\n",
    "import networkx as nx\n",
    "\n",
    "df = 0\n",
    "xMIN, xMAX = [], []\n",
    "yMIN, yMAX = [], []\n",
    "Text = []\n",
    "\n",
    "\n",
    "def findRight(df, df_ind, xMIN, xMAX, yMIN, yMAX):\n",
    "    S_list = []\n",
    "    xmax = xMAX[df_ind]\n",
    "    ymin = yMIN[df_ind]\n",
    "    ymax = yMAX[df_ind]\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        if(xMIN[i] > xmax):\n",
    "            if not (yMIN[i] > ymax or yMAX[i] < ymin):\n",
    "                if(yMIN[i] <= ymin and yMAX[i] <= ymax):\n",
    "                    S_list.append(i)\n",
    "                elif (yMIN[i] <= ymin and yMAX[i] >= ymax):\n",
    "                    S_list.append(i)\n",
    "                elif (yMIN[i] >= ymin and yMAX[i] <= ymax):\n",
    "                    S_list.append(i)\n",
    "                elif (yMIN[i] >= ymin and yMAX[i] >= ymax):\n",
    "                    S_list.append(i)\n",
    "                elif (yMIN[i] == ymin and yMAX[i] == ymax):\n",
    "                    S_list.append(i)\n",
    "\n",
    "    if S_list:\n",
    "        consec = S_list[0]\n",
    "        for j in S_list:\n",
    "            if(xMIN[consec] > xMIN[j]):\n",
    "                consec = j\n",
    "        return consec\n",
    "\n",
    "    return -1\n",
    "\n",
    "\n",
    "def findLeft(df, df_ind, xMIN, xMAX, yMIN, yMAX):\n",
    "    S_list = []\n",
    "    xmin = xMIN[df_ind]\n",
    "    ymin = yMIN[df_ind]\n",
    "    ymax = yMAX[df_ind]\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        if(xMAX[i] < xmin):\n",
    "            if not (yMIN[i] > ymax or yMAX[i] < ymin):\n",
    "                if(yMIN[i] <= ymin and yMAX[i] <= ymax):\n",
    "                    S_list.append(i)\n",
    "                elif (yMIN[i] <= ymin and yMAX[i] >= ymax):\n",
    "                    S_list.append(i)\n",
    "                elif (yMIN[i] >= ymin and yMAX[i] <= ymax):\n",
    "                    S_list.append(i)\n",
    "                elif (yMIN[i] >= ymin and yMAX[i] >= ymax):\n",
    "                    S_list.append(i)\n",
    "                elif (yMIN[i] == ymin and yMAX[i] == ymax):\n",
    "                    S_list.append(i)\n",
    "\n",
    "    if S_list:\n",
    "        consec = S_list[0]\n",
    "        for j in S_list:\n",
    "            if(xMAX[j] > xMAX[consec]):\n",
    "                consec = j\n",
    "        return consec\n",
    "    return -1\n",
    "\n",
    "\n",
    "def findUp(df, df_ind, xMIN, xMAX, yMIN, yMAX):\n",
    "    S_list = []\n",
    "    xmin = xMIN[df_ind]\n",
    "    xmax = xMAX[df_ind]\n",
    "    ymin = yMIN[df_ind]\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        if(yMAX[i] < ymin):\n",
    "            if not (xMAX[i] < xmin or xMIN[i] > xmax):\n",
    "                if(xMIN[i] <= xmin and xMAX[i] <= xmax):\n",
    "                    S_list.append(i)\n",
    "                elif (xMIN[i] <= xmin and xMAX[i] >= xmax):\n",
    "                    S_list.append(i)\n",
    "                elif (xMIN[i] >= xmin and xMAX[i] <= xmax):\n",
    "                    S_list.append(i)\n",
    "                elif (xMIN[i] >= xmin and xMAX[i] >= xmax):\n",
    "                    S_list.append(i)\n",
    "                elif (xMIN[i] == xmin and xMAX[i] == xmax):\n",
    "                    S_list.append(i)\n",
    "\n",
    "    if S_list:\n",
    "        consec = S_list[0]\n",
    "        for j in S_list:\n",
    "            if(yMAX[j] > yMAX[consec]):\n",
    "                consec = j\n",
    "        return consec\n",
    "\n",
    "    return -1\n",
    "\n",
    "\n",
    "def findDown(df, df_ind, xMIN, xMAX, yMIN, yMAX):\n",
    "    S_list = []\n",
    "    xmin = xMIN[df_ind]\n",
    "    xmax = xMAX[df_ind]\n",
    "    ymax = yMAX[df_ind]\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        if(yMIN[i] > ymax):\n",
    "            if not (xMAX[i] < xmin or xMIN[i] > xmax):\n",
    "                if(xMIN[i] <= xmin and xMAX[i] <= xmax):\n",
    "                    S_list.append(i)\n",
    "                elif (xMIN[i] <= xmin and xMAX[i] >= xmax):\n",
    "                    S_list.append(i)\n",
    "                elif (xMIN[i] >= xmin and xMAX[i] <= xmax):\n",
    "                    S_list.append(i)\n",
    "                elif (xMIN[i] >= xmin and xMAX[i] >= xmax):\n",
    "                    S_list.append(i)\n",
    "                elif (xMIN[i] == xmin and xMAX[i] == xmax):\n",
    "                    S_list.append(i)\n",
    "\n",
    "    if S_list:\n",
    "        consec = S_list[0]\n",
    "        for j in S_list:\n",
    "            if(yMIN[j] < yMIN[consec]):\n",
    "                consec = j\n",
    "        return consec\n",
    "\n",
    "    return -1\n",
    "\n",
    "\n",
    "def makeGraph(df):\n",
    "    G = nx.Graph()\n",
    "    xMIN = df['xmin']\n",
    "    xMAX = df['xmax']\n",
    "    yMIN = df['ymin']\n",
    "    yMAX = df['ymax']\n",
    "    Text = df['Object']\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        if findUp(df, i, xMIN, xMAX, yMIN, yMAX):\n",
    "            l = findUp(df, i, xMIN, xMAX, yMIN, yMAX)\n",
    "            if(l != -1):\n",
    "                text = Text[l]\n",
    "                G.add_edge(Text[i], text)\n",
    "        if findRight(df, i, xMIN, xMAX, yMIN, yMAX):\n",
    "            l = findRight(df, i, xMIN, xMAX, yMIN, yMAX)\n",
    "            if (l != -1):\n",
    "                text = Text[l]\n",
    "                G.add_edge(Text[i], text)\n",
    "        if findDown(df, i, xMIN, xMAX, yMIN, yMAX):\n",
    "            l = findDown(df, i, xMIN, xMAX, yMIN, yMAX)\n",
    "            if (l != -1):\n",
    "                text = Text[l]\n",
    "                G.add_edge(Text[i], text)\n",
    "        if findLeft(df, i, xMIN, xMAX, yMIN, yMAX):\n",
    "            l = findLeft(df, i, xMIN, xMAX, yMIN, yMAX)\n",
    "            if (l != -1):\n",
    "                text = Text[l]\n",
    "                G.add_edge(Text[i], text)\n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrices Over Diagonal\n",
    "Place an incident Matrix over diagonal with existing matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import SupportsAbs\n",
    "from numpy.core.fromnumeric import shape\n",
    "import numpy as np\n",
    "\n",
    "# Test Matrices\n",
    "# mat1 = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "# mat2 = np.array([[3, 2, 1, 7], [6, 5, 4, 9], [9, 8, 7, 4], [1, 5, 7, 2]])\n",
    "\n",
    "\n",
    "def alignDiagonally(M1, M2, prev_Len):\n",
    "    for i in range(prev_Len, np.shape(M1)[0]):\n",
    "        for j in range(prev_Len, np.shape(M1)[0]):\n",
    "            x = i - prev_Len\n",
    "            y = j - prev_Len\n",
    "            M1[i][j] = M2[x][y]\n",
    "\n",
    "    return M1\n",
    "\n",
    "\n",
    "def resizeMatrix(M, I):\n",
    "    oldMat_Len = np.shape(M)[0]\n",
    "    z = np.zeros((oldMat_Len, np.shape(I)[0]), dtype=np.int64)\n",
    "    newArray = np.append(M, z, axis=1)\n",
    "    M = newArray\n",
    "\n",
    "    # Appending 1D arrays of zeros in the original Matrix\n",
    "    # (i.e. the matrix in which we want to align othe rmatrices diagonally)\n",
    "    appZero = np.shape(I)[0]\n",
    "    x = oldMat_Len + appZero\n",
    "    L = np.zeros((np.shape(I)[0], x), dtype=np.int64)\n",
    "    newArray = np.append(M, L, axis=0)\n",
    "    M = newArray\n",
    "\n",
    "    M = alignDiagonally(M, I, oldMat_Len)\n",
    "    return M\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from numpy.core.fromnumeric import shape\n",
    "\n",
    "Z_file = lab_files_path + train[0]\n",
    "df = pd.read_csv(Z_file)\n",
    "G = makeGraph(df)\n",
    "M1 = nx.to_numpy_array(G, dtype=np.int32)\n",
    "\n",
    "for i in range(1, len(train)):\n",
    "    print(\"\\n= = = = = = = = = = = = = = = =\")\n",
    "    print(\"Iteration No.: \", i)\n",
    "    # Getting file\n",
    "    f = lab_files_path + files[i]\n",
    "    # Making dataframe of file\n",
    "    df = pd.read_csv(f)\n",
    "    # Making graph of the dataframe.\n",
    "    G = makeGraph(df)\n",
    "    # Storing the graph as an incident matrix(an adjacency matrix)\n",
    "    I = nx.to_numpy_array(G, dtype=np.int32)\n",
    "    # Now resizing the original sparce matrix with the new incident matrix\n",
    "    M1 = resizeMatrix(M1, I)\n",
    "\n",
    "print(\"Dimentions of Batch 1 matrix is: \", np.shape(M1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z1_file = lab_files_path + valid_set[0]\n",
    "df = pd.read_csv(Z1_file)\n",
    "G = makeGraph(df)\n",
    "M2 = nx.to_numpy_array(G, dtype=np.int32)\n",
    "\n",
    "for i in range(1, len(valid_set)):\n",
    "    print(\"\\n= = = = = = = = = = = = = = = =\")\n",
    "    print(\"Iteration No.: \", i)\n",
    "    # Getting file\n",
    "    f = lab_files_path + valid_set[i]\n",
    "    # Making dataframe of file\n",
    "    df = pd.read_csv(f)\n",
    "    # Making graph of the dataframe.\n",
    "    G = makeGraph(df)\n",
    "    # Storing the graph as an incident matrix(an adjacency matrix)\n",
    "    I = nx.to_numpy_array(G, dtype=np.int32)\n",
    "    # Now resizing the original sparce matrix with the new incident matrix\n",
    "    M2 = resizeMatrix(M2, I)\n",
    "\n",
    "print(\"Dimentions of Batch 2 matrix is: \", np.shape(M2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z2_file = lab_files_path + test[0]\n",
    "df = pd.read_csv(Z2_file)\n",
    "G = makeGraph(df)\n",
    "M3 = nx.to_numpy_array(G, dtype=np.int32)\n",
    "\n",
    "for i in range(1, len(test)):\n",
    "    print(\"\\n= = = = = = = = = = = = = = = =\")\n",
    "    print(\"Iteration No.: \", i)\n",
    "    # Getting file\n",
    "    f = lab_files_path + test[i]\n",
    "    # Making dataframe of file\n",
    "    df = pd.read_csv(f)\n",
    "    # Making graph of the dataframe.\n",
    "    G = makeGraph(df)\n",
    "    # Storing the graph as an incident matrix(an adjacency matrix)\n",
    "    I = nx.to_numpy_array(G, dtype=np.int32)\n",
    "    # Now resizing the original sparce matrix with the new incident matrix\n",
    "    M3 = resizeMatrix(M3, I)\n",
    "\n",
    "print(\"Dimentions of Batch 3 matrix is: \", np.shape(M3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Saving Matrix\")\n",
    "# np.save(\"./Matrices/Matrix_b1.npy\", M1)\n",
    "np.save(\"./IDS Model File Matrices/Train.npy\", M1)\n",
    "np.save(\"./IDS Model File Matrices/Validation.npy\", M2)\n",
    "np.save(\"./IDS Model File Matrices/Test.npy\", M3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Training Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A1 = np.load(\"./IDS Model File Matrices/Train.npy\")\n",
    "A2 = np.load(\"./IDS Model File Matrices/Validation.npy\")\n",
    "A3 = np.load(\"./IDS Model File Matrices/Test.npy\")\n",
    "\n",
    "print(\"Printing Training Batch\")\n",
    "print(A1)\n",
    "print(\"Printing Validation Batch\")\n",
    "print(A2)\n",
    "print(\"Printing Test Batch\")\n",
    "print(A3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Train Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.matshow(A1)\n",
    "print(np.shape(A1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import nan\n",
    "\n",
    "\n",
    "def returnLabels(direc, files):\n",
    "    labels = []\n",
    "    for f in range(len(files)):\n",
    "        seenList = []\n",
    "        df = pd.read_csv(direc + files[f])\n",
    "        text = df['Object'].to_list()\n",
    "        T_labels = df['labels'].to_list()\n",
    "\n",
    "        for i in range(len(text)):\n",
    "            if(text[i] not in seenList):\n",
    "                labels.append(T_labels[i])\n",
    "            seenList.append(text[i])\n",
    "\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding Labels\n",
    "1. Get a list of labels from CSV files.\n",
    "2. Catagorically encode the labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "train_labels = returnLabels(lab_files_path, train)\n",
    "print(\"Total train labels are: \", len(train_labels))\n",
    "\n",
    "lab_encoder = LabelEncoder()\n",
    "train_encodings = lab_encoder.fit_transform(train_labels)\n",
    "print(\"Total train encodings are: \", len(train_encodings))\n",
    "\n",
    "\n",
    "uni_train_enc = set(train_encodings)\n",
    "print(\"Unique train encodings are: \", uni_train_enc)\n",
    "\n",
    "\n",
    "valid_labels = returnLabels(lab_files_path, valid_set)\n",
    "print(\"Total validation labels are: \", len(valid_labels))\n",
    "\n",
    "lab_encoder = LabelEncoder()\n",
    "valid_encodings = lab_encoder.fit_transform(valid_labels)\n",
    "print(\"Total train encodings are: \", len(valid_encodings))\n",
    "\n",
    "\n",
    "uni_valid_enc = set(valid_encodings)\n",
    "print(\"Unique validation encodings are: \", uni_valid_enc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras import Sequential\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dropout, Dense, Flatten, Activation\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "\n",
    "from spektral.utils.sparse import sp_matrix_to_sp_tensor\n",
    "from spektral.layers import GCNConv\n",
    "from sklearn.metrics import classification_report\n",
    "from spektral.utils import normalized_laplacian\n",
    "from spektral.utils.sparse import sp_matrix_to_sp_tensor\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "print(\"Libraries Imported..\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "channels = 32\n",
    "dropout = 0.5\n",
    "learning_rate = 5e-4\n",
    "l2_reg = 0.001\n",
    "batch_size = 16\n",
    "es_patience = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings for train matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "Emb_Model = \"./Models/Word2Vec_Model.bin\"\n",
    "Loaded_model = Word2Vec.load(Emb_Model)\n",
    "\n",
    "train_text = []\n",
    "for i in os.listdir(lab_files_path):\n",
    "    df = pd.read_csv(lab_files_path+i)\n",
    "    Text = df['Object'].to_list()\n",
    "    \n",
    "    for T in Text:\n",
    "        train_text.append(T)\n",
    "    \n",
    "\n",
    "_embeddings = []\n",
    "for t in train_text:\n",
    "    _embeddings.append(Emb_Model.wv[t])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = A1\n",
    "N = A.shape[0]\n",
    "b1_features = _embeddings\n",
    "F = b1_features.shape[1]\n",
    "\n",
    "classes = 12\n",
    "\n",
    "# X_in = Input(batch_size=N, shape=(F))\n",
    "X_in = Input(batch_size=None, shape=(F,))\n",
    "print(\"Shape of X-in is: \", np.shape(X_in))\n",
    "\n",
    "# A_in = Input(tensor=sp_matrix_to_sp_tensor(fltr), sparse=True)\n",
    "# A_in = Input(shape=(None,), sparse=True)\n",
    "A_in = Input(shape=(None,))\n",
    "print(\"Shape of A-in is: \", np.shape(A_in))\n",
    "\n",
    "# D1 = Dropout(dropout)(X_in)\n",
    "G1 = GCNConv(channels, activation='LeakyReLU',\n",
    "             kernel_regularizer=l2(l2_reg), use_bias=True)([X_in, A_in])\n",
    "# G1 = GCNConv(channels, activation='LeakyReLU', use_bias=True)([X_in, A_in])\n",
    "# D2 = Dropout(dropout)(G1)\n",
    "G2 = GCNConv(channels, activation='LeakyReLU',\n",
    "             kernel_regularizer=l2(l2_reg), use_bias=True)([G1, A_in])\n",
    "# G2 = GCNConv(channels, activation='LeakyReLU', use_bias=True)([G1, A_in])\n",
    "\n",
    "# # BN = BatchNormalization()(G2)\n",
    "flatten = Flatten()(G2)\n",
    "\n",
    "D1 = Dense(128, activation='relu')(flatten)\n",
    "Dr1 = Dropout(dropout)(D1)\n",
    "D2 = Dense(32, activation='relu')(Dr1)\n",
    "Dr2 = Dropout(dropout)(D2)\n",
    "# D3 = Dense(16, activation='LeakyReLU')(D2)\n",
    "\n",
    "D2_out = Dense(classes, activation='softmax')(Dr2)\n",
    "\n",
    "model = Model(inputs=[X_in, A_in], outputs=D2_out)\n",
    "# other loss = sparse_categorical_crossentropy\n",
    "# model.compile(optimizer='Adagrad',loss='sparse_categorical_crossentropy', weighted_metrics=['acc'])\n",
    "model.compile(optimizer='Adam',\n",
    "              loss='sparse_categorical_crossentropy', weighted_metrics=['acc'])\n",
    "\n",
    "model.summary()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "35146d2ce121e8e653791712ad16cca408ea08bd1c84e92938f653a743d84e6c"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('Graphs': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
