{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 4,
    "language_info": {
      "name": "python",
      "version": "3.8.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.10 64-bit ('Graphs': conda)"
    },
    "interpreter": {
      "hash": "35146d2ce121e8e653791712ad16cca408ea08bd1c84e92938f653a743d84e6c"
    },
    "colab": {
      "name": "IDS_Model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcCNAgviN6Ki"
      },
      "source": [
        "# Mounting Drive\n",
        "Mounting drive to fetch dataset and other resources from the drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmchhkO9S-1R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f886fcd9-25be-4fa6-da74-16cc357334fe"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caLVJ7fVTETv"
      },
      "source": [
        "# Installing Spektral Library\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pbGlTrXTIl8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abd5c6af-7c42-4b04-ca06-72efae44e57e"
      },
      "source": [
        "!pip install git+https://github.com/danielegrattarola/spektral"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/danielegrattarola/spektral\n",
            "  Cloning https://github.com/danielegrattarola/spektral to /tmp/pip-req-build-3_htqn45\n",
            "  Running command git clone -q https://github.com/danielegrattarola/spektral /tmp/pip-req-build-3_htqn45\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from spektral==1.0.8) (1.0.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from spektral==1.0.8) (4.2.6)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from spektral==1.0.8) (2.6.2)\n",
            "Requirement already satisfied: numpy<1.20 in /usr/local/lib/python3.7/dist-packages (from spektral==1.0.8) (1.19.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from spektral==1.0.8) (1.1.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from spektral==1.0.8) (2.23.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from spektral==1.0.8) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from spektral==1.0.8) (1.4.1)\n",
            "Requirement already satisfied: tensorflow>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from spektral==1.0.8) (2.6.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from spektral==1.0.8) (4.62.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral==1.0.8) (1.39.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral==1.0.8) (1.12.1)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral==1.0.8) (1.12)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral==1.0.8) (0.4.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral==1.0.8) (1.15.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral==1.0.8) (0.12.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral==1.0.8) (0.37.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral==1.0.8) (3.7.4.3)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral==1.0.8) (0.2.0)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral==1.0.8) (3.1.0)\n",
            "Requirement already satisfied: keras~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral==1.0.8) (2.6.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral==1.0.8) (1.1.2)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral==1.0.8) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral==1.0.8) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-estimator~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral==1.0.8) (2.6.0)\n",
            "Requirement already satisfied: clang~=5.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral==1.0.8) (5.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral==1.0.8) (1.6.3)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral==1.0.8) (2.6.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral==1.0.8) (3.17.3)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow>=2.1.0->spektral==1.0.8) (1.5.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.1.0->spektral==1.0.8) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.1.0->spektral==1.0.8) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.1.0->spektral==1.0.8) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.1.0->spektral==1.0.8) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.1.0->spektral==1.0.8) (0.4.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.1.0->spektral==1.0.8) (3.3.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow>=2.1.0->spektral==1.0.8) (1.34.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow>=2.1.0->spektral==1.0.8) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow>=2.1.0->spektral==1.0.8) (4.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow>=2.1.0->spektral==1.0.8) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=2.1.0->spektral==1.0.8) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow>=2.1.0->spektral==1.0.8) (4.6.4)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow>=2.1.0->spektral==1.0.8) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->spektral==1.0.8) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->spektral==1.0.8) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->spektral==1.0.8) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->spektral==1.0.8) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow>=2.1.0->spektral==1.0.8) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.6->tensorflow>=2.1.0->spektral==1.0.8) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->spektral==1.0.8) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->spektral==1.0.8) (2018.9)\n",
            "Building wheels for collected packages: spektral\n",
            "  Building wheel for spektral (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for spektral: filename=spektral-1.0.8-py3-none-any.whl size=123417 sha256=a5240f2a3934b9da9c1aced11b10a927234d2e7e41bf3e0a68b8526db13b922f\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-xs0arq3t/wheels/af/7c/1f/e06aba9c0f493bb708968b8b396fe7523fdfb1c1c0818730be\n",
            "Successfully built spektral\n",
            "Installing collected packages: spektral\n",
            "Successfully installed spektral-1.0.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbVDhozDN0Nd"
      },
      "source": [
        "# Loading Label Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xm6klvIIN0Nk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88c8c423-4af6-40e7-d8b2-2cb19962f371"
      },
      "source": [
        "# Dataset loading\n",
        "import os\n",
        "\n",
        "dataset = \"drive/MyDrive/Project GCN Dataset/Dataset (Labelled Images)/\"\n",
        "lab_files_path = dataset+\"label/\"\n",
        "\n",
        "files = os.listdir(lab_files_path)\n",
        "print(\"Total Label files are: \", len(files))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Label files are:  129\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEkBSTLnN0No"
      },
      "source": [
        "# Sparse Matrix Creation\n",
        "\n",
        "Iterate all the csv files and do the following three steps:\n",
        "1. Make a graph of CSV file.\n",
        "2. Turn the graph into adjacency matrix.\n",
        "3. Append that adjacency matrix into sparce matrix(i.e. bigger matrix)\n",
        "\n",
        "# DON'T RUN THIS TAKES A HELL LOT OF TIME\n",
        "# SPARCE MATRIX IS ALREADY CREATED AND SAVED"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJwo6iIWN0Np"
      },
      "source": [
        "import pandas as pd\n",
        "import Mat_Package.Grapher as GMaker\n",
        "import Mat_Package.MatricesOverDiagonal as DiagPlacer\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "from numpy.core.fromnumeric import shape\n",
        "\n",
        "\n",
        "Z_file = lab_files_path + files[0]\n",
        "df = pd.read_csv(Z_file)\n",
        "G = GMaker.makeGraph(df)\n",
        "M = nx.to_numpy_array(G, dtype=np.int32)\n",
        "\n",
        "\n",
        "for i in range(1, len(files)):\n",
        "    print(\"\\n= = = = = = = = = = = = = = = =\")\n",
        "    print(\"Iteration No.: \", i)\n",
        "    # Getting file\n",
        "    f = lab_files_path + files[i]\n",
        "    # Making dataframe of file\n",
        "    df = pd.read_csv(f)\n",
        "    # Making graph of the dataframe. \n",
        "    G = GMaker.makeGraph(df)\n",
        "    # Storing the graph as an incident matrix(an adjacency matrix)\n",
        "    I = nx.to_numpy_array(G, dtype=np.int32)\n",
        "    # Now resizing the original sparce matrix with the new incident matrix\n",
        "    M = DiagPlacer.resizeMatrix(M, I)\n",
        "    \n",
        "print(\"Final Matix Done\")\n",
        "print(\"Dimentions of sparce matrix are: \", np.shape(M))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nomU0PgWZLqu"
      },
      "source": [
        "# Geometric Algorithms for converting dataframes to Graphs\n",
        "It connects a node with its neighbouring nodes  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QTxAf2dxkJ_8"
      },
      "source": [
        "from PIL.Image import Image\n",
        "import networkx as nx\n",
        "from networkx.algorithms.shortest_paths.weighted import single_source_dijkstra\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pandas.core.frame import DataFrame\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "# csv = './A-10.csv'\n",
        "# df = pd.read_csv(csv)\n",
        "df = 0\n",
        "xMIN, xMAX = [], []\n",
        "yMIN, yMAX = [], []\n",
        "Text = []\n",
        "\n",
        "\n",
        "def findRight(df, df_ind, xMIN, xMAX, yMIN, yMAX):\n",
        "    S_list = []\n",
        "    xmax = xMAX[df_ind]\n",
        "    ymin = yMIN[df_ind]\n",
        "    ymax = yMAX[df_ind]\n",
        "\n",
        "    for i in range(len(df)):\n",
        "        if(xMIN[i] > xmax):\n",
        "            if not (yMIN[i] > ymax or yMAX[i] < ymin):\n",
        "                if(yMIN[i] <= ymin and yMAX[i] <= ymax):\n",
        "                    S_list.append(i)\n",
        "                elif (yMIN[i] <= ymin and yMAX[i] >= ymax):\n",
        "                    S_list.append(i)\n",
        "                elif (yMIN[i] >= ymin and yMAX[i] <= ymax):\n",
        "                    S_list.append(i)\n",
        "                elif (yMIN[i] >= ymin and yMAX[i] >= ymax):\n",
        "                    S_list.append(i)\n",
        "                elif (yMIN[i] == ymin and yMAX[i] == ymax):\n",
        "                    S_list.append(i)\n",
        "\n",
        "    # print(S_list)\n",
        "    if S_list:\n",
        "        consec = S_list[0]\n",
        "        for j in S_list:\n",
        "            if(xMIN[consec] > xMIN[j]):\n",
        "                consec = j\n",
        "        return consec\n",
        "\n",
        "    return -1\n",
        "\n",
        "\n",
        "def findLeft(df, df_ind, xMIN, xMAX, yMIN, yMAX):\n",
        "    S_list = []\n",
        "    xmin = xMIN[df_ind]\n",
        "    ymin = yMIN[df_ind]\n",
        "    ymax = yMAX[df_ind]\n",
        "\n",
        "    for i in range(len(df)):\n",
        "        if(xMAX[i] < xmin):\n",
        "            if not (yMIN[i] > ymax or yMAX[i] < ymin):\n",
        "                if(yMIN[i] <= ymin and yMAX[i] <= ymax):\n",
        "                    S_list.append(i)\n",
        "                elif (yMIN[i] <= ymin and yMAX[i] >= ymax):\n",
        "                    S_list.append(i)\n",
        "                elif (yMIN[i] >= ymin and yMAX[i] <= ymax):\n",
        "                    S_list.append(i)\n",
        "                elif (yMIN[i] >= ymin and yMAX[i] >= ymax):\n",
        "                    S_list.append(i)\n",
        "                elif (yMIN[i] == ymin and yMAX[i] == ymax):\n",
        "                    S_list.append(i)\n",
        "    # print(S_list)\n",
        "    if S_list:\n",
        "        consec = S_list[0]\n",
        "        for j in S_list:\n",
        "            if(xMAX[j] > xMAX[consec]):\n",
        "                consec = j\n",
        "        return consec\n",
        "    return -1\n",
        "\n",
        "\n",
        "def findUp(df, df_ind, xMIN, xMAX, yMIN, yMAX):\n",
        "    S_list = []\n",
        "    xmin = xMIN[df_ind]\n",
        "    xmax = xMAX[df_ind]\n",
        "    ymin = yMIN[df_ind]\n",
        "\n",
        "    for i in range(len(df)):\n",
        "        if(yMAX[i] < ymin):\n",
        "            if not (xMAX[i] < xmin or xMIN[i] > xmax):\n",
        "                if(xMIN[i] <= xmin and xMAX[i] <= xmax):\n",
        "                    S_list.append(i)\n",
        "                elif (xMIN[i] <= xmin and xMAX[i] >= xmax):\n",
        "                    S_list.append(i)\n",
        "                elif (xMIN[i] >= xmin and xMAX[i] <= xmax):\n",
        "                    S_list.append(i)\n",
        "                elif (xMIN[i] >= xmin and xMAX[i] >= xmax):\n",
        "                    S_list.append(i)\n",
        "                elif (xMIN[i] == xmin and xMAX[i] == xmax):\n",
        "                    S_list.append(i)\n",
        "    # print(S_list)\n",
        "\n",
        "    if S_list:\n",
        "        consec = S_list[0]\n",
        "        for j in S_list:\n",
        "            if(yMAX[j] > yMAX[consec]):\n",
        "                consec = j\n",
        "        return consec\n",
        "\n",
        "    return -1\n",
        "\n",
        "\n",
        "def findDown(df, df_ind, xMIN, xMAX, yMIN, yMAX):\n",
        "    S_list = []\n",
        "    xmin = xMIN[df_ind]\n",
        "    xmax = xMAX[df_ind]\n",
        "    ymax = yMAX[df_ind]\n",
        "\n",
        "    for i in range(len(df)):\n",
        "        if(yMIN[i] > ymax):\n",
        "            if not (xMAX[i] < xmin or xMIN[i] > xmax):\n",
        "                if(xMIN[i] <= xmin and xMAX[i] <= xmax):\n",
        "                    S_list.append(i)\n",
        "                elif (xMIN[i] <= xmin and xMAX[i] >= xmax):\n",
        "                    S_list.append(i)\n",
        "                elif (xMIN[i] >= xmin and xMAX[i] <= xmax):\n",
        "                    S_list.append(i)\n",
        "                elif (xMIN[i] >= xmin and xMAX[i] >= xmax):\n",
        "                    S_list.append(i)\n",
        "                elif (xMIN[i] == xmin and xMAX[i] == xmax):\n",
        "                    S_list.append(i)\n",
        "    # print(S_list)\n",
        "    if S_list:\n",
        "        consec = S_list[0]\n",
        "        for j in S_list:\n",
        "            if(yMIN[j] < yMIN[consec]):\n",
        "                consec = j\n",
        "        return consec\n",
        "\n",
        "    return -1\n",
        "\n",
        "\n",
        "def makeGraph(df):\n",
        "    G = nx.Graph()\n",
        "    xMIN = df['xmin']\n",
        "    xMAX = df['xmax']\n",
        "    yMIN = df['ymin']\n",
        "    yMAX = df['ymax']\n",
        "    Text = df['Object']\n",
        "\n",
        "    for i in range(len(df)):\n",
        "        if findUp(df, i, xMIN, xMAX, yMIN, yMAX):\n",
        "            l = findUp(df, i, xMIN, xMAX, yMIN, yMAX)\n",
        "            if(l != -1):\n",
        "                text = Text[l]\n",
        "                G.add_edge(Text[i], text)\n",
        "        if findRight(df, i, xMIN, xMAX, yMIN, yMAX):\n",
        "            l = findRight(df, i, xMIN, xMAX, yMIN, yMAX)\n",
        "            if (l != -1):\n",
        "                text = Text[l]\n",
        "                G.add_edge(Text[i], text)\n",
        "        if findDown(df, i, xMIN, xMAX, yMIN, yMAX):\n",
        "            l = findDown(df, i, xMIN, xMAX, yMIN, yMAX)\n",
        "            if (l != -1):\n",
        "                text = Text[l]\n",
        "                G.add_edge(Text[i], text)\n",
        "        if findLeft(df, i, xMIN, xMAX, yMIN, yMAX):\n",
        "            l = findLeft(df, i, xMIN, xMAX, yMIN, yMAX)\n",
        "            if (l != -1):\n",
        "                text = Text[l]\n",
        "                G.add_edge(Text[i], text)\n",
        "    return G\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln7Fs0dRkoIf"
      },
      "source": [
        "# Matrices Over Diagonal\n",
        "Place an incident Matrix over diagonal with existing matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRjN1IPHk_bO"
      },
      "source": [
        "from typing import SupportsAbs\n",
        "import numpy as np\n",
        "from numpy.core.fromnumeric import shape\n",
        "\n",
        "# Test Matrices\n",
        "# mat1 = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "# mat2 = np.array([[3, 2, 1, 7], [6, 5, 4, 9], [9, 8, 7, 4], [1, 5, 7, 2]])\n",
        "\n",
        "\n",
        "def alignDiagonally(M1, M2, prev_Len):\n",
        "    for i in range(prev_Len, np.shape(M1)[0]):\n",
        "        for j in range(prev_Len, np.shape(M1)[0]):\n",
        "            x = i - prev_Len\n",
        "            y = j - prev_Len\n",
        "            M1[i][j] = M2[x][y]\n",
        "    return M1\n",
        "\n",
        "\n",
        "def resizeMatrix(M, I):\n",
        "    oldMat_Len = np.shape(M)[0]\n",
        "    z = np.zeros((oldMat_Len, np.shape(I)[0]), dtype=np.int64)\n",
        "    newArray = np.append(M, z, axis=1)\n",
        "    M = newArray\n",
        "\n",
        "    # Appending 1D arrays of zeros in the original Matrix\n",
        "    # (i.e. the matrix in which we want to align othe rmatrices diagonally)\n",
        "    # for i in range(appZero):\n",
        "    #     M = np.vstack((M, L))\n",
        "    appZero = np.shape(I)[0]\n",
        "    x = oldMat_Len + appZero\n",
        "    L = np.zeros((np.shape(I)[0], x), dtype=np.int64)\n",
        "    newArray = np.append(M, L, axis=0)\n",
        "    M = newArray\n",
        "\n",
        "    M = alignDiagonally(M, I, oldMat_Len)\n",
        "    return M\n",
        "\n",
        "\n",
        "# print(resizeMatrix(mat1, mat2))\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4migFjSpUxCP"
      },
      "source": [
        "# Dividing Dataset into smaller datasets\n",
        "Creating a total of four sparse matrices and saving it to drive. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Kuu2G9BAY_O"
      },
      "source": [
        "# Dataset Batch 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StRWr_C0U8uj"
      },
      "source": [
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "from numpy.core.fromnumeric import shape\n",
        "\n",
        "Z_file = lab_files_path + files[0]\n",
        "df = pd.read_csv(Z_file)\n",
        "G = makeGraph(df)\n",
        "M1 = nx.to_numpy_array(G, dtype=np.int32)\n",
        "\n",
        "for i in range(1, 26):\n",
        "    print(\"\\n= = = = = = = = = = = = = = = =\")\n",
        "    print(\"Iteration No.: \", i)\n",
        "    # Getting file\n",
        "    f = lab_files_path + files[i]\n",
        "    # Making dataframe of file\n",
        "    df = pd.read_csv(f)\n",
        "    # Making graph of the dataframe. \n",
        "    G = makeGraph(df)\n",
        "    # Storing the graph as an incident matrix(an adjacency matrix)\n",
        "    I = nx.to_numpy_array(G, dtype=np.int32)\n",
        "    # Now resizing the original sparce matrix with the new incident matrix\n",
        "    M1 = resizeMatrix(M1, I)\n",
        "\n",
        "print(\"Dimentions of Batch 1 matrix is: \", np.shape(M1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14pae4cQAmoa"
      },
      "source": [
        "# Dataset Batch 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuO0iRx_Aphl"
      },
      "source": [
        "Z1_file = lab_files_path + files[26]\n",
        "df = pd.read_csv(Z1_file)\n",
        "G = makeGraph(df)\n",
        "M2 = nx.to_numpy_array(G, dtype=np.int32)\n",
        "\n",
        "for i in range(27, 65):\n",
        "    print(\"\\n= = = = = = = = = = = = = = = =\")\n",
        "    print(\"Iteration No.: \", i)\n",
        "    # Getting file\n",
        "    f = lab_files_path + files[i]\n",
        "    # Making dataframe of file\n",
        "    df = pd.read_csv(f)\n",
        "    # Making graph of the dataframe. \n",
        "    G = makeGraph(df)\n",
        "    # Storing the graph as an incident matrix(an adjacency matrix)\n",
        "    I = nx.to_numpy_array(G, dtype=np.int32)\n",
        "    # Now resizing the original sparce matrix with the new incident matrix\n",
        "    M2 = resizeMatrix(M2, I)\n",
        "\n",
        "print(\"Dimentions of Batch 2 matrix is: \", np.shape(M2))  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-2GlwRWArD1"
      },
      "source": [
        "# Dataset Batch 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4b3ucBbqAt8Z"
      },
      "source": [
        "Z2_file = lab_files_path + files[65]\n",
        "df = pd.read_csv(Z2_file)\n",
        "G = makeGraph(df)\n",
        "M3 = nx.to_numpy_array(G, dtype=np.int32)\n",
        "    \n",
        "for i in range(66, 100):\n",
        "    print(\"\\n= = = = = = = = = = = = = = = =\")\n",
        "    print(\"Iteration No.: \", i)\n",
        "    # Getting file\n",
        "    f = lab_files_path + files[i]\n",
        "    # Making dataframe of file\n",
        "    df = pd.read_csv(f)\n",
        "    # Making graph of the dataframe. \n",
        "    G = makeGraph(df)\n",
        "    # Storing the graph as an incident matrix(an adjacency matrix)\n",
        "    I = nx.to_numpy_array(G, dtype=np.int32)\n",
        "    # Now resizing the original sparce matrix with the new incident matrix\n",
        "    M3 = resizeMatrix(M3, I)\n",
        "\n",
        "print(\"Dimentions of Batch 3 matrix is: \", np.shape(M3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfGcavncA3Iu"
      },
      "source": [
        "# Dataset Batch 4\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YA9wfeuLA5Kv"
      },
      "source": [
        "Z3_file = lab_files_path + files[100]\n",
        "df = pd.read_csv(Z3_file)\n",
        "G = makeGraph(df)\n",
        "M4 = nx.to_numpy_array(G, dtype=np.int32)\n",
        "\n",
        "for i in range(101, len(files)):\n",
        "    print(\"\\n= = = = = = = = = = = = = = = =\")\n",
        "    print(\"Iteration No.: \", i)\n",
        "    # Getting file\n",
        "    f = lab_files_path + files[i]\n",
        "    # Making dataframe of file\n",
        "    df = pd.read_csv(f)\n",
        "    # Making graph of the dataframe. \n",
        "    G = makeGraph(df)\n",
        "    # Storing the graph as an incident matrix(an adjacency matrix)\n",
        "    I = nx.to_numpy_array(G, dtype=np.int32)\n",
        "    # Now resizing the original sparce matrix with the new incident matrix\n",
        "    M4 = resizeMatrix(M4, I)\n",
        "\n",
        "print(\"Dimentions of Batch 4 matrix is: \", np.shape(M4))\n",
        "\n",
        "print(\"Total files checked are: \", checked_files)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FWHWx2nGk6K"
      },
      "source": [
        "# Verifying dataset batches size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAMbTr8WGrrc"
      },
      "source": [
        "t_size = np.shape(M1)[0] + np.shape(M2)[0]  + np.shape(M3)[0] + np.shape(M4)[0] \n",
        "print(\"Overall dimention is: \", t_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrbU0aV1lX75"
      },
      "source": [
        "# Saving the four Matrices\n",
        "Saving the matrices named Mat_batch(no.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfifTHr6lowY"
      },
      "source": [
        "np.save(\"drive/MyDrive/Project GCN Dataset/Mat_b1.npy\", M1)\n",
        "np.save(\"drive/MyDrive/Project GCN Dataset/Mat_b2.npy\", M2)\n",
        "np.save(\"drive/MyDrive/Project GCN Dataset/Mat_b3.npy\", M3)\n",
        "np.save(\"drive/MyDrive/Project GCN Dataset/Mat_b4.npy\", M4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNtE4-hmN0Nq"
      },
      "source": [
        "# Saving Matrix\n",
        "\n",
        "# MATRIX ALREADY SAVED\n",
        "# PLEASE DONOT RUN THIS CELL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XdyNtrDN0Nr"
      },
      "source": [
        "print(\"Saving Matrix\")\n",
        "np.save(\"drive/MyDrive/Project GCN Dataset/Matrix.npy\", M)\n",
        "\n",
        "print(\"Saved\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDBgT62fN0Ns"
      },
      "source": [
        "# Loading Matrix\n",
        "Loading the sparce matrix in a variable A."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8Popt5iN0Nt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "961e8a07-7b11-404d-af5e-a3f13952abbb"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# loading matrix\n",
        "A = np.load(\"drive/MyDrive/Project GCN Dataset/Mat_b1.npy\")\n",
        "print(\"Printing Sparse Matrix..\")\n",
        "print(A)\n",
        "print(np.shape(A))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Printing Sparse Matrix..\n",
            "[[0 1 1 ... 0 0 0]\n",
            " [1 0 0 ... 0 0 0]\n",
            " [1 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 1]\n",
            " [0 0 0 ... 0 1 0]]\n",
            "(5762, 5762)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oWMhfx0o5n8"
      },
      "source": [
        "# Checking Sparse Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uadt4RY4o800",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "47a9b914-99fb-48d4-ce1f-4a3e58682444"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "plt.matshow(A)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fb65a872350>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 288x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "br5DRJSKn2ha"
      },
      "source": [
        "# Generating Labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfSeOkKon5mi"
      },
      "source": [
        "from numpy import nan\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def returnLabels(direc, files):\n",
        "    labels = []\n",
        "    for f in range(len(files)):\n",
        "        seenList = []\n",
        "        df = pd.read_csv(direc + files[f])\n",
        "        text = df['Object'].to_list()\n",
        "        T_labels = df['labels'].to_list()\n",
        "\n",
        "        for i in range(len(text)):\n",
        "            if(text[i] not in seenList):\n",
        "                labels.append(T_labels[i])\n",
        "            seenList.append(text[i])\n",
        "\n",
        "    return labels\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcOYerByN0Nv"
      },
      "source": [
        "# Encoding Labels\n",
        "1. Get a list of labels from CSV files.\n",
        "2. Catagorically encode the labels "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6o6X8zRN0Nx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12e12170-0561-40fe-e4d5-48c123948bf8"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "labels = returnLabels(lab_files_path, files)\n",
        "print(\"Total labels are: \", len(labels))\n",
        "\n",
        "print(\"Encoding list..\")\n",
        "lab_encoder = LabelEncoder()\n",
        "encodings = lab_encoder.fit_transform(labels)\n",
        "print(\"Total Encodings are: \", len(encodings))\n",
        "\n",
        "\n",
        "# t_file = open(\"Encodings.txt\", \"w\")\n",
        "# for element in encodings:\n",
        "#     t_file.write(\"%i\\n\" % element)\n",
        "\n",
        "# t_file.close()\n",
        "\n",
        "\n",
        "uni_enc = set(encodings)\n",
        "print(\"Unique Encodings are: \", uni_enc)\n",
        "encodings = np.transpose(encodings)  \n",
        "\n",
        "# Encodings for Mat_b1 are\n",
        "e1_encodings_size = np.shape(A)[0]\n",
        "print(\"Size of batch1 matrix is: \", e1_encodings_size)\n",
        "e1 = encodings[0:e1_encodings_size]\n",
        "print(\"Total encodings for batch1 matrix are: \", len(e1))\n",
        "\n",
        "print(\"Encodings for Batch 1 Matrix is: \", set(e1))\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total labels are:  21097\n",
            "Encoding list..\n",
            "Total Encodings are:  21097\n",
            "Unique Encodings are:  {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11}\n",
            "Size of batch1 matrix is:  5762\n",
            "Total encodings for batch1 matrix are:  5762\n",
            "Encodings for Batch 1 Matrix is:  {2, 3, 5, 6, 8, 9, 10, 11}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhFDji2YN0Ny"
      },
      "source": [
        "# Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iejwmS9VN0Nz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e74754e1-dcd7-4f6d-e1b5-952a1c867898"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "# from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dropout, Dense, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "\n",
        "from spektral.utils.sparse import sp_matrix_to_sp_tensor\n",
        "from spektral.layers import GCNConv\n",
        "from sklearn.metrics import classification_report\n",
        "from spektral.utils import normalized_laplacian\n",
        "from spektral.utils.sparse import sp_matrix_to_sp_tensor\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "print(\"Libraries Imported..\")\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries Imported..\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTQOoH9xN0N0"
      },
      "source": [
        "# Model Hyper-parameters\n",
        "Adding model hyper-parameters:\n",
        "1. Learning rate(At whar pace should the model be learning)\n",
        "2. Channels(Neurons)\n",
        "3. Dropout rate(Now many neurons should be active at a time)\n",
        "4. Epochs(How many times a data is passed through the model)\n",
        "5. Batch size(How much data should be fed to the model at a time)\n",
        "etc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVOMWygNN0N0"
      },
      "source": [
        "# Hyper-parameters\n",
        "channels = 32\n",
        "dropout = 0.5\n",
        "l2_reg = 5e-4\n",
        "learning_rate = 1e-2\n",
        "epochs = 200\n",
        "batch_size = 1\n",
        "es_patience = 10"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UNUsu-PzHLh"
      },
      "source": [
        "# Getting Node Features\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJWoN1DDzOU6",
        "outputId": "26bdcf22-5a30-4dba-964b-09f44341d4c9"
      },
      "source": [
        "# X = Features Matrix\n",
        "X = np.matrix([[i, -i] for i in range(A.shape[0])], dtype=float)\n",
        "# print(\"Printing Feature Matrix..\")\n",
        "# print(X)\n",
        "\n",
        "# Identity matrix of the same shape as adjacency matrix\n",
        "I = np.matrix(np.eye(A.shape[0]))\n",
        "\n",
        "# Self looping making node connections with themselves\n",
        "# i.e. multiply the adjacency matrix with identity matrix\n",
        "A_new = A + I\n",
        "\n",
        "# Multiplying the self-looped graph with the feature matrix\n",
        "\n",
        "# Stores the sum of array(Row wise) in a new array called D\n",
        "D = np.array(np.sum(A, axis=0))[0]\n",
        "\n",
        "# Making the diagonal matrix named D containing the elements of 1D array D\n",
        "# D = np.matrix(np.diag(D))\n",
        "# print(\"Printing the 2D array named D containing the elemnets of 1D D in diagonal\")\n",
        "# print(D)\n",
        "\n",
        "# Stores the degree of the graph in a 1D array called D\n",
        "D = np.array(np.sum(A_new, axis=0))[0]\n",
        "# Transforming that 1D array to a degree matrix\n",
        "D = np.matrix(np.diag(D))\n",
        "# print(\"Degree matrix of A_new multiplied by A_new..\")\n",
        "D_new = D * A_new\n",
        "# print(D_new)\n",
        "\n",
        "# Weight Matrix\n",
        "W0 = np.random.randn(X.shape[1], 8) * 0.01\n",
        "\n",
        "# Reducing feature of output feature representation..\n",
        "print(\"Output feature representation..\")\n",
        "final = D**-1 * A_new * X * W0\n",
        "\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "\n",
        "H_1 = relu(final)\n",
        "# print(\"After applying relu function..\")\n",
        "# print(H_1)\n",
        "\n",
        "output = H_1\n",
        "\n",
        "# print(\"Printing A_new matrix..\")\n",
        "# print(A_new)\n",
        "\n",
        "G = nx.from_numpy_matrix(np.array(A_new))\n",
        "# nx.draw_networkx_edges(G, pos=nx.spring_layout(\n",
        "#     G), arrowstyle=\"<|-\", style=\"dashed\")\n",
        "# plt.show()\n",
        "# # nx.draw(G)\n",
        "# # plt.savefig(\"Sample Graph.png\")\n",
        "feature_representations = {\n",
        "    node: np.array(output)[node]\n",
        "    for node in G.nodes()\n",
        "}\n",
        "\n",
        "features = np.array(feature_representations.values())\n",
        "\n",
        "# print(\"Printing feature representation..\")\n",
        "# print(features)\n",
        "\n",
        "vals = []\n",
        "\n",
        "for v in feature_representations.values():\n",
        "  vals.append(v)\n",
        "\n",
        "vals = np.array(vals)\n",
        "\n",
        "print(\"Prining features in an array\", vals)\n",
        "\n",
        "print(\"Shape of vals is: \", np.shape(vals))\n",
        "\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output feature representation..\n",
            "Prining features in an array [[ 0.          0.          0.         ...  0.          0.27914551\n",
            "   0.        ]\n",
            " [ 0.          0.          0.         ...  0.          0.22154406\n",
            "   0.        ]\n",
            " [ 0.          0.          0.         ...  0.          0.31348484\n",
            "   0.        ]\n",
            " ...\n",
            " [ 0.          0.          0.         ...  0.         50.86356169\n",
            "   0.        ]\n",
            " [ 0.          0.          0.         ...  0.         50.7920768\n",
            "   0.        ]\n",
            " [ 0.          0.          0.         ...  0.         50.92744022\n",
            "   0.        ]]\n",
            "Shape of vals is:  (5762, 8)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U87KGv4sN0N1"
      },
      "source": [
        "# Model Definition\n",
        "3 GCN-Conv Layers & 2 Dense Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZwuo9T0N0N1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fc814c9-7a85-4fcc-c437-49aa1f9742aa"
      },
      "source": [
        "N = A.shape[0]\n",
        "F = vals.shape[1]\n",
        "# print(\"Node has \", F, \" features\")\n",
        "\n",
        "fltr = normalized_laplacian(A)\n",
        "X_in = Input(shape=(N, F))\n",
        "# print(\"Shape of X-in is: \", np.shape(X_in))\n",
        "\n",
        "A_in = Input(tensor=sp_matrix_to_sp_tensor(fltr))\n",
        "# print(\"Shape of A-in is: \", np.shape(A_in))\n",
        "\n",
        "D1 = Dropout(dropout)(X_in)\n",
        "G1 = GCNConv(channels, activation='relu', kernel_regularizer=l2(l2_reg), use_bias=True)([X_in, A_in])\n",
        "D2 = Dropout(dropout)(G1)\n",
        "G2 = GCNConv(channels, activation='relu', kernel_regularizer=l2(l2_reg), use_bias=True)([G1, A_in])\n",
        "\n",
        "flatten = Flatten()(G2)\n",
        "D1 = Dense(512, activation='relu')(flatten)\n",
        "D2_out = Dense(len(set(e1)), activation='softmax')(D1)\n",
        "\n",
        "model = Model(inputs = [X_in, A_in], outputs=D2_out)\n",
        "model.compile(optimizer = 'Adam', loss = 'catagorical_crossentropy', weighted_metrics=['acc'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_6\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_13 (InputLayer)           [(None, 5762, 8)]    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_14 (InputLayer)           [(5762, 5762)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "gcn_conv_12 (GCNConv)           (None, 5762, 32)     288         input_13[0][0]                   \n",
            "                                                                 input_14[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "gcn_conv_13 (GCNConv)           (None, 5762, 32)     1056        gcn_conv_12[0][0]                \n",
            "                                                                 input_14[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten_6 (Flatten)             (None, 184384)       0           gcn_conv_13[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_12 (Dense)                (None, 512)          94405120    flatten_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_13 (Dense)                (None, 8)            4104        dense_12[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 94,410,568\n",
            "Trainable params: 94,410,568\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sQcuJa7N0N2"
      },
      "source": [
        "# Training Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gg_hDbFYN0N2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        },
        "outputId": "2cac97c7-b373-4597-8676-23a0c99290d6"
      },
      "source": [
        "# print(np.shape(vals))\n",
        "# print(np.shape(A))\n",
        "\n",
        "# T = tf.convert_to_tensor(vals, np.int32)\n",
        "\n",
        "# print(tf.shape(T))\n",
        "# tf.reshape(T, (None, N, F))\n",
        "# print(tf.shape(T))\n",
        "\n",
        "model.fit([vals, A],\n",
        "          e1,\n",
        "          epochs = epochs,\n",
        "          batch_size = batch_size,\n",
        "          )"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-27da809dc192>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m           \u001b[0me1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m           \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m           \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m           )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    758\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    759\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 760\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3064\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3065\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3066\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3067\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3462\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3463\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3464\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3306\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3307\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3308\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3309\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3310\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    992\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 994\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    995\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:853 train_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:842 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:1286 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:2849 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py:3632 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:835 run_step  **\n        outputs = model.train_step(data)\n    /usr/local/lib/python3.7/dist-packages/keras/engine/training.py:787 train_step\n        y_pred = self(x, training=True)\n    /usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py:1020 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /usr/local/lib/python3.7/dist-packages/keras/engine/input_spec.py:269 assert_input_compatibility\n        ', found shape=' + display_shape(x.shape))\n\n    ValueError: Input 0 is incompatible with layer model_6: expected shape=(None, 5762, 8), found shape=(1, 8)\n"
          ]
        }
      ]
    }
  ]
}